{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, TensorBoard\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from datetime import datetime\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import Input, Conv2D, UpSampling2D, Dropout, LeakyReLU, BatchNormalization, Activation, Add, Subtract\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "# from keras.applications import VGG16\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "# from libs.pconv_model_UNet import PConvUnet\n",
    "from keras.models import load_model  \n",
    "\n",
    "from copy import deepcopy\n",
    "from libs.util import random_mask\n",
    "\n",
    "# Settings\n",
    "MAX_BATCH_SIZE = 64\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "matrix_length = 32\n",
    "\n",
    "matrix_df = pd.read_csv('./data/trafficV_M.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createTrainArray(week_history_num=0, minute_history_num=0):\n",
    "    week_delta_list = [pd.Timedelta(i+1, unit='W') for i in range(week_history_num)]\n",
    "    minute_delta_list = [pd.Timedelta((i+1)*15, unit='m') for i in range(minute_history_num)]\n",
    "    # 参考历史数据时间点list\n",
    "    delta_list = week_delta_list+minute_delta_list\n",
    "    print(delta_list)\n",
    "    \n",
    "    set_up_time = pd.Timedelta(week_history_num, unit='W')\n",
    "    # 根据历史数据选取多少，重新构建数据集\n",
    "    # 相当于去除最开始week_history_num个周的数据，因为这些数据无法找到更前的数据\n",
    "    train_df = matrix_df.truncate(before=matrix_df.index.min() + set_up_time)\n",
    "    \n",
    "    train_ago_array_tuple = tuple([np.array(matrix_df.loc[train_df.index - i]).reshape(-1, matrix_length, matrix_length, 1) for i in delta_list])\n",
    "    train_df = np.array(train_df).reshape(-1, matrix_length, matrix_length, 1)\n",
    "    # concatenate保持 待修复数据在前，参考历史数据在后。与random_mask函数生成mask相一致\n",
    "    train_array = np.concatenate((train_df,)+train_ago_array_tuple, axis=3)\n",
    "    print(train_array.shape)\n",
    "    return train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timedelta('7 days 00:00:00'), Timedelta('14 days 00:00:00'), Timedelta('0 days 00:15:00'), Timedelta('0 days 00:30:00'), Timedelta('0 days 00:45:00')]\n",
      "(16032, 32, 32, 6)\n"
     ]
    }
   ],
   "source": [
    "week_history_num = 2\n",
    "minute_history_num = 3\n",
    "\n",
    "channel_num = week_history_num +minute_history_num +1\n",
    "smooth_time = channel_num-1\n",
    "\n",
    "train_array = createTrainArray(week_history_num, minute_history_num)\n",
    "X_train, X_test = train_test_split(train_array, test_size = 0.1, shuffle=False)\n",
    "# X_train, X_test = train_test_split(train_array, test_size = 0.1, random_state=42, shuffle=False)\n",
    "\n",
    "# X_train = train_array[:16704-900-900]\n",
    "# X_val = train_array[16704-900-900:16704-900]\n",
    "# X_test = train_array[16704-900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_shape = (matrix_length, matrix_length, channel_num)\n",
    "true_volume_shape = (matrix_length, matrix_length, 1)\n",
    "history_volume_shape = (matrix_length, matrix_length, channel_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_steps = X_train.shape[0] // MAX_BATCH_SIZE\n",
    "val_steps = X_test.shape[0] // MAX_BATCH_SIZE\n",
    "epoch_steps, val_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以第一数据为例. 第一列为待预测数据\n",
    "# 第一例：1.15 0:00  二：1.8 0:00  三：1.1 0:00  四：1.14 23:45  五：1.14 23:30  六：1.14 23:15\n",
    "# X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_size = 0.8\n",
    "mask_type = 'rand'\n",
    "block_size = (32, 32)\n",
    "\n",
    "# 单个矩阵mask\n",
    "rand_mask = random_mask(matrix_length, matrix_length, size=rand_size, channels=channel_num, smooth_time=smooth_time, type=mask_type, block_size=block_size)\n",
    "# 堆叠成多个mask，方便对batch数据进行处理\n",
    "mask = np.stack([rand_mask for _ in range(MAX_BATCH_SIZE)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def l2(y_true, y_pred):\n",
    "    size = 0\n",
    "    if rand_size<=1:\n",
    "        size = int((matrix_length * matrix_length) * rand_size)\n",
    "    else:\n",
    "        size = rand_size\n",
    "        \n",
    "    if size == 0:\n",
    "        raise Exception(\"size == 0\")\n",
    "    return math.sqrt(np.sum(np.mean(np.square(y_true - y_pred), axis=0))/size)\n",
    "\n",
    "def l1(y_true, y_pred):\n",
    "    size = 0\n",
    "    if rand_size<=1:\n",
    "        size = int((matrix_length * matrix_length) * rand_size)\n",
    "    else:\n",
    "        size = rand_size\n",
    "        \n",
    "    if size == 0:\n",
    "        raise Exception(\"size == 0\")\n",
    "    return np.sum(np.mean(np.abs(y_true - y_pred), axis=0))/size\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    size = 0\n",
    "    if rand_size<=1:\n",
    "        size = int((matrix_length * matrix_length) * rand_size)\n",
    "    else:\n",
    "        size = rand_size\n",
    "        \n",
    "    if size == 0:\n",
    "        raise Exception(\"size == 0\")\n",
    "        \n",
    "    return np.sum(np.mean((np.abs(y_true - y_pred)/y_true)*100, axis=0))/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "def load_data(volume_matrix, batch_size=MAX_BATCH_SIZE):\n",
    "    n_batches=batch_size\n",
    "    len_of_matrix = len(volume_matrix)\n",
    "\n",
    "    batch_i = 0\n",
    "    while ((batch_i+1)*batch_size < len_of_matrix):\n",
    "        batch_matrix = volume_matrix[batch_i*batch_size: (batch_i+1)*batch_size]\n",
    "        masked = deepcopy(batch_matrix)\n",
    "        # true_volume为待修复数据， history_volume为历史数据及当前残差待修复数据\n",
    "        true_volume = deepcopy(batch_matrix[:, :, :, :1])\n",
    "        # mask==1代表有效采集点，0代表待预测采集点\n",
    "        traffic_mean = masked[mask==1].mean()\n",
    "        # 待预测点的值用已知值的平均值初始化\n",
    "        masked[mask==0] = traffic_mean\n",
    "        history_volume = deepcopy(masked)\n",
    "        \n",
    "        batch_i+=1\n",
    "\n",
    "        yield true_volume, history_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(y_true, y_pred):\n",
    "        \"\"\"Calculate the L1 loss used in all loss calculations\"\"\"\n",
    "        if K.ndim(y_true) == 4:\n",
    "            return K.sum(K.square(y_pred - y_true), axis=[1,2,3])\n",
    "        elif K.ndim(y_true) == 3:\n",
    "            return K.sum(K.square(y_pred - y_true), axis=[1,2])\n",
    "        else:\n",
    "            raise NotImplementedError(\"Calculating L1 loss on 1D tensors? should not occur for this network\")\n",
    "\n",
    "# 缺失点mse\n",
    "def loss_hole(y_true, y_pred):\n",
    "    return l2_loss((1-mask) * y_true, (1-mask) * y_pred)\n",
    "\n",
    "# 非缺失点mse\n",
    "def loss_bg(y_true, y_pred):\n",
    "    return l2_loss(mask * y_true, mask * y_pred)\n",
    "\n",
    "def loss_fuc(y_true, y_pred):\n",
    "    return loss_hole(y_true, y_pred)*3 + loss_bg(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_init = 'glorot_uniform'\n",
    "bias_init = 'zeros'\n",
    "\n",
    "# kernel_init = initializers.he_uniform()\n",
    "# bias_init = initializers.he_uniform()\n",
    "kernel_regul = regularizers.l2(1)\n",
    "activity_regul = regularizers.l2(1)\n",
    "\n",
    "learn_rate = 0.0002\n",
    "\n",
    "# ResNet block\n",
    "def identity_block(X, filters, f):\n",
    "\n",
    "    F1, F2 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(filters=F1, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(X)\n",
    "\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(X)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "# ENCODER\n",
    "def encoder_layer(img_in, filters, kernel_size, bn=True, resid=True):\n",
    "    # conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding='same')(img_in)\n",
    "    conv = img_in\n",
    "    if bn:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "#             conv = MaxPooling2D((2, 2))(conv)\n",
    "\n",
    "\n",
    "    if resid:\n",
    "        conv = identity_block(conv, (filters, filters), kernel_size)\n",
    "\n",
    "    return conv\n",
    "\n",
    "# DECODER\n",
    "def decoder_layer(img_in, e_conv, filters, kernel_size, bn=True, resid=True):\n",
    "    # up_img = UpSampling2D(size=(2,2))(img_in)\n",
    "    up_img = img_in\n",
    "    concat_img = Concatenate(axis=3)([e_conv,up_img])\n",
    "    conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding='same',\n",
    "                  kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(concat_img)\n",
    "    if bn:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    conv = LeakyReLU(alpha=0.1)(conv)\n",
    "\n",
    "    if resid:\n",
    "        conv = identity_block(conv, (filters, filters), kernel_size)\n",
    "    return conv\n",
    "\n",
    "\n",
    "\n",
    "def build_unet():      \n",
    "\n",
    "    # INPUTS\n",
    "    history_traffic_volume = Input(shape=history_volume_shape)\n",
    "\n",
    "    # kernel_init = initializers.he_normal()\n",
    "    # bias_init = initializers.he_normal()\n",
    "    kernel_init = 'glorot_uniform'\n",
    "    bias_init = 'zeros'\n",
    "\n",
    "#         kernel_init = initializers.he_uniform()\n",
    "#         bias_init = 'Orthogonal'\n",
    "    kernel_regul = regularizers.l2(1)\n",
    "    activity_regul = regularizers.l2(1)\n",
    "\n",
    "    filters_base = 32\n",
    "    e_conv1_head = Conv2D(filters=filters_base, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(history_traffic_volume)\n",
    "#         e_conv1_head = Conv2D(filters=filters_base*1, kernel_size=3, strides=1, padding='same',\n",
    "#                               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "#                       kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv1_head)\n",
    "    e_conv1_tail = AveragePooling2D((2, 2))(e_conv1_head)\n",
    "    e_conv1 = encoder_layer(e_conv1_tail, filters_base, 3, bn=False)\n",
    "\n",
    "    e_conv2_head = Conv2D(filters=filters_base*2, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv1)\n",
    "    e_conv2_tail = AveragePooling2D((2, 2))(e_conv2_head)\n",
    "    e_conv2 = encoder_layer(e_conv2_tail, filters_base*2, 3)\n",
    "\n",
    "    e_conv3_head = Conv2D(filters=filters_base*4, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv2)\n",
    "    e_conv3_tail = AveragePooling2D((2, 2))(e_conv3_head)\n",
    "    d_conv3_head = encoder_layer(e_conv3_tail, filters_base*4, 3)\n",
    "    resid1 = Add()([e_conv3_tail, d_conv3_head])\n",
    "    d_conv3_tail = UpSampling2D(size=(2, 2))(resid1)\n",
    "\n",
    "\n",
    "    d_conv4_head = decoder_layer(d_conv3_tail, e_conv3_head, filters_base*2, 3)\n",
    "    resid2 = Add()([d_conv4_head, e_conv2_tail])\n",
    "    d_conv4_tail = UpSampling2D(size=(2, 2))(resid2)\n",
    "\n",
    "\n",
    "    d_conv5_head = decoder_layer(d_conv4_tail, e_conv2_head, filters_base*1, 3)\n",
    "    resid3 = Add()([d_conv5_head, e_conv1_tail])\n",
    "    d_conv5_tail = UpSampling2D(size=(2, 2))(resid3)\n",
    "\n",
    "    d_conv6_head = decoder_layer(d_conv5_tail, e_conv1_head, filters_base//2, 3, bn=False)\n",
    "\n",
    "\n",
    "    outputs = Conv2D(1, 1, activation = 'relu', kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(d_conv6_head)\n",
    "\n",
    "    # Setup the model inputs / outputs\n",
    "    model = Model(inputs=history_traffic_volume, outputs=outputs)\n",
    "\n",
    "    # Compile the model RMSprop\n",
    "    model.compile(\n",
    "        optimizer = Adam(lr=learn_rate),\n",
    "#         loss='mse'\n",
    "        loss = loss_hole\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_masked = deepcopy(X_test)\n",
    "test_true_volume = deepcopy(X_test[:, :, :, :1])\n",
    "\n",
    "test_length = len(X_test)\n",
    "test_mask = np.stack([rand_mask for _ in range(test_length)], axis=0)\n",
    "\n",
    "test_traffic_mean = X_test[test_mask==1].mean()\n",
    "test_masked[test_mask==0] = test_traffic_mean\n",
    "test_history_volume = deepcopy(test_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_step = []\n",
    "l2_validation = []\n",
    "\n",
    "unet = build_unet()\n",
    "\n",
    "\n",
    "def train(train_matrix, epochs, batch_size=MAX_BATCH_SIZE, learn_rate=0.01):\n",
    "\n",
    "    min_mse = 999\n",
    "    start_time = datetime.now()\n",
    "    print(\"train start \"+str(start_time))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch>100 and epoch % 5 == 0 and epoch != 0:\n",
    "            unet_lr = K.get_value(unet.optimizer.lr)\n",
    "            if unet_lr>0.0001:\n",
    "                K.set_value(unet.optimizer.lr, unet_lr*0.9)\n",
    "                \n",
    "        for batch_i, (true_volume, history_volume) in enumerate(load_data(train_matrix,batch_size)):\n",
    "            # true_volume 真实待预测路网交通量  history_volume 路网交通量历史数据\n",
    "            #  训练 unet\n",
    "            #  训练 Generator\n",
    "            g_loss = unet.train_on_batch(history_volume, true_volume)\n",
    "\n",
    "\n",
    "        elapsed_time = datetime.now() - start_time\n",
    "        # Plot the progress\n",
    "        y_pred = unet.predict(test_history_volume)\n",
    "        \n",
    "        y_true = (1-test_mask[:,:,:,:1])*test_true_volume\n",
    "        y_pred = (1-test_mask[:,:,:,:1])*y_pred\n",
    "        l2_epoch_validation = l2(y_true, y_pred)\n",
    "        l1_epoch_validation = l1(y_true, y_pred)\n",
    "        \n",
    "        y_pred[y_true==0] += 1\n",
    "        y_true[y_true==0] += 1\n",
    "        mape_epoch_validation = mape(y_true, y_pred)\n",
    "        \n",
    "        if(l2_epoch_validation < min_mse and l2_epoch_validation<15):\n",
    "            unet.save_weights('./model/dataRecorvey20191109/tmp/min_runet.h5')\n",
    "            min_mse = l2_epoch_validation\n",
    "        \n",
    "        lr_step.append(K.get_value(unet.optimizer.lr))\n",
    "        l2_validation.append(l2_epoch_validation)\n",
    "        if epoch%1==0:\n",
    "#             print(\"unet lr:\"+ str(K.get_value(unet.optimizer.lr)))\n",
    "            print (\"[Epoch %d/%d]  [mse: %f] [mae: %f] [mape: %f] [G loss: %f] time: %s\" % (epoch+1, epochs,\n",
    "                                                                    l2_epoch_validation,\n",
    "                                                                    l1_epoch_validation,\n",
    "                                                                    mape_epoch_validation,\n",
    "                                                                    g_loss,\n",
    "                                                                    elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(X_train, epochs=200, batch_size=MAX_BATCH_SIZE, learn_rate=learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet.save_weights('./model/dataRecorvey20191109/runet_10_rmse11.97.h5')\n",
    "# unet.load_weights('./model/RUnet/unet_60epoch_18rmse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_masked_tmp = deepcopy(X_test)\n",
    "# test_true_volume_tmp = deepcopy(X_test[:, :, :, :1])\n",
    "\n",
    "# test_length_tmp = len(X_test)\n",
    "# rand_mask_tmp = random_mask(matrix_length, matrix_length, size=rand_size, channels=channel_num, smooth_time=smooth_time, type=mask_type, block_size=block_size)\n",
    "# test_mask_tmp = np.stack([rand_mask_tmp for _ in range(test_length_tmp)], axis=0)\n",
    "\n",
    "# test_traffic_mean_tmp = X_test[test_mask_tmp==1].mean()\n",
    "# test_masked_tmp[test_mask_tmp==0] = test_traffic_mean_tmp\n",
    "# test_history_volume_tmp = deepcopy(test_masked_tmp)\n",
    "\n",
    "\n",
    "# y_pred = unet.predict(test_history_volume_tmp)\n",
    "\n",
    "# # 仅对缺失数据进行l2评价。（对预测来说既对第一层进行评价，验证）\n",
    "# y_true = (1-test_mask[:,:,:,:1])*test_true_volume_tmp\n",
    "# y_pred = (1-test_mask[:,:,:,:1])*y_pred\n",
    "\n",
    "\n",
    "y_pred = unet.predict(test_history_volume)\n",
    "\n",
    "# 仅对缺失数据进行l2评价。（对预测来说既对第一层进行评价，验证）\n",
    "y_true = (1-test_mask[:,:,:,:1])*test_true_volume\n",
    "y_pred = (1-test_mask[:,:,:,:1])*y_pred\n",
    "\n",
    "l2(y_true, y_pred), l1(y_true, y_pred)\n",
    "# (13.612251463372885, 7.896321495663051)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_true==0] += 1\n",
    "y_true[y_true==0] += 1\n",
    "\n",
    "mape(y_true, y_pred)\n",
    "# 5.013244341615349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_model = build_unet()\n",
    "min_model.load_weights('./model/dataRecorvey20191109/tmp/min_runet.h5')\n",
    "\n",
    "y_pred = min_model.predict(test_history_volume)\n",
    "\n",
    "# 仅对缺失数据进行l2评价。（对预测来说既对第一层进行评价，验证）\n",
    "y_true = (1-test_mask[:,:,:,:1])*test_true_volume\n",
    "y_pred = (1-test_mask[:,:,:,:1])*y_pred\n",
    "\n",
    "l2(y_true, y_pred), l1(y_true, y_pred)\n",
    "# (11.085728026573435, 6.957813774885677)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_true==0] += 1\n",
    "y_true[y_true==0] += 1\n",
    "\n",
    "mape(y_true, y_pred)\n",
    "# 4.429743492502313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posX = 0\n",
    "posY = 2\n",
    "startX = 500\n",
    "gapX = 200\n",
    "\n",
    "y = test_true_volume[:, posX, posY, :][startX: startX+gapX]\n",
    "\n",
    "yf = y_pred[:, posX, posY, :][startX: startX+gapX]\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "fig, ax = plt.subplots(figsize=(25, 8))\n",
    "lines = plt.plot(x, yf, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start_time = 1600\n",
    "y = y_true.reshape(-1,)[start_time: start_time+100]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[start_time: start_time+100]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 10))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi = l2_validation\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "lines = plt.plot(xi, yi, 'k^--', linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lr_step\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "lines = plt.plot(x, y, 'ko-', linewidth=1, markersize=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 3600\n",
    "y = y_true.reshape(-1,)[start_time: start_time+100]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[start_time: start_time+100]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 0\n",
    "y = y_true.reshape(-1,)[start_time: start_time+100]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[start_time: start_time+100]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_volume"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
