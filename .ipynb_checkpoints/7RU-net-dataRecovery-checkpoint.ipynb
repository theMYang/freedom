{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, TensorBoard\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from datetime import datetime\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import Input, Conv2D, UpSampling2D, Dropout, LeakyReLU, BatchNormalization, Activation, Add, Subtract\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "# from keras.applications import VGG16\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "# from libs.pconv_model_UNet import PConvUnet\n",
    "from keras.models import load_model  \n",
    "\n",
    "from copy import deepcopy\n",
    "from libs.util import random_mask\n",
    "\n",
    "# Settings\n",
    "MAX_BATCH_SIZE = 64\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "matrix_length = 32\n",
    "\n",
    "matrix_df = pd.read_csv('./data/trafficV_M.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createTrainArray(week_history_num=0, minute_history_num=0):\n",
    "    week_delta_list = [pd.Timedelta(i+1, unit='W') for i in range(week_history_num)]\n",
    "    minute_delta_list = [pd.Timedelta((i+1)*15, unit='m') for i in range(minute_history_num)]\n",
    "    # 参考历史数据时间点list\n",
    "    delta_list = week_delta_list+minute_delta_list\n",
    "    print(delta_list)\n",
    "    \n",
    "    set_up_time = pd.Timedelta(week_history_num, unit='W')\n",
    "    # 根据历史数据选取多少，重新构建数据集\n",
    "    # 相当于去除最开始week_history_num个周的数据，因为这些数据无法找到更前的数据\n",
    "    train_df = matrix_df.truncate(before=matrix_df.index.min() + set_up_time)\n",
    "    \n",
    "    train_ago_array_tuple = tuple([np.array(matrix_df.loc[train_df.index - i]).reshape(-1, matrix_length, matrix_length, 1) for i in delta_list])\n",
    "    train_df = np.array(train_df).reshape(-1, matrix_length, matrix_length, 1)\n",
    "    # concatenate保持 待修复数据在前，参考历史数据在后。与random_mask函数生成mask相一致\n",
    "    train_array = np.concatenate((train_df,)+train_ago_array_tuple, axis=3)\n",
    "    print(train_array.shape)\n",
    "    return train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timedelta('7 days 00:00:00'), Timedelta('14 days 00:00:00'), Timedelta('0 days 00:15:00'), Timedelta('0 days 00:30:00'), Timedelta('0 days 00:45:00')]\n",
      "(16032, 32, 32, 6)\n"
     ]
    }
   ],
   "source": [
    "week_history_num = 2\n",
    "minute_history_num = 3\n",
    "\n",
    "channel_num = week_history_num +minute_history_num +1\n",
    "smooth_time = channel_num-1\n",
    "\n",
    "train_array = createTrainArray(week_history_num, minute_history_num)\n",
    "X_train, X_test = train_test_split(train_array, test_size = 0.1, shuffle=False)\n",
    "# X_train, X_test = train_test_split(train_array, test_size = 0.1, random_state=42, shuffle=False)\n",
    "\n",
    "# X_train = train_array[:16704-900-900]\n",
    "# X_val = train_array[16704-900-900:16704-900]\n",
    "# X_test = train_array[16704-900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_shape = (matrix_length, matrix_length, channel_num)\n",
    "true_volume_shape = (matrix_length, matrix_length, 1)\n",
    "history_volume_shape = (matrix_length, matrix_length, channel_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 25)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_steps = X_train.shape[0] // MAX_BATCH_SIZE\n",
    "val_steps = X_test.shape[0] // MAX_BATCH_SIZE\n",
    "epoch_steps, val_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以第一数据为例. 第一列为待预测数据\n",
    "# 第一例：1.15 0:00  二：1.8 0:00  三：1.1 0:00  四：1.14 23:45  五：1.14 23:30  六：1.14 23:15\n",
    "# X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_size = 0.1\n",
    "mask_type = 'rand'\n",
    "block_size = (32, 32)\n",
    "\n",
    "# 单个矩阵mask\n",
    "rand_mask = random_mask(matrix_length, matrix_length, size=rand_size, channels=channel_num, smooth_time=smooth_time, type=mask_type, block_size=block_size)\n",
    "# 堆叠成多个mask，方便对batch数据进行处理\n",
    "mask = np.stack([rand_mask for _ in range(MAX_BATCH_SIZE)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def l2(y_true, y_pred):\n",
    "    size = 0\n",
    "    if rand_size<=1:\n",
    "        size = int((matrix_length * matrix_length) * rand_size)\n",
    "    else:\n",
    "        size = rand_size\n",
    "        \n",
    "    if size == 0:\n",
    "        raise Exception(\"size == 0\")\n",
    "    return math.sqrt(np.sum(np.mean(np.square(y_true - y_pred), axis=0))/size)\n",
    "\n",
    "def l1(y_true, y_pred):\n",
    "    size = 0\n",
    "    if rand_size<=1:\n",
    "        size = int((matrix_length * matrix_length) * rand_size)\n",
    "    else:\n",
    "        size = rand_size\n",
    "        \n",
    "    if size == 0:\n",
    "        raise Exception(\"size == 0\")\n",
    "    return np.sum(np.mean(np.abs(y_true - y_pred), axis=0))/size\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    size = 0\n",
    "    if rand_size<=1:\n",
    "        size = int((matrix_length * matrix_length) * rand_size)\n",
    "    else:\n",
    "        size = rand_size\n",
    "        \n",
    "    if size == 0:\n",
    "        raise Exception(\"size == 0\")\n",
    "        \n",
    "    return np.sum(np.mean((np.abs(y_true - y_pred)/y_true)*100, axis=0))/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "def load_data(volume_matrix, batch_size=MAX_BATCH_SIZE):\n",
    "    n_batches=batch_size\n",
    "    len_of_matrix = len(volume_matrix)\n",
    "\n",
    "    batch_i = 0\n",
    "    while ((batch_i+1)*batch_size < len_of_matrix):\n",
    "        batch_matrix = volume_matrix[batch_i*batch_size: (batch_i+1)*batch_size]\n",
    "        masked = deepcopy(batch_matrix)\n",
    "        # true_volume为待修复数据， history_volume为历史数据及当前残差待修复数据\n",
    "        true_volume = deepcopy(batch_matrix[:, :, :, :1])\n",
    "        # mask==1代表有效采集点，0代表待预测采集点\n",
    "        traffic_mean = masked[mask==1].mean()\n",
    "        # 待预测点的值用已知值的平均值初始化\n",
    "        masked[mask==0] = traffic_mean\n",
    "        history_volume = deepcopy(masked)\n",
    "        \n",
    "        batch_i+=1\n",
    "\n",
    "        yield true_volume, history_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(y_true, y_pred):\n",
    "        \"\"\"Calculate the L1 loss used in all loss calculations\"\"\"\n",
    "        if K.ndim(y_true) == 4:\n",
    "            return K.sum(K.square(y_pred - y_true), axis=[1,2,3])\n",
    "        elif K.ndim(y_true) == 3:\n",
    "            return K.sum(K.square(y_pred - y_true), axis=[1,2])\n",
    "        else:\n",
    "            raise NotImplementedError(\"Calculating L1 loss on 1D tensors? should not occur for this network\")\n",
    "\n",
    "# 缺失点mse\n",
    "def loss_hole(y_true, y_pred):\n",
    "    return l2_loss((1-mask) * y_true, (1-mask) * y_pred)\n",
    "\n",
    "# 非缺失点mse\n",
    "def loss_bg(y_true, y_pred):\n",
    "    return l2_loss(mask * y_true, mask * y_pred)\n",
    "\n",
    "def loss_fuc(y_true, y_pred):\n",
    "    return loss_hole(y_true, y_pred)*3 + loss_bg(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_init = 'glorot_uniform'\n",
    "bias_init = 'zeros'\n",
    "\n",
    "# kernel_init = initializers.he_uniform()\n",
    "# bias_init = initializers.he_uniform()\n",
    "kernel_regul = regularizers.l2(1)\n",
    "activity_regul = regularizers.l2(1)\n",
    "\n",
    "learn_rate = 0.0002\n",
    "\n",
    "# ResNet block\n",
    "def identity_block(X, filters, f):\n",
    "\n",
    "    F1, F2 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(filters=F1, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(X)\n",
    "\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(X)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "# ENCODER\n",
    "def encoder_layer(img_in, filters, kernel_size, bn=True, resid=True):\n",
    "    # conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding='same')(img_in)\n",
    "    conv = img_in\n",
    "    if bn:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "#             conv = MaxPooling2D((2, 2))(conv)\n",
    "\n",
    "\n",
    "    if resid:\n",
    "        conv = identity_block(conv, (filters, filters), kernel_size)\n",
    "\n",
    "    return conv\n",
    "\n",
    "# DECODER\n",
    "def decoder_layer(img_in, e_conv, filters, kernel_size, bn=True, resid=True):\n",
    "    # up_img = UpSampling2D(size=(2,2))(img_in)\n",
    "    up_img = img_in\n",
    "    concat_img = Concatenate(axis=3)([e_conv,up_img])\n",
    "    conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding='same',\n",
    "                  kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(concat_img)\n",
    "    if bn:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    conv = LeakyReLU(alpha=0.1)(conv)\n",
    "\n",
    "    if resid:\n",
    "        conv = identity_block(conv, (filters, filters), kernel_size)\n",
    "    return conv\n",
    "\n",
    "\n",
    "\n",
    "def build_unet():      \n",
    "\n",
    "    # INPUTS\n",
    "    history_traffic_volume = Input(shape=history_volume_shape)\n",
    "\n",
    "    # kernel_init = initializers.he_normal()\n",
    "    # bias_init = initializers.he_normal()\n",
    "    kernel_init = 'glorot_uniform'\n",
    "    bias_init = 'zeros'\n",
    "\n",
    "#         kernel_init = initializers.he_uniform()\n",
    "#         bias_init = 'Orthogonal'\n",
    "    kernel_regul = regularizers.l2(1)\n",
    "    activity_regul = regularizers.l2(1)\n",
    "\n",
    "    filters_base = 32\n",
    "    e_conv1_head = Conv2D(filters=filters_base, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(history_traffic_volume)\n",
    "#         e_conv1_head = Conv2D(filters=filters_base*1, kernel_size=3, strides=1, padding='same',\n",
    "#                               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "#                       kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv1_head)\n",
    "    e_conv1_tail = AveragePooling2D((2, 2))(e_conv1_head)\n",
    "    e_conv1 = encoder_layer(e_conv1_tail, filters_base, 3, bn=False)\n",
    "\n",
    "    e_conv2_head = Conv2D(filters=filters_base*2, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv1)\n",
    "    e_conv2_tail = AveragePooling2D((2, 2))(e_conv2_head)\n",
    "    e_conv2 = encoder_layer(e_conv2_tail, filters_base*2, 3)\n",
    "\n",
    "    e_conv3_head = Conv2D(filters=filters_base*4, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv2)\n",
    "    e_conv3_tail = AveragePooling2D((2, 2))(e_conv3_head)\n",
    "    d_conv3_head = encoder_layer(e_conv3_tail, filters_base*4, 3)\n",
    "    resid1 = Add()([e_conv3_tail, d_conv3_head])\n",
    "    d_conv3_tail = UpSampling2D(size=(2, 2))(resid1)\n",
    "\n",
    "\n",
    "    d_conv4_head = decoder_layer(d_conv3_tail, e_conv3_head, filters_base*2, 3)\n",
    "    resid2 = Add()([d_conv4_head, e_conv2_tail])\n",
    "    d_conv4_tail = UpSampling2D(size=(2, 2))(resid2)\n",
    "\n",
    "\n",
    "    d_conv5_head = decoder_layer(d_conv4_tail, e_conv2_head, filters_base*1, 3)\n",
    "    resid3 = Add()([d_conv5_head, e_conv1_tail])\n",
    "    d_conv5_tail = UpSampling2D(size=(2, 2))(resid3)\n",
    "\n",
    "    d_conv6_head = decoder_layer(d_conv5_tail, e_conv1_head, filters_base//2, 3, bn=False)\n",
    "\n",
    "\n",
    "    outputs = Conv2D(1, 1, activation = 'relu', kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(d_conv6_head)\n",
    "\n",
    "    # Setup the model inputs / outputs\n",
    "    model = Model(inputs=history_traffic_volume, outputs=outputs)\n",
    "\n",
    "    # Compile the model RMSprop\n",
    "    model.compile(\n",
    "        optimizer = Adam(lr=learn_rate),\n",
    "#         loss='mse'\n",
    "        loss = loss_hole\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_masked = deepcopy(X_test)\n",
    "test_true_volume = deepcopy(X_test[:, :, :, :1])\n",
    "\n",
    "test_length = len(X_test)\n",
    "test_mask = np.stack([rand_mask for _ in range(test_length)], axis=0)\n",
    "\n",
    "test_traffic_mean = X_test[test_mask==1].mean()\n",
    "test_masked[test_mask==0] = test_traffic_mean\n",
    "test_history_volume = deepcopy(test_masked)\n",
    "\n",
    "rand_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_step = []\n",
    "l2_validation = []\n",
    "\n",
    "unet = build_unet()\n",
    "\n",
    "\n",
    "def train(train_matrix, epochs, batch_size=MAX_BATCH_SIZE, learn_rate=0.01):\n",
    "\n",
    "    min_mse = 999\n",
    "    start_time = datetime.now()\n",
    "    print(\"train start \"+str(start_time))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch>100 and epoch % 5 == 0 and epoch != 0:\n",
    "            unet_lr = K.get_value(unet.optimizer.lr)\n",
    "            if unet_lr>0.0001:\n",
    "                K.set_value(unet.optimizer.lr, unet_lr*0.9)\n",
    "                \n",
    "        for batch_i, (true_volume, history_volume) in enumerate(load_data(train_matrix,batch_size)):\n",
    "            # true_volume 真实待预测路网交通量  history_volume 路网交通量历史数据\n",
    "            #  训练 unet\n",
    "            #  训练 Generator\n",
    "            g_loss = unet.train_on_batch(history_volume, true_volume)\n",
    "\n",
    "\n",
    "        elapsed_time = datetime.now() - start_time\n",
    "        # Plot the progress\n",
    "        y_pred = unet.predict(test_history_volume)\n",
    "        \n",
    "        y_true = (1-test_mask[:,:,:,:1])*test_true_volume\n",
    "        y_pred = (1-test_mask[:,:,:,:1])*y_pred\n",
    "        l2_epoch_validation = l2(y_true, y_pred)\n",
    "        l1_epoch_validation = l1(y_true, y_pred)\n",
    "        \n",
    "        y_pred[y_true==0] += 1\n",
    "        y_true[y_true==0] += 1\n",
    "        mape_epoch_validation = mape(y_true, y_pred)\n",
    "        \n",
    "        if(l2_epoch_validation < min_mse and l2_epoch_validation<15):\n",
    "            unet.save_weights('./model/dataRecorvey20191109/tmp/min_runet.h5')\n",
    "            min_mse = l2_epoch_validation\n",
    "        \n",
    "        lr_step.append(K.get_value(unet.optimizer.lr))\n",
    "        l2_validation.append(l2_epoch_validation)\n",
    "        if epoch%1==0:\n",
    "#             print(\"unet lr:\"+ str(K.get_value(unet.optimizer.lr)))\n",
    "            print (\"[Epoch %d/%d]  [mse: %f] [mae: %f] [mape: %f] [G loss: %f] time: %s\" % (epoch+1, epochs,\n",
    "                                                                    l2_epoch_validation,\n",
    "                                                                    l1_epoch_validation,\n",
    "                                                                    mape_epoch_validation,\n",
    "                                                                    g_loss,\n",
    "                                                                    elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start 2019-11-11 22:03:20.188773\n",
      "[Epoch 1/200]  [mse: 38.231980] [mae: 26.510149] [mape: 22.277607] [G loss: 181942.046875] time: 0:00:56.103147\n",
      "[Epoch 2/200]  [mse: 30.433928] [mae: 21.724143] [mape: 19.279556] [G loss: 94479.046875] time: 0:01:16.894716\n",
      "[Epoch 3/200]  [mse: 23.753994] [mae: 16.962208] [mape: 15.315462] [G loss: 67826.320312] time: 0:01:35.367286\n",
      "[Epoch 4/200]  [mse: 21.090645] [mae: 15.141022] [mape: 12.666149] [G loss: 71689.437500] time: 0:01:53.780183\n",
      "[Epoch 5/200]  [mse: 18.738041] [mae: 13.350664] [mape: 11.209967] [G loss: 67281.687500] time: 0:02:12.292177\n",
      "[Epoch 6/200]  [mse: 17.813895] [mae: 12.653451] [mape: 10.112361] [G loss: 68773.296875] time: 0:02:30.824126\n",
      "[Epoch 7/200]  [mse: 17.411887] [mae: 12.389374] [mape: 9.959958] [G loss: 60989.355469] time: 0:02:49.321794\n",
      "[Epoch 8/200]  [mse: 16.584648] [mae: 11.788559] [mape: 9.491091] [G loss: 79874.664062] time: 0:03:07.795667\n",
      "[Epoch 9/200]  [mse: 16.395229] [mae: 11.622274] [mape: 9.467112] [G loss: 57914.507812] time: 0:03:26.260411\n",
      "[Epoch 10/200]  [mse: 15.481120] [mae: 10.852750] [mape: 8.405655] [G loss: 73602.070312] time: 0:03:44.766564\n",
      "[Epoch 11/200]  [mse: 14.547037] [mae: 10.198678] [mape: 8.676819] [G loss: 50689.484375] time: 0:04:03.447894\n",
      "[Epoch 12/200]  [mse: 16.465368] [mae: 11.389223] [mape: 7.803549] [G loss: 55513.156250] time: 0:04:35.678600\n",
      "[Epoch 13/200]  [mse: 13.651368] [mae: 9.516167] [mape: 7.624230] [G loss: 27929.730469] time: 0:04:54.142915\n",
      "[Epoch 14/200]  [mse: 13.797970] [mae: 9.568395] [mape: 7.249243] [G loss: 64060.195312] time: 0:05:13.271544\n",
      "[Epoch 15/200]  [mse: 16.172129] [mae: 11.283000] [mape: 7.301303] [G loss: 143495.734375] time: 0:05:32.257262\n",
      "[Epoch 16/200]  [mse: 14.334059] [mae: 10.024947] [mape: 7.409516] [G loss: 24744.771484] time: 0:05:51.560634\n",
      "[Epoch 17/200]  [mse: 13.350632] [mae: 9.229675] [mape: 6.781926] [G loss: 23272.183594] time: 0:06:10.374164\n",
      "[Epoch 18/200]  [mse: 12.726044] [mae: 8.746355] [mape: 6.562567] [G loss: 24163.125000] time: 0:06:29.876817\n",
      "[Epoch 19/200]  [mse: 13.019030] [mae: 9.048679] [mape: 7.195674] [G loss: 32402.527344] time: 0:06:50.450855\n",
      "[Epoch 20/200]  [mse: 12.556655] [mae: 8.680263] [mape: 6.701701] [G loss: 21973.609375] time: 0:07:10.020082\n",
      "[Epoch 21/200]  [mse: 12.494595] [mae: 8.567442] [mape: 6.144971] [G loss: 21725.765625] time: 0:07:29.374803\n",
      "[Epoch 22/200]  [mse: 12.917296] [mae: 8.842925] [mape: 6.136820] [G loss: 23563.017578] time: 0:07:49.894431\n",
      "[Epoch 23/200]  [mse: 12.274813] [mae: 8.451859] [mape: 6.262491] [G loss: 21663.884766] time: 0:08:08.743344\n",
      "[Epoch 24/200]  [mse: 12.693686] [mae: 8.775101] [mape: 6.711995] [G loss: 27489.539062] time: 0:08:27.582705\n",
      "[Epoch 25/200]  [mse: 12.165705] [mae: 8.315131] [mape: 5.971837] [G loss: 21161.312500] time: 0:08:46.032401\n",
      "[Epoch 26/200]  [mse: 12.764422] [mae: 8.734966] [mape: 6.145416] [G loss: 20751.142578] time: 0:09:04.817153\n",
      "[Epoch 27/200]  [mse: 12.347538] [mae: 8.462594] [mape: 5.696312] [G loss: 39277.429688] time: 0:09:23.282072\n",
      "[Epoch 28/200]  [mse: 14.984497] [mae: 10.333641] [mape: 7.532669] [G loss: 29800.919922] time: 0:09:41.695531\n",
      "[Epoch 29/200]  [mse: 12.564741] [mae: 8.567503] [mape: 5.882980] [G loss: 45075.042969] time: 0:10:00.076954\n",
      "[Epoch 30/200]  [mse: 14.687090] [mae: 10.213954] [mape: 6.531835] [G loss: 30205.164062] time: 0:10:18.576523\n",
      "[Epoch 31/200]  [mse: 11.922040] [mae: 8.142802] [mape: 5.611135] [G loss: 18310.886719] time: 0:10:37.032762\n",
      "[Epoch 32/200]  [mse: 11.931725] [mae: 8.078340] [mape: 5.379399] [G loss: 20897.007812] time: 0:10:55.944940\n",
      "[Epoch 33/200]  [mse: 13.027655] [mae: 8.815470] [mape: 5.889955] [G loss: 34124.699219] time: 0:11:14.784815\n",
      "[Epoch 34/200]  [mse: 15.659500] [mae: 10.922845] [mape: 6.627535] [G loss: 35743.492188] time: 0:11:33.512998\n",
      "[Epoch 35/200]  [mse: 12.000964] [mae: 8.168057] [mape: 5.610480] [G loss: 20997.460938] time: 0:11:52.280514\n",
      "[Epoch 36/200]  [mse: 14.176475] [mae: 9.794601] [mape: 6.137684] [G loss: 21762.753906] time: 0:12:10.943818\n",
      "[Epoch 37/200]  [mse: 11.970894] [mae: 8.096500] [mape: 5.301137] [G loss: 17995.197266] time: 0:12:29.575165\n",
      "[Epoch 38/200]  [mse: 12.334964] [mae: 8.353426] [mape: 5.415681] [G loss: 31619.107422] time: 0:12:48.547675\n",
      "[Epoch 39/200]  [mse: 13.884775] [mae: 9.547530] [mape: 5.979272] [G loss: 30396.666016] time: 0:13:07.788604\n",
      "[Epoch 40/200]  [mse: 12.484603] [mae: 8.497698] [mape: 5.755599] [G loss: 16667.476562] time: 0:13:27.083431\n",
      "[Epoch 41/200]  [mse: 13.246727] [mae: 8.951372] [mape: 5.771077] [G loss: 17436.699219] time: 0:13:46.132683\n",
      "[Epoch 42/200]  [mse: 13.927068] [mae: 9.297904] [mape: 5.734537] [G loss: 22867.541016] time: 0:14:06.207585\n",
      "[Epoch 43/200]  [mse: 19.533723] [mae: 13.808267] [mape: 8.065271] [G loss: 26686.521484] time: 0:14:25.623739\n",
      "[Epoch 44/200]  [mse: 22.108491] [mae: 15.088390] [mape: 8.715242] [G loss: 136533.093750] time: 0:14:45.676153\n",
      "[Epoch 45/200]  [mse: 20.780798] [mae: 14.223328] [mape: 7.599953] [G loss: 36417.007812] time: 0:15:05.197475\n",
      "[Epoch 46/200]  [mse: 13.041520] [mae: 8.917387] [mape: 5.672409] [G loss: 23000.119141] time: 0:15:24.553322\n",
      "[Epoch 47/200]  [mse: 11.546078] [mae: 7.813634] [mape: 5.144031] [G loss: 19447.697266] time: 0:15:44.055812\n",
      "[Epoch 48/200]  [mse: 11.524684] [mae: 7.831187] [mape: 5.040498] [G loss: 17599.238281] time: 0:16:04.151105\n",
      "[Epoch 49/200]  [mse: 24.731362] [mae: 18.079939] [mape: 9.079361] [G loss: 101888.625000] time: 0:16:25.023629\n",
      "[Epoch 50/200]  [mse: 18.143864] [mae: 12.772929] [mape: 7.240065] [G loss: 22788.947266] time: 0:16:44.633914\n",
      "[Epoch 51/200]  [mse: 15.409661] [mae: 10.516263] [mape: 6.030802] [G loss: 33145.804688] time: 0:17:04.676236\n",
      "[Epoch 52/200]  [mse: 11.298569] [mae: 7.642120] [mape: 4.983568] [G loss: 18614.335938] time: 0:17:24.280376\n",
      "[Epoch 53/200]  [mse: 13.783767] [mae: 9.430059] [mape: 5.913386] [G loss: 16512.640625] time: 0:17:44.220929\n",
      "[Epoch 54/200]  [mse: 18.502416] [mae: 12.629828] [mape: 6.813765] [G loss: 44750.144531] time: 0:18:03.828451\n",
      "[Epoch 55/200]  [mse: 20.248084] [mae: 14.590768] [mape: 7.970779] [G loss: 59811.968750] time: 0:18:23.763391\n",
      "[Epoch 56/200]  [mse: 16.263132] [mae: 11.230129] [mape: 6.205772] [G loss: 20011.585938] time: 0:18:43.294712\n",
      "[Epoch 57/200]  [mse: 14.894203] [mae: 10.203377] [mape: 5.552714] [G loss: 26443.023438] time: 0:19:02.867246\n",
      "[Epoch 58/200]  [mse: 15.538829] [mae: 10.832393] [mape: 5.947121] [G loss: 29302.615234] time: 0:19:22.878171\n",
      "[Epoch 59/200]  [mse: 16.749738] [mae: 11.837127] [mape: 6.979272] [G loss: 39982.835938] time: 0:19:42.208733\n",
      "[Epoch 60/200]  [mse: 15.303272] [mae: 10.839552] [mape: 6.533725] [G loss: 28225.632812] time: 0:20:01.428739\n",
      "[Epoch 61/200]  [mse: 18.910620] [mae: 12.760649] [mape: 6.943463] [G loss: 65602.406250] time: 0:20:20.913260\n",
      "[Epoch 62/200]  [mse: 14.626391] [mae: 10.144604] [mape: 6.399446] [G loss: 18922.007812] time: 0:20:40.226897\n",
      "[Epoch 63/200]  [mse: 13.085619] [mae: 8.853580] [mape: 5.473186] [G loss: 33812.097656] time: 0:20:59.600466\n",
      "[Epoch 64/200]  [mse: 12.633565] [mae: 8.512021] [mape: 4.908294] [G loss: 21819.013672] time: 0:21:19.276710\n",
      "[Epoch 65/200]  [mse: 12.681722] [mae: 8.583331] [mape: 5.245686] [G loss: 36815.046875] time: 0:21:39.213871\n",
      "[Epoch 66/200]  [mse: 24.021114] [mae: 16.625955] [mape: 7.489870] [G loss: 73777.796875] time: 0:21:59.871677\n",
      "[Epoch 67/200]  [mse: 13.815374] [mae: 9.439901] [mape: 5.844678] [G loss: 33275.277344] time: 0:22:19.815331\n",
      "[Epoch 68/200]  [mse: 13.008473] [mae: 8.717771] [mape: 5.058779] [G loss: 16397.283203] time: 0:22:39.548366\n",
      "[Epoch 69/200]  [mse: 15.400030] [mae: 10.529817] [mape: 5.572395] [G loss: 19701.011719] time: 0:22:58.835350\n",
      "[Epoch 70/200]  [mse: 24.766664] [mae: 17.476826] [mape: 7.722637] [G loss: 49622.550781] time: 0:23:18.562124\n",
      "[Epoch 71/200]  [mse: 16.840556] [mae: 11.127295] [mape: 5.782701] [G loss: 17299.542969] time: 0:23:38.001518\n",
      "[Epoch 72/200]  [mse: 19.421477] [mae: 13.361655] [mape: 6.414549] [G loss: 16898.574219] time: 0:23:57.745560\n",
      "[Epoch 73/200]  [mse: 12.188047] [mae: 8.158347] [mape: 5.053801] [G loss: 16808.322266] time: 0:24:17.280293\n",
      "[Epoch 74/200]  [mse: 14.960201] [mae: 10.242340] [mape: 5.591855] [G loss: 17872.746094] time: 0:24:36.861157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 75/200]  [mse: 12.794026] [mae: 8.602403] [mape: 5.300110] [G loss: 18945.701172] time: 0:24:58.204304\n",
      "[Epoch 76/200]  [mse: 12.688766] [mae: 8.360149] [mape: 4.706959] [G loss: 33439.738281] time: 0:25:23.174042\n",
      "[Epoch 77/200]  [mse: 13.334230] [mae: 9.066233] [mape: 5.381072] [G loss: 18901.455078] time: 0:25:44.008152\n",
      "[Epoch 78/200]  [mse: 26.635432] [mae: 18.954668] [mape: 8.682867] [G loss: 26719.994141] time: 0:26:04.876611\n",
      "[Epoch 79/200]  [mse: 18.563336] [mae: 12.535443] [mape: 6.805909] [G loss: 63705.449219] time: 0:26:25.590800\n",
      "[Epoch 80/200]  [mse: 21.887563] [mae: 15.482613] [mape: 7.052641] [G loss: 67171.484375] time: 0:26:46.373074\n",
      "[Epoch 81/200]  [mse: 10.966136] [mae: 7.356176] [mape: 4.665210] [G loss: 16558.339844] time: 0:27:07.106823\n",
      "[Epoch 82/200]  [mse: 12.982439] [mae: 8.820256] [mape: 5.221328] [G loss: 23890.775391] time: 0:27:28.915619\n",
      "[Epoch 83/200]  [mse: 12.608503] [mae: 8.486776] [mape: 5.209854] [G loss: 20869.880859] time: 0:27:49.662152\n",
      "[Epoch 84/200]  [mse: 20.999926] [mae: 14.310104] [mape: 6.907088] [G loss: 28532.822266] time: 0:28:10.398562\n",
      "[Epoch 85/200]  [mse: 20.368125] [mae: 14.318585] [mape: 7.679932] [G loss: 46095.632812] time: 0:28:31.074547\n",
      "[Epoch 86/200]  [mse: 30.052121] [mae: 20.833476] [mape: 8.214112] [G loss: 55910.160156] time: 0:28:51.666725\n",
      "[Epoch 87/200]  [mse: 33.362730] [mae: 23.713579] [mape: 9.620868] [G loss: 17012.464844] time: 0:29:12.301171\n",
      "[Epoch 88/200]  [mse: 10.778573] [mae: 7.194171] [mape: 4.325085] [G loss: 21756.263672] time: 0:29:33.178795\n",
      "[Epoch 89/200]  [mse: 10.681074] [mae: 7.167565] [mape: 4.370318] [G loss: 18738.876953] time: 0:29:54.126515\n",
      "[Epoch 90/200]  [mse: 10.591790] [mae: 7.026478] [mape: 4.222176] [G loss: 13640.029297] time: 0:30:14.529951\n",
      "[Epoch 91/200]  [mse: 13.224681] [mae: 8.926447] [mape: 4.777593] [G loss: 14883.013672] time: 0:30:33.664606\n",
      "[Epoch 92/200]  [mse: 12.259913] [mae: 8.331116] [mape: 4.831680] [G loss: 14799.217773] time: 0:30:52.283812\n",
      "[Epoch 93/200]  [mse: 12.700603] [mae: 8.633567] [mape: 5.263792] [G loss: 16872.474609] time: 0:31:12.116791\n",
      "[Epoch 94/200]  [mse: 13.218090] [mae: 8.822509] [mape: 5.134498] [G loss: 15059.997070] time: 0:31:32.721765\n",
      "[Epoch 95/200]  [mse: 14.193623] [mae: 9.716924] [mape: 5.806012] [G loss: 20118.939453] time: 0:31:53.347547\n",
      "[Epoch 96/200]  [mse: 24.404973] [mae: 17.009144] [mape: 7.564875] [G loss: 40577.113281] time: 0:32:14.335779\n",
      "[Epoch 97/200]  [mse: 17.244404] [mae: 11.416913] [mape: 5.609992] [G loss: 31829.275391] time: 0:32:35.227371\n",
      "[Epoch 98/200]  [mse: 10.921858] [mae: 7.208559] [mape: 4.259512] [G loss: 22934.882812] time: 0:32:55.711584\n",
      "[Epoch 99/200]  [mse: 14.176878] [mae: 9.413653] [mape: 4.760880] [G loss: 18327.248047] time: 0:33:14.533906\n",
      "[Epoch 100/200]  [mse: 12.692169] [mae: 8.368970] [mape: 4.663886] [G loss: 28722.873047] time: 0:33:33.407433\n",
      "[Epoch 101/200]  [mse: 18.271070] [mae: 12.521471] [mape: 5.911346] [G loss: 42364.027344] time: 0:33:52.503366\n",
      "[Epoch 102/200]  [mse: 18.165952] [mae: 12.689578] [mape: 6.285202] [G loss: 27920.652344] time: 0:34:11.585337\n",
      "[Epoch 103/200]  [mse: 12.579750] [mae: 8.368797] [mape: 5.055877] [G loss: 23913.783203] time: 0:34:30.588017\n",
      "[Epoch 104/200]  [mse: 19.113066] [mae: 13.028765] [mape: 6.172068] [G loss: 19557.791016] time: 0:34:49.729828\n",
      "[Epoch 105/200]  [mse: 15.982160] [mae: 10.597544] [mape: 5.041329] [G loss: 19613.890625] time: 0:35:09.002332\n",
      "[Epoch 106/200]  [mse: 12.776527] [mae: 8.720920] [mape: 5.040909] [G loss: 33947.218750] time: 0:35:34.743534\n",
      "[Epoch 107/200]  [mse: 13.784602] [mae: 9.389741] [mape: 5.407149] [G loss: 33580.609375] time: 0:35:55.385962\n",
      "[Epoch 108/200]  [mse: 14.653684] [mae: 10.226551] [mape: 5.715645] [G loss: 26313.621094] time: 0:36:16.047996\n",
      "[Epoch 109/200]  [mse: 12.007046] [mae: 8.190245] [mape: 4.824579] [G loss: 20537.474609] time: 0:36:36.699528\n",
      "[Epoch 110/200]  [mse: 13.273133] [mae: 8.710693] [mape: 4.846466] [G loss: 19507.513672] time: 0:36:57.576687\n",
      "[Epoch 111/200]  [mse: 12.927490] [mae: 8.579809] [mape: 5.038261] [G loss: 14384.649414] time: 0:37:18.022598\n",
      "[Epoch 112/200]  [mse: 11.653594] [mae: 7.779513] [mape: 4.785595] [G loss: 16612.894531] time: 0:37:37.255431\n",
      "[Epoch 113/200]  [mse: 11.239483] [mae: 7.557086] [mape: 4.885408] [G loss: 16704.710938] time: 0:37:58.404382\n",
      "[Epoch 114/200]  [mse: 12.097753] [mae: 8.042465] [mape: 4.729353] [G loss: 18246.244141] time: 0:38:19.050478\n",
      "[Epoch 115/200]  [mse: 13.797208] [mae: 9.135755] [mape: 4.861776] [G loss: 14267.902344] time: 0:38:39.898566\n",
      "[Epoch 116/200]  [mse: 14.233202] [mae: 9.369091] [mape: 5.123852] [G loss: 21678.580078] time: 0:39:00.554320\n",
      "[Epoch 117/200]  [mse: 12.932414] [mae: 8.694540] [mape: 5.133070] [G loss: 30491.171875] time: 0:39:21.167658\n",
      "[Epoch 118/200]  [mse: 15.247917] [mae: 10.384467] [mape: 5.458258] [G loss: 23210.275391] time: 0:39:41.867793\n",
      "[Epoch 119/200]  [mse: 13.706892] [mae: 9.116739] [mape: 4.854985] [G loss: 15769.696289] time: 0:40:02.536776\n",
      "[Epoch 120/200]  [mse: 10.864170] [mae: 7.164907] [mape: 4.262029] [G loss: 12497.023438] time: 0:40:23.544056\n",
      "[Epoch 121/200]  [mse: 9.945487] [mae: 6.635189] [mape: 4.101078] [G loss: 13579.403320] time: 0:40:44.162774\n",
      "[Epoch 122/200]  [mse: 9.890405] [mae: 6.562079] [mape: 4.010182] [G loss: 13425.610352] time: 0:41:05.175548\n",
      "[Epoch 123/200]  [mse: 10.017074] [mae: 6.596745] [mape: 3.932828] [G loss: 12702.428711] time: 0:41:26.327982\n",
      "[Epoch 124/200]  [mse: 10.207400] [mae: 6.677317] [mape: 3.920740] [G loss: 12383.651367] time: 0:41:47.103898\n",
      "[Epoch 125/200]  [mse: 10.411166] [mae: 6.776575] [mape: 3.971726] [G loss: 11535.265625] time: 0:42:07.747924\n",
      "[Epoch 126/200]  [mse: 10.189440] [mae: 6.694508] [mape: 3.976277] [G loss: 14041.061523] time: 0:42:26.912142\n",
      "[Epoch 127/200]  [mse: 10.196283] [mae: 6.738347] [mape: 4.087080] [G loss: 12226.607422] time: 0:42:45.405218\n",
      "[Epoch 128/200]  [mse: 11.879296] [mae: 7.855748] [mape: 4.437281] [G loss: 12019.317383] time: 0:43:04.063321\n",
      "[Epoch 129/200]  [mse: 18.571201] [mae: 12.829896] [mape: 6.098683] [G loss: 28613.050781] time: 0:43:22.619697\n",
      "[Epoch 130/200]  [mse: 13.683973] [mae: 8.850057] [mape: 4.620712] [G loss: 12388.813477] time: 0:43:42.821192\n",
      "[Epoch 131/200]  [mse: 10.647314] [mae: 7.119206] [mape: 4.199159] [G loss: 12434.480469] time: 0:44:03.484691\n",
      "[Epoch 132/200]  [mse: 10.842729] [mae: 7.258835] [mape: 4.271647] [G loss: 14385.753906] time: 0:44:24.048340\n",
      "[Epoch 133/200]  [mse: 10.457306] [mae: 6.905390] [mape: 4.129488] [G loss: 11823.030273] time: 0:44:44.761645\n",
      "[Epoch 134/200]  [mse: 11.146629] [mae: 7.233354] [mape: 4.197442] [G loss: 11467.594727] time: 0:45:05.460611\n",
      "[Epoch 135/200]  [mse: 14.948764] [mae: 9.862263] [mape: 4.884004] [G loss: 13658.309570] time: 0:45:26.290747\n",
      "[Epoch 136/200]  [mse: 10.434594] [mae: 6.931494] [mape: 4.169373] [G loss: 12923.099609] time: 0:45:46.973263\n",
      "[Epoch 137/200]  [mse: 10.617917] [mae: 6.883733] [mape: 4.056987] [G loss: 10845.653320] time: 0:46:07.743204\n",
      "[Epoch 138/200]  [mse: 10.410101] [mae: 6.818768] [mape: 4.139764] [G loss: 11145.434570] time: 0:46:28.492350\n",
      "[Epoch 139/200]  [mse: 10.400258] [mae: 6.845148] [mape: 4.168077] [G loss: 12032.522461] time: 0:46:49.253318\n",
      "[Epoch 140/200]  [mse: 10.489880] [mae: 6.865437] [mape: 4.135419] [G loss: 11654.492188] time: 0:47:09.984817\n",
      "[Epoch 141/200]  [mse: 10.487792] [mae: 6.848522] [mape: 4.113289] [G loss: 11278.736328] time: 0:47:30.695115\n",
      "[Epoch 142/200]  [mse: 10.453017] [mae: 6.824185] [mape: 4.081233] [G loss: 10956.509766] time: 0:47:51.366326\n",
      "[Epoch 143/200]  [mse: 10.532432] [mae: 6.862179] [mape: 4.077836] [G loss: 10702.620117] time: 0:48:12.111447\n",
      "[Epoch 144/200]  [mse: 10.516841] [mae: 6.842226] [mape: 4.047065] [G loss: 10149.384766] time: 0:48:32.931608\n",
      "[Epoch 145/200]  [mse: 10.546164] [mae: 6.879201] [mape: 4.029825] [G loss: 10430.835938] time: 0:48:53.619219\n",
      "[Epoch 146/200]  [mse: 11.027868] [mae: 7.140058] [mape: 4.164859] [G loss: 9487.032227] time: 0:49:14.373138\n",
      "[Epoch 147/200]  [mse: 11.002779] [mae: 7.198372] [mape: 4.241941] [G loss: 10280.454102] time: 0:49:35.095732\n",
      "[Epoch 148/200]  [mse: 10.338590] [mae: 6.751004] [mape: 4.094368] [G loss: 12134.369141] time: 0:49:54.757314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 149/200]  [mse: 11.003828] [mae: 7.270146] [mape: 4.268644] [G loss: 18256.941406] time: 0:50:13.759995\n",
      "[Epoch 150/200]  [mse: 10.977634] [mae: 7.134495] [mape: 4.120059] [G loss: 10037.280273] time: 0:50:32.714255\n",
      "[Epoch 151/200]  [mse: 11.115729] [mae: 7.263520] [mape: 4.266387] [G loss: 10786.852539] time: 0:50:51.719417\n",
      "[Epoch 152/200]  [mse: 11.695554] [mae: 7.559102] [mape: 4.271784] [G loss: 11047.438477] time: 0:51:10.248895\n",
      "[Epoch 153/200]  [mse: 12.069567] [mae: 7.777037] [mape: 4.377955] [G loss: 10296.895508] time: 0:51:28.769272\n",
      "[Epoch 154/200]  [mse: 11.308093] [mae: 7.405395] [mape: 4.424597] [G loss: 13185.836914] time: 0:51:47.625072\n",
      "[Epoch 155/200]  [mse: 11.623846] [mae: 7.650243] [mape: 4.662553] [G loss: 13838.763672] time: 0:52:06.597392\n",
      "[Epoch 156/200]  [mse: 12.366989] [mae: 8.384646] [mape: 5.220937] [G loss: 19405.816406] time: 0:52:26.478076\n",
      "[Epoch 157/200]  [mse: 12.919995] [mae: 8.835984] [mape: 5.667689] [G loss: 14317.713867] time: 0:52:45.633026\n",
      "[Epoch 158/200]  [mse: 12.439352] [mae: 8.243086] [mape: 4.994629] [G loss: 11015.271484] time: 0:53:04.712134\n",
      "[Epoch 159/200]  [mse: 12.533215] [mae: 8.031927] [mape: 4.504992] [G loss: 9692.232422] time: 0:53:23.683797\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-172-3f03ee3fa4c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_BATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-171-c11b05144ed4>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_matrix, epochs, batch_size, learn_rate)\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munet_lr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrue_volume\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_volume\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[1;31m# true_volume 真实待预测路网交通量  history_volume 路网交通量历史数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;31m#  训练 unet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-167-62be4f798e60>\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(volume_matrix, batch_size)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mtrue_volume\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# mask==1代表有效采集点，0代表待预测采集点\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mtraffic_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmasked\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;31m# 待预测点的值用已知值的平均值初始化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mmasked\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraffic_mean\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(X_train, epochs=200, batch_size=MAX_BATCH_SIZE, learn_rate=learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet.save_weights('./model/dataRecorvey20191109/runet_10_rmse11.97.h5')\n",
    "# unet.load_weights('./model/RUnet/unet_60epoch_18rmse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_masked_tmp = deepcopy(X_test)\n",
    "# test_true_volume_tmp = deepcopy(X_test[:, :, :, :1])\n",
    "\n",
    "# test_length_tmp = len(X_test)\n",
    "# rand_mask_tmp = random_mask(matrix_length, matrix_length, size=rand_size, channels=channel_num, smooth_time=smooth_time, type=mask_type, block_size=block_size)\n",
    "# test_mask_tmp = np.stack([rand_mask_tmp for _ in range(test_length_tmp)], axis=0)\n",
    "\n",
    "# test_traffic_mean_tmp = X_test[test_mask_tmp==1].mean()\n",
    "# test_masked_tmp[test_mask_tmp==0] = test_traffic_mean_tmp\n",
    "# test_history_volume_tmp = deepcopy(test_masked_tmp)\n",
    "\n",
    "\n",
    "# y_pred = unet.predict(test_history_volume_tmp)\n",
    "\n",
    "# # 仅对缺失数据进行l2评价。（对预测来说既对第一层进行评价，验证）\n",
    "# y_true = (1-test_mask[:,:,:,:1])*test_true_volume_tmp\n",
    "# y_pred = (1-test_mask[:,:,:,:1])*y_pred\n",
    "\n",
    "\n",
    "y_pred = unet.predict(test_history_volume)\n",
    "\n",
    "# 仅对缺失数据进行l2评价。（对预测来说既对第一层进行评价，验证）\n",
    "y_true = (1-test_mask[:,:,:,:1])*test_true_volume\n",
    "y_pred = (1-test_mask[:,:,:,:1])*y_pred\n",
    "\n",
    "l2(y_true, y_pred), l1(y_true, y_pred)\n",
    "# (13.612251463372885, 7.896321495663051)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_true==0] += 1\n",
    "y_true[y_true==0] += 1\n",
    "\n",
    "mape(y_true, y_pred)\n",
    "# 5.013244341615349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_model = build_unet()\n",
    "min_model.load_weights('./model/dataRecorvey20191109/tmp/min_runet.h5')\n",
    "\n",
    "y_pred = min_model.predict(test_history_volume)\n",
    "\n",
    "# 仅对缺失数据进行l2评价。（对预测来说既对第一层进行评价，验证）\n",
    "y_true = (1-test_mask[:,:,:,:1])*test_true_volume\n",
    "y_pred = (1-test_mask[:,:,:,:1])*y_pred\n",
    "\n",
    "l2(y_true, y_pred), l1(y_true, y_pred)\n",
    "# (11.085728026573435, 6.957813774885677)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_true==0] += 1\n",
    "y_true[y_true==0] += 1\n",
    "\n",
    "mape(y_true, y_pred)\n",
    "# 4.429743492502313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posX = 0\n",
    "posY = 2\n",
    "startX = 500\n",
    "gapX = 200\n",
    "\n",
    "y = test_true_volume[:, posX, posY, :][startX: startX+gapX]\n",
    "\n",
    "yf = y_pred[:, posX, posY, :][startX: startX+gapX]\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "fig, ax = plt.subplots(figsize=(25, 8))\n",
    "lines = plt.plot(x, yf, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pic_matrix =y_pred[170].reshape(32, 32, 1)\n",
    "true_matrix =y_true[170].reshape(32, 32, 1)\n",
    "plt.imshow(pic_matrix[:,:,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_shape = pic_matrix.shape\n",
    "# Load mask\n",
    "# mask = random_mask(pic_shape[0], pic_shape[1], size=0.3)\n",
    "# mask = random_mask(pic_shape[0], pic_shape[1], size=0.9, type='block', block_size=(25, 25))\n",
    "\n",
    "# Image + mask\n",
    "masked_img = deepcopy(pic_matrix)\n",
    "# masked_img = (pic_matrix - pic_matrix.min())/(pic_matrix.max() - pic_matrix.min())*255\n",
    "masked_img[rand_mask[:,:,0]==0] = 0\n",
    "\n",
    "# Show side by side\n",
    "_, axes = plt.subplots(1, 4, figsize=(25, 12))\n",
    "axes[0].imshow(true_matrix[:,:,0])\n",
    "axes[1].imshow(pic_matrix[:,:,0])\n",
    "axes[2].imshow(rand_mask[:,:,0]*255, cmap ='gray')\n",
    "axes[3].imshow(masked_img[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start_time = 1600\n",
    "y = y_true.reshape(-1,)[start_time: start_time+100]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[start_time: start_time+100]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 10))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi = l2_validation\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "lines = plt.plot(xi, yi, 'k^--', linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lr_step\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "lines = plt.plot(x, y, 'ko-', linewidth=1, markersize=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 3600\n",
    "y = y_true.reshape(-1,)[start_time: start_time+100]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[start_time: start_time+100]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 0\n",
    "y = y_true.reshape(-1,)[start_time: start_time+100]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[start_time: start_time+100]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_volume"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
