{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, TensorBoard\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from datetime import datetime\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import Input, Conv2D, UpSampling2D, Dropout, LeakyReLU, BatchNormalization, Activation, Add, Subtract\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "# from keras.applications import VGG16\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "# from libs.pconv_model_UNet import PConvUnet\n",
    "from keras.models import load_model  \n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# Settings\n",
    "MAX_BATCH_SIZE = 64\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "matrix_length = 32\n",
    "\n",
    "matrix_df = pd.read_csv('./data/trafficV_M.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mask(height=32, width=32, size=1024, channels=1, smooth_time=0, type='rand', block_size=(32, 32)):\n",
    "    \"\"\"Generates a random irregular mask with lines, circles and elipses\n",
    "       channels: time segment numbers of road net\n",
    "       smooth_time: no block time segment numbers\n",
    "    \"\"\" \n",
    "    # channels of block time segment   channels-smooth_time需要mask的channels层数\n",
    "    img = np.zeros((height, width, channels-smooth_time), np.uint8)\n",
    "\n",
    "    # Set size scale\n",
    "    if size<=1:\n",
    "        size = int((width * height) * size)\n",
    "    else:\n",
    "        size = size\n",
    "    \n",
    "    if width < 16 or height < 16:\n",
    "        raise Exception(\"Width and Height of mask must be at least 16!\")\n",
    "    if len(block_size)!=2:\n",
    "        raise Exception(\"block size unmatch!\")\n",
    "    if block_size[0]>height or block_size[1]>width:\n",
    "        raise Exception(\"block size are too big\")\n",
    "    \n",
    "    if type=='rand':\n",
    "        # 不放回的在height*width中选取size个元素作为mask的缺失点\n",
    "        elements = np.random.choice(height*width, size, replace=False)\n",
    "        coordinate_map = map(lambda x: np.unravel_index(x, (height, width)), elements)\n",
    "        def f(x, y):\n",
    "            img[x][y]=1\n",
    "\n",
    "        list(map(lambda x: f(x[0],x[1]), list(coordinate_map)))\n",
    "    elif type=='block':\n",
    "        block_height = block_size[0]\n",
    "        block_width = block_size[1]\n",
    "        height = height\n",
    "        width = width\n",
    "\n",
    "        # element = randint(1, height*width-1)\n",
    "        # element_coordinate = np.unravel_index(element, (height, width))\n",
    "        # element_coordinate_x = element_coordinate[0]\n",
    "        # element_coordinate_y = element_coordinate[1]\n",
    "        # 选取blockMask的左上角为基点，防止数组越界\n",
    "        element_coordinate_x = randint(0, height-block_height)\n",
    "        element_coordinate_y = randint(0, width-block_width)\n",
    "\n",
    "        while block_height > 0:\n",
    "            while block_width > 0:\n",
    "                img[element_coordinate_x+block_height-1][element_coordinate_y+block_width-1] = 1\n",
    "                block_width = block_width -1\n",
    "            block_width = block_size[1]\n",
    "            block_height = block_height - 1\n",
    "            \n",
    "    # smooth_time为参考历史交通数据无缺失数据的时间点数\n",
    "    img_tmp = np.zeros((height, width, smooth_time), np.uint8)\n",
    "    img = np.concatenate((img, img_tmp), axis=2)\n",
    "\n",
    "    return 1-img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createTrainArray(week_history_num=0, minute_history_num=0):\n",
    "    week_delta_list = [pd.Timedelta(i+1, unit='W') for i in range(week_history_num)]\n",
    "    minute_delta_list = [pd.Timedelta((i+1)*15, unit='m') for i in range(minute_history_num)]\n",
    "    # 参考历史数据时间点list\n",
    "    delta_list = week_delta_list+minute_delta_list\n",
    "    print(delta_list)\n",
    "    \n",
    "    set_up_time = pd.Timedelta(week_history_num, unit='W')\n",
    "    # 根据历史数据选取多少，重新构建数据集\n",
    "    # 相当于去除最开始week_history_num个周的数据，因为这些数据无法找到更前的数据\n",
    "    train_df = matrix_df.truncate(before=matrix_df.index.min() + set_up_time)\n",
    "    \n",
    "    train_ago_array_tuple = tuple([np.array(matrix_df.loc[train_df.index - i]).reshape(-1, matrix_length, matrix_length, 1) for i in delta_list])\n",
    "    train_df = np.array(train_df).reshape(-1, matrix_length, matrix_length, 1)\n",
    "    # concatenate保持 待修复数据在前，参考历史数据在后。与random_mask函数生成mask相一致\n",
    "    train_array = np.concatenate((train_df,)+train_ago_array_tuple, axis=3)\n",
    "    print(train_array.shape)\n",
    "    return train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timedelta('7 days 00:00:00'), Timedelta('14 days 00:00:00'), Timedelta('0 days 00:15:00'), Timedelta('0 days 00:30:00'), Timedelta('0 days 00:45:00')]\n",
      "(16032, 32, 32, 6)\n"
     ]
    }
   ],
   "source": [
    "week_history_num = 2\n",
    "minute_history_num = 3\n",
    "\n",
    "channel_num = week_history_num +minute_history_num +1\n",
    "smooth_time = channel_num-1\n",
    "\n",
    "train_array = createTrainArray(week_history_num, minute_history_num)\n",
    "X_train, X_test = train_test_split(train_array, test_size = 0.1, shuffle=False)\n",
    "# X_train, X_test = train_test_split(train_array, test_size = 0.1, random_state=42, shuffle=False)\n",
    "\n",
    "# X_train = train_array[:16704-900-900]\n",
    "# X_val = train_array[16704-900-900:16704-900]\n",
    "# X_test = train_array[16704-900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_shape = (matrix_length, matrix_length, channel_num)\n",
    "true_volume_shape = (matrix_length, matrix_length, 1)\n",
    "history_volume_shape = (matrix_length, matrix_length, channel_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 25)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_steps = X_train.shape[0] // MAX_BATCH_SIZE\n",
    "val_steps = X_test.shape[0] // MAX_BATCH_SIZE\n",
    "epoch_steps, val_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以第一数据为例. 第一列为待预测数据\n",
    "# 第一例：1.15 0:00  二：1.8 0:00  三：1.1 0:00  四：1.14 23:45  五：1.14 23:30  六：1.14 23:15\n",
    "# X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_size = 1024\n",
    "mask_type = 'rand'\n",
    "block_size = (32, 32)\n",
    "\n",
    "# 单个矩阵mask\n",
    "rand_mask = random_mask(matrix_length, matrix_length, size=rand_size, channels=channel_num, smooth_time=smooth_time, type=mask_type, block_size=block_size)\n",
    "# 堆叠成多个mask，方便对batch数据进行处理\n",
    "mask = np.stack([rand_mask for _ in range(MAX_BATCH_SIZE)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def l2(y_true, y_pred):\n",
    "    size = 0\n",
    "    if rand_size<=1:\n",
    "        size = int((matrix_length * matrix_length) * rand_size)\n",
    "    else:\n",
    "        size = rand_size\n",
    "        \n",
    "    if size == 0:\n",
    "        raise Exception(\"size == 0\")\n",
    "    return math.sqrt(np.sum(np.mean(np.square(y_true - y_pred), axis=0))/size)\n",
    "\n",
    "def l1(y_true, y_pred):\n",
    "    size = 0\n",
    "    if rand_size<=1:\n",
    "        size = int((matrix_length * matrix_length) * rand_size)\n",
    "    else:\n",
    "        size = rand_size\n",
    "        \n",
    "    if size == 0:\n",
    "        raise Exception(\"size == 0\")\n",
    "    return np.sum(np.mean(np.abs(y_true - y_pred), axis=0))/size\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    size = 0\n",
    "    if rand_size<=1:\n",
    "        size = int((matrix_length * matrix_length) * rand_size)\n",
    "    else:\n",
    "        size = rand_size\n",
    "        \n",
    "    if size == 0:\n",
    "        raise Exception(\"size == 0\")\n",
    "        \n",
    "    return np.sum(np.mean((np.abs(y_true - y_pred)/y_true)*100, axis=0))/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "def load_data(volume_matrix, batch_size=MAX_BATCH_SIZE):\n",
    "    n_batches=batch_size\n",
    "    len_of_matrix = len(volume_matrix)\n",
    "\n",
    "    batch_i = 0\n",
    "    while ((batch_i+1)*batch_size < len_of_matrix):\n",
    "        batch_matrix = volume_matrix[batch_i*batch_size: (batch_i+1)*batch_size]\n",
    "        masked = deepcopy(batch_matrix)\n",
    "        # true_volume为待修复数据， history_volume为历史数据及当前残差待修复数据\n",
    "        true_volume = deepcopy(batch_matrix[:, :, :, :1])\n",
    "        # mask==1代表有效采集点，0代表待预测采集点\n",
    "        traffic_mean = masked[mask==1].mean()\n",
    "        # 待预测点的值用已知值的平均值初始化\n",
    "        masked[mask==0] = traffic_mean\n",
    "        history_volume = deepcopy(masked)\n",
    "        \n",
    "        batch_i+=1\n",
    "\n",
    "        yield true_volume, history_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(y_true, y_pred):\n",
    "        \"\"\"Calculate the L1 loss used in all loss calculations\"\"\"\n",
    "        if K.ndim(y_true) == 4:\n",
    "            return K.sum(K.square(y_pred - y_true), axis=[1,2,3])\n",
    "        elif K.ndim(y_true) == 3:\n",
    "            return K.sum(K.square(y_pred - y_true), axis=[1,2])\n",
    "        else:\n",
    "            raise NotImplementedError(\"Calculating L1 loss on 1D tensors? should not occur for this network\")\n",
    "\n",
    "# 缺失点mse\n",
    "def loss_hole(y_true, y_pred):\n",
    "    return l2_loss((1-mask) * y_true, (1-mask) * y_pred)\n",
    "\n",
    "# 非缺失点mse\n",
    "def loss_bg(y_true, y_pred):\n",
    "    return l2_loss(mask * y_true, mask * y_pred)\n",
    "\n",
    "def loss_fuc(y_true, y_pred):\n",
    "    return loss_hole(y_true, y_pred)*5 + loss_bg(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_init = 'glorot_uniform'\n",
    "bias_init = 'zeros'\n",
    "\n",
    "# kernel_init = initializers.he_uniform()\n",
    "# bias_init = initializers.he_uniform()\n",
    "kernel_regul = regularizers.l2(1)\n",
    "activity_regul = regularizers.l2(1)\n",
    "\n",
    "learn_rate = 0.004\n",
    "\n",
    "# ENCODER\n",
    "def encoder_layer(img_in, filters, kernel_size, bn=True, resid=True):\n",
    "    # conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding='same')(img_in)\n",
    "    conv = Conv2D(filters, (kernel_size, kernel_size), padding=\"same\",\n",
    "       strides=1,kernel_initializer='glorot_uniform')(img_in)\n",
    "    if bn:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "\n",
    "    conv = Conv2D(filters, (kernel_size, kernel_size), padding=\"same\",\n",
    "       strides=1,kernel_initializer='glorot_uniform')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    return conv\n",
    "\n",
    "# DECODER\n",
    "def decoder_layer(img_in, filters, kernel_size, bn=True, resid=True):\n",
    "    conv = Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(img_in)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "\n",
    "    conv = Conv2D(filters=filters//2, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "\n",
    "    conv = UpSampling2D(size = (2,2))(conv)\n",
    "    return conv\n",
    "\n",
    "\n",
    "def build_unet(): \n",
    "    history_traffic_volume = Input(shape=history_volume_shape)\n",
    "    \n",
    "    conv1 = encoder_layer(history_traffic_volume, 32, 3, bn=False)\n",
    "    pool1 = AveragePooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = encoder_layer(pool1, 64, 3, bn=True)\n",
    "    pool2 = AveragePooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = encoder_layer(pool2, 128, 3, bn=True)\n",
    "    pool3 = AveragePooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    #         conv4 = encoder_layer(pool3, 256, 3, bn=True)\n",
    "    #         pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = decoder_layer(pool3, 256, 3, bn=True)\n",
    "    merge1 = Concatenate()([conv3,conv5])\n",
    "\n",
    "    conv6 = decoder_layer(merge1, 128, 3, bn=True)\n",
    "    merge2 = Concatenate()([conv2,conv6])\n",
    "\n",
    "    conv7 = decoder_layer(merge2, 64, 3, bn=True)\n",
    "    merge3 = Concatenate()([conv1,conv7])\n",
    "\n",
    "    #         conv8 = decoder_layer(merge3, 32, 3, bn=True)\n",
    "    #         merge4 = Concatenate()([conv1,conv8])\n",
    "\n",
    "    conv9 = encoder_layer(merge3, 32, 3, bn=False)\n",
    "\n",
    "\n",
    "    model_output =  Conv2D(1, 1, activation = 'relu', kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(conv9)\n",
    "\n",
    "    # Setup the model inputs / outputs\n",
    "    model = Model(inputs=history_traffic_volume, outputs=model_output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer = Adam(lr=learn_rate),\n",
    "        loss=loss_hole\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_masked = deepcopy(X_test)\n",
    "test_true_volume = deepcopy(X_test[:, :, :, :1])\n",
    "\n",
    "test_length = len(X_test)\n",
    "test_mask = np.stack([rand_mask for _ in range(test_length)], axis=0)\n",
    "\n",
    "test_traffic_mean = X_test[test_mask==1].mean()\n",
    "test_masked[test_mask==0] = test_traffic_mean\n",
    "test_history_volume = deepcopy(test_masked)\n",
    "\n",
    "rand_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_step = []\n",
    "l2_validation = []\n",
    "\n",
    "unet = build_unet()\n",
    "\n",
    "\n",
    "def train(train_matrix, epochs, batch_size=MAX_BATCH_SIZE, learn_rate=0.01):\n",
    "\n",
    "    min_mse = 999\n",
    "    start_time = datetime.now()\n",
    "    print(\"train start \"+str(start_time))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "#         if epoch>=100 and epoch % 5 == 0 and epoch != 0:\n",
    "#             unet_lr = K.get_value(unet.optimizer.lr)\n",
    "#             if unet_lr>0.002:\n",
    "#                 K.set_value(unet.optimizer.lr, unet_lr*0.9)\n",
    "                \n",
    "        for batch_i, (true_volume, history_volume) in enumerate(load_data(train_matrix,batch_size)):\n",
    "            # true_volume 真实待预测路网交通量  history_volume 路网交通量历史数据\n",
    "            #  训练 unet\n",
    "            #  训练 Generator\n",
    "            g_loss = unet.train_on_batch(history_volume, true_volume)\n",
    "\n",
    "\n",
    "        elapsed_time = datetime.now() - start_time\n",
    "        # Plot the progress\n",
    "        y_pred = unet.predict(test_history_volume)\n",
    "        \n",
    "        y_true = (1-test_mask[:,:,:,:1])*test_true_volume\n",
    "        y_pred = (1-test_mask[:,:,:,:1])*y_pred\n",
    "        l2_epoch_validation = l2(y_true, y_pred)\n",
    "        l1_epoch_validation = l1(y_true, y_pred)\n",
    "        \n",
    "        y_pred[y_true==0] += 1\n",
    "        y_true[y_true==0] += 1\n",
    "        mape_epoch_validation = mape(y_true, y_pred)\n",
    "        \n",
    "        if(l2_epoch_validation < min_mse and l2_epoch_validation<20):\n",
    "            unet.save_weights('./model/dataRecorvey20191109/tmp/min_unet.h5')\n",
    "            min_mse = l2_epoch_validation\n",
    "        \n",
    "        lr_step.append(K.get_value(unet.optimizer.lr))\n",
    "        l2_validation.append(l2_epoch_validation)\n",
    "        if epoch%1==0:\n",
    "#             print(\"unet lr:\"+ str(K.get_value(unet.optimizer.lr)))\n",
    "            print (\"[Epoch %d/%d]  [mse: %f] [mae: %f] [mape: %f] [G loss: %f] time: %s\" % (epoch+1, epochs,\n",
    "                                                                    l2_epoch_validation,\n",
    "                                                                    l1_epoch_validation,\n",
    "                                                                    mape_epoch_validation,\n",
    "                                                                    g_loss,\n",
    "                                                                    elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train(X_train, epochs=200, batch_size=MAX_BATCH_SIZE, learn_rate=learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet.save_weights('./model/dataRecorvey20191109/runet_10_rmse11.97.h5')\n",
    "# unet.load_weights('./model/dataRecorvey20191109/unet_1024.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-911b1e3a4817>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# y_pred = (1-test_mask[:,:,:,:1])*y_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0ml2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "# test_masked_tmp = deepcopy(X_test)\n",
    "# test_true_volume_tmp = deepcopy(X_test[:, :, :, :1])\n",
    "\n",
    "# test_length_tmp = len(X_test)\n",
    "# rand_mask_tmp = random_mask(matrix_length, matrix_length, size=rand_size, channels=channel_num, smooth_time=smooth_time, type=mask_type, block_size=block_size)\n",
    "# test_mask_tmp = np.stack([rand_mask_tmp for _ in range(test_length_tmp)], axis=0)\n",
    "\n",
    "# test_traffic_mean_tmp = X_test[test_mask_tmp==1].mean()\n",
    "# test_masked_tmp[test_mask_tmp==0] = test_traffic_mean_tmp\n",
    "# test_history_volume_tmp = deepcopy(test_masked_tmp)\n",
    "\n",
    "\n",
    "# y_pred = unet.predict(test_history_volume_tmp)\n",
    "\n",
    "# # 仅对缺失数据进行l2评价。（对预测来说既对第一层进行评价，验证）\n",
    "# y_true = (1-test_mask[:,:,:,:1])*test_true_volume_tmp\n",
    "# y_pred = (1-test_mask[:,:,:,:1])*y_pred\n",
    "\n",
    "\n",
    "y_pred = unet.predict(test_history_volume)\n",
    "\n",
    "# 仅对缺失数据进行l2评价。（对预测来说既对第一层进行评价，验证）\n",
    "# y_true = (1-test_mask[:,:,:,:1])*test_true_volume\n",
    "# y_pred = (1-test_mask[:,:,:,:1])*y_pred\n",
    "\n",
    "l2(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel_init = 'glorot_uniform'\n",
    "# bias_init = 'zeros'\n",
    "\n",
    "# # kernel_init = initializers.he_uniform()\n",
    "# # bias_init = initializers.he_uniform()\n",
    "# kernel_regul = regularizers.l2(1)\n",
    "# activity_regul = regularizers.l2(1)\n",
    "\n",
    "# learn_rate = 0.0002\n",
    "\n",
    "# # ResNet block\n",
    "# def identity_block(X, filters, f):\n",
    "\n",
    "#     F1, F2 = filters\n",
    "\n",
    "#     X_shortcut = X\n",
    "\n",
    "#     X = BatchNormalization(axis=3)(X)\n",
    "#     X = Activation('relu')(X)\n",
    "#     X = Conv2D(filters=F1, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
    "#                kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "#               kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(X)\n",
    "\n",
    "#     X = BatchNormalization(axis=3)(X)\n",
    "#     X = Activation('relu')(X)\n",
    "#     X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
    "#                kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "#               kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(X)\n",
    "\n",
    "#     X = Add()([X, X_shortcut])\n",
    "#     X = Activation('relu')(X)\n",
    "\n",
    "#     return X\n",
    "\n",
    "# # ENCODER\n",
    "# def encoder_layer(img_in, filters, kernel_size, bn=True, resid=True):\n",
    "#     # conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding='same')(img_in)\n",
    "#     conv = img_in\n",
    "#     if bn:\n",
    "#         conv = BatchNormalization()(conv)\n",
    "#     conv = Activation('relu')(conv)\n",
    "# #             conv = MaxPooling2D((2, 2))(conv)\n",
    "\n",
    "\n",
    "#     if resid:\n",
    "#         conv = identity_block(conv, (filters, filters), kernel_size)\n",
    "\n",
    "#     return conv\n",
    "\n",
    "# # DECODER\n",
    "# def decoder_layer(img_in, e_conv, filters, kernel_size, bn=True, resid=True):\n",
    "#     # up_img = UpSampling2D(size=(2,2))(img_in)\n",
    "#     up_img = img_in\n",
    "#     concat_img = Concatenate(axis=3)([e_conv,up_img])\n",
    "#     conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding='same',\n",
    "#                   kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "#               kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(concat_img)\n",
    "#     if bn:\n",
    "#         conv = BatchNormalization()(conv)\n",
    "#     conv = LeakyReLU(alpha=0.1)(conv)\n",
    "\n",
    "#     if resid:\n",
    "#         conv = identity_block(conv, (filters, filters), kernel_size)\n",
    "#     return conv\n",
    "\n",
    "\n",
    "\n",
    "# def build_runet():      \n",
    "\n",
    "#     # INPUTS\n",
    "#     history_traffic_volume = Input(shape=history_volume_shape)\n",
    "\n",
    "#     # kernel_init = initializers.he_normal()\n",
    "#     # bias_init = initializers.he_normal()\n",
    "#     kernel_init = 'glorot_uniform'\n",
    "#     bias_init = 'zeros'\n",
    "\n",
    "# #         kernel_init = initializers.he_uniform()\n",
    "# #         bias_init = 'Orthogonal'\n",
    "#     kernel_regul = regularizers.l2(1)\n",
    "#     activity_regul = regularizers.l2(1)\n",
    "\n",
    "#     filters_base = 32\n",
    "#     e_conv1_head = Conv2D(filters=filters_base, kernel_size=3, strides=1, padding='same',\n",
    "#                           kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "#                   kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(history_traffic_volume)\n",
    "# #         e_conv1_head = Conv2D(filters=filters_base*1, kernel_size=3, strides=1, padding='same',\n",
    "# #                               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "# #                       kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv1_head)\n",
    "#     e_conv1_tail = AveragePooling2D((2, 2))(e_conv1_head)\n",
    "#     e_conv1 = encoder_layer(e_conv1_tail, filters_base, 3, bn=False)\n",
    "\n",
    "#     e_conv2_head = Conv2D(filters=filters_base*2, kernel_size=3, strides=1, padding='same',\n",
    "#                           kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "#                   kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv1)\n",
    "#     e_conv2_tail = AveragePooling2D((2, 2))(e_conv2_head)\n",
    "#     e_conv2 = encoder_layer(e_conv2_tail, filters_base*2, 3)\n",
    "\n",
    "#     e_conv3_head = Conv2D(filters=filters_base*4, kernel_size=3, strides=1, padding='same',\n",
    "#                           kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "#                   kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv2)\n",
    "#     e_conv3_tail = AveragePooling2D((2, 2))(e_conv3_head)\n",
    "#     d_conv3_head = encoder_layer(e_conv3_tail, filters_base*4, 3)\n",
    "#     resid1 = Add()([e_conv3_tail, d_conv3_head])\n",
    "#     d_conv3_tail = UpSampling2D(size=(2, 2))(resid1)\n",
    "\n",
    "\n",
    "#     d_conv4_head = decoder_layer(d_conv3_tail, e_conv3_head, filters_base*2, 3)\n",
    "#     resid2 = Add()([d_conv4_head, e_conv2_tail])\n",
    "#     d_conv4_tail = UpSampling2D(size=(2, 2))(resid2)\n",
    "\n",
    "\n",
    "#     d_conv5_head = decoder_layer(d_conv4_tail, e_conv2_head, filters_base*1, 3)\n",
    "#     resid3 = Add()([d_conv5_head, e_conv1_tail])\n",
    "#     d_conv5_tail = UpSampling2D(size=(2, 2))(resid3)\n",
    "\n",
    "#     d_conv6_head = decoder_layer(d_conv5_tail, e_conv1_head, filters_base//2, 3, bn=False)\n",
    "\n",
    "\n",
    "#     outputs = Conv2D(1, 1, activation = 'relu', kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "#                   kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(d_conv6_head)\n",
    "\n",
    "#     # Setup the model inputs / outputs\n",
    "#     model = Model(inputs=history_traffic_volume, outputs=outputs)\n",
    "\n",
    "#     # Compile the model RMSprop\n",
    "#     model.compile(\n",
    "#         optimizer = Adam(lr=learn_rate),\n",
    "# #         loss='mse'\n",
    "#         loss = loss_hole\n",
    "#     )\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_model = build_unet()\n",
    "min_model.load_weights('./model/dataRecorvey20191109/unet_1024.h5')\n",
    "\n",
    "y_pred = min_model.predict(test_history_volume)\n",
    "y_true = test_true_volume\n",
    "# 仅对缺失数据进行l2评价。（对预测来说既对第一层进行评价，验证）\n",
    "# y_true = (1-test_mask[:,:,:,:1])*test_true_volume\n",
    "# y_pred = (1-test_mask[:,:,:,:1])*y_pred\n",
    "\n",
    "l2(y_true, y_pred), l1(y_true, y_pred)\n",
    "# (11.085728026573435, 6.957813774885677)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "posX = 15\n",
    "posY = 15\n",
    "startX = 0\n",
    "gapX = len(test_true_volume)\n",
    "\n",
    "zero_line = np.random.randint(0, 1, (gapX,))\n",
    "\n",
    "y = test_true_volume[:, posX, posY, :][startX: startX+gapX]\n",
    "\n",
    "yf = y_pred[:, posX, posY, :][startX: startX+gapX]\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "fig, ax = plt.subplots(figsize=(25, 18))\n",
    "lines = plt.plot(x, yf, 'k^--', x, y, 'ko-', x, yf-y, 'k.-', x, zero_line, 'k-', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.font_manager import FontProperties\n",
    "font = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\")\n",
    "fontsz = 24\n",
    "\n",
    "posX = 2\n",
    "posY = 2\n",
    "startX = 1230\n",
    "gapX = 192\n",
    "\n",
    "zero_line = np.random.randint(0, 1, (gapX,))\n",
    "\n",
    "y = test_true_volume[:, posX, posY, :][startX: startX+gapX]\n",
    "yf = y_pred[:, posX, posY, :][startX: startX+gapX]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5.8))\n",
    "lines = plt.plot(x, y, x, yf, x,y-yf, x,zero_line)\n",
    "l1, l2, l3, l4 = lines\n",
    "\n",
    "plt.setp(lines, markersize=7, linewidth=1.34)\n",
    "plt.setp(l1, color='k', linestyle='--', marker='o')  # line1 is thick and red\n",
    "plt.setp(l2, color='grey', linestyle='-', marker='^')  # line2 is thinner and green\n",
    "plt.setp(l3, color='k', linestyle='-', marker='.')  # line2 is thinner and green\n",
    "plt.setp(l4, color='k', linestyle='-')  # line2 is thinner and green\n",
    "\n",
    "plt.ylabel('交通量/ 辆/小时', fontproperties=font, fontsize=fontsz)\n",
    "plt.xlabel('观测时间点', fontproperties=font, fontsize=fontsz)\n",
    "\n",
    "qwe = range(0,2000,500)\n",
    "plt.yticks(qwe, ('0', '500', '1 000', '1 500'))\n",
    "ax.legend(('真实值', '修复值', '误差'), prop=font, loc=1)\n",
    "plt.rcParams.update({'font.size': fontsz})\n",
    "plt.savefig('U-Net日交通量预测图.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.font_manager import FontProperties\n",
    "font = FontProperties(fname=r\"c:\\windows\\fonts\\STSong.TTF\")\n",
    "\n",
    "posX = 29\n",
    "posY = 5\n",
    "startX = 270\n",
    "gapX = 200\n",
    "\n",
    "zero_line = np.random.randint(0, 1, (gapX,))\n",
    "\n",
    "y = test_true_volume[:, posX, posY, :][startX: startX+gapX]\n",
    "yf = y_pred[:, posX, posY, :][startX: startX+gapX]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "lines = plt.plot(x, y, x, yf, x,y-yf, x,zero_line)\n",
    "l1, l2, l3, l4 = lines\n",
    "\n",
    "plt.setp(lines, markersize=5, linewidth=2)\n",
    "plt.setp(l1, color='k', linestyle='--', marker='o')  # line1 is thick and red\n",
    "plt.setp(l2, color='grey', linestyle='-', marker='^')  # line2 is thinner and green\n",
    "plt.setp(l3, color='k', linestyle='-', marker='.')  # line2 is thinner and green\n",
    "plt.setp(l4, color='k', linestyle='-')  # line2 is thinner and green\n",
    "\n",
    "plt.ylabel('交通量(辆/小时)', fontproperties=font, fontsize=12)\n",
    "plt.xlabel('观测时间', fontproperties=font, fontsize=12)\n",
    "ax.legend(('真实值', '修复值', '真实值与修复值的差值'), prop=font, loc='best')\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "# plt.savefig('aaa.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_matrix =y_pred[170].reshape(32, 32, 1)\n",
    "true_matrix =test_true_volume[170].reshape(32, 32, 1)\n",
    "\n",
    "pic_shape = pic_matrix.shape\n",
    "# Load mask\n",
    "# mask = random_mask(pic_shape[0], pic_shape[1], size=0.3)\n",
    "# mask = random_mask(pic_shape[0], pic_shape[1], size=0.9, type='block', block_size=(25, 25))\n",
    "\n",
    "# Image + mask\n",
    "masked_img = deepcopy(pic_matrix)\n",
    "# masked_img = (pic_matrix - pic_matrix.min())/(pic_matrix.max() - pic_matrix.min())*255\n",
    "masked_img[rand_mask[:,:,0]==0] = np.min(masked_img)\n",
    "\n",
    "# Show side by side\n",
    "_, axes = plt.subplots(1, 3, figsize=(25, 12))\n",
    "axes[0].imshow(true_matrix[:,:,0])\n",
    "axes[1].imshow(pic_matrix[:,:,0])\n",
    "axes[2].imshow(rand_mask[:,:,0]*255, cmap ='gray')\n",
    "# axes[3].imshow(masked_img[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posX = 31\n",
    "posY = 2\n",
    "startX = 300\n",
    "gapX = 1000\n",
    "\n",
    "y = test_true_volume[:, posX, posY, :][startX: startX+gapX]\n",
    "\n",
    "yf = y_pred[:, posX, posY, :][startX: startX+gapX]\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "fig, ax = plt.subplots(figsize=(25, 8))\n",
    "lines = plt.plot(x, yf, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start_time = 1600\n",
    "y = y_true.reshape(-1,)[start_time: start_time+100]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[start_time: start_time+100]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 10))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi = l2_validation\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "lines = plt.plot(xi, yi, 'k^--', linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lr_step\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "lines = plt.plot(x, y, 'ko-', linewidth=1, markersize=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 3600\n",
    "y = y_true.reshape(-1,)[start_time: start_time+100]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[start_time: start_time+100]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 0\n",
    "y = y_true.reshape(-1,)[start_time: start_time+100]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[start_time: start_time+100]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_volume"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
