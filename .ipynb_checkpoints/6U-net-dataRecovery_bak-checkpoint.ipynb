{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, TensorBoard\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from datetime import datetime\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers import Input, Conv2D, UpSampling2D, Dropout, LeakyReLU, BatchNormalization, Activation, Add, Subtract\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "# from keras.applications import VGG16\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "\n",
    "# from libs.pconv_model_UNet import PConvUnet\n",
    "from keras.models import load_model  \n",
    "\n",
    "from copy import deepcopy\n",
    "from libs.util import random_mask\n",
    "\n",
    "# Settings\n",
    "MAX_BATCH_SIZE = 64\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "matrix_length = 32\n",
    "\n",
    "matrix_df = pd.read_csv('./data/trafficV_M.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createTrainArray(week_history_num=0, minute_history_num=0):\n",
    "    week_delta_list = [pd.Timedelta(i+1, unit='W') for i in range(week_history_num)]\n",
    "    minute_delta_list = [pd.Timedelta((i+1)*15, unit='m') for i in range(minute_history_num)]\n",
    "    # 参考历史数据时间点list\n",
    "    delta_list = week_delta_list+minute_delta_list\n",
    "    print(delta_list)\n",
    "    \n",
    "    set_up_time = pd.Timedelta(week_history_num, unit='W')\n",
    "    # 根据历史数据选取多少，重新构建数据集\n",
    "    # 相当于去除最开始week_history_num个周的数据，因为这些数据无法找到更前的数据\n",
    "    train_df = matrix_df.truncate(before=matrix_df.index.min() + set_up_time)\n",
    "    \n",
    "    train_ago_array_tuple = tuple([np.array(matrix_df.loc[train_df.index - i]).reshape(-1, matrix_length, matrix_length, 1) for i in delta_list])\n",
    "    train_df = np.array(train_df).reshape(-1, matrix_length, matrix_length, 1)\n",
    "    # concatenate保持 待修复数据在前，参考历史数据在后。与random_mask函数生成mask相一致\n",
    "    train_array = np.concatenate((train_df,)+train_ago_array_tuple, axis=3)\n",
    "    print(train_array.shape)\n",
    "    return train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timedelta('7 days 00:00:00'), Timedelta('14 days 00:00:00'), Timedelta('0 days 00:15:00'), Timedelta('0 days 00:30:00'), Timedelta('0 days 00:45:00')]\n",
      "(16032, 32, 32, 6)\n"
     ]
    }
   ],
   "source": [
    "week_history_num = 2\n",
    "minute_history_num = 3\n",
    "\n",
    "channel_num = week_history_num +minute_history_num +1\n",
    "smooth_time = channel_num-1\n",
    "\n",
    "train_array = createTrainArray(week_history_num, minute_history_num)\n",
    "X_train, X_test = train_test_split(train_array, test_size = 0.1, shuffle=False)\n",
    "# X_train, X_test = train_test_split(train_array, test_size = 0.1, random_state=42, shuffle=False)\n",
    "\n",
    "# X_train = train_array[:16704-900-900]\n",
    "# X_val = train_array[16704-900-900:16704-900]\n",
    "# X_test = train_array[16704-900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_shape = (matrix_length, matrix_length, channel_num)\n",
    "true_volume_shape = (matrix_length, matrix_length, 1)\n",
    "history_volume_shape = (matrix_length, matrix_length, channel_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_steps = X_train.shape[0] // MAX_BATCH_SIZE\n",
    "val_steps = X_test.shape[0] // MAX_BATCH_SIZE\n",
    "epoch_steps, val_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以第一数据为例. 第一列为待预测数据\n",
    "# 第一例：1.15 0:00  二：1.8 0:00  三：1.1 0:00  四：1.14 23:45  五：1.14 23:30  六：1.14 23:15\n",
    "# X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_size = 0.4\n",
    "mask_type = 'rand'\n",
    "block_size = (32, 32)\n",
    "\n",
    "# 单个矩阵mask\n",
    "rand_mask = random_mask(matrix_length, matrix_length, size=rand_size, channels=channel_num, smooth_time=smooth_time, type=mask_type, block_size=block_size)\n",
    "# 堆叠成多个mask，方便对batch数据进行处理\n",
    "mask = np.stack([rand_mask for _ in range(MAX_BATCH_SIZE)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def l2(y_true, y_pred):\n",
    "    size = 0\n",
    "    if rand_size<=1:\n",
    "        size = int((matrix_length * matrix_length) * rand_size)\n",
    "    else:\n",
    "        size = rand_size\n",
    "        \n",
    "    if size == 0:\n",
    "        raise Exception(\"size == 0\")\n",
    "    return math.sqrt(np.sum(np.mean(np.square(y_true - y_pred), axis=0))/size)\n",
    "\n",
    "def l1(y_true, y_pred):\n",
    "    size = 0\n",
    "    if rand_size<=1:\n",
    "        size = int((matrix_length * matrix_length) * rand_size)\n",
    "    else:\n",
    "        size = rand_size\n",
    "        \n",
    "    if size == 0:\n",
    "        raise Exception(\"size == 0\")\n",
    "    return np.sum(np.mean(np.abs(y_true - y_pred), axis=0))/size\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    size = 0\n",
    "    if rand_size<=1:\n",
    "        size = int((matrix_length * matrix_length) * rand_size)\n",
    "    else:\n",
    "        size = rand_size\n",
    "        \n",
    "    if size == 0:\n",
    "        raise Exception(\"size == 0\")\n",
    "        \n",
    "    return np.sum(np.mean((np.abs(y_true - y_pred)/y_true)*100, axis=0))/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "def load_data(volume_matrix, batch_size=MAX_BATCH_SIZE):\n",
    "    n_batches=batch_size\n",
    "    len_of_matrix = len(volume_matrix)\n",
    "\n",
    "    batch_i = 0\n",
    "    while ((batch_i+1)*batch_size < len_of_matrix):\n",
    "        batch_matrix = volume_matrix[batch_i*batch_size: (batch_i+1)*batch_size]\n",
    "        masked = deepcopy(batch_matrix)\n",
    "        # true_volume为待修复数据， history_volume为历史数据及当前残差待修复数据\n",
    "        true_volume = deepcopy(batch_matrix[:, :, :, :1])\n",
    "        # mask==1代表有效采集点，0代表待预测采集点\n",
    "        traffic_mean = masked[mask==1].mean()\n",
    "        # 待预测点的值用已知值的平均值初始化\n",
    "        masked[mask==0] = traffic_mean\n",
    "        history_volume = deepcopy(masked)\n",
    "        \n",
    "        batch_i+=1\n",
    "\n",
    "        yield true_volume, history_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_loss(y_true, y_pred):\n",
    "        \"\"\"Calculate the L1 loss used in all loss calculations\"\"\"\n",
    "        if K.ndim(y_true) == 4:\n",
    "            return K.sum(K.square(y_pred - y_true), axis=[1,2,3])\n",
    "        elif K.ndim(y_true) == 3:\n",
    "            return K.sum(K.square(y_pred - y_true), axis=[1,2])\n",
    "        else:\n",
    "            raise NotImplementedError(\"Calculating L1 loss on 1D tensors? should not occur for this network\")\n",
    "\n",
    "# 缺失点mse\n",
    "def loss_hole(y_true, y_pred):\n",
    "    return l2_loss((1-mask) * y_true, (1-mask) * y_pred)\n",
    "\n",
    "# 非缺失点mse\n",
    "def loss_bg(y_true, y_pred):\n",
    "    return l2_loss(mask * y_true, mask * y_pred)\n",
    "\n",
    "def loss_fuc(y_true, y_pred):\n",
    "    return loss_hole(y_true, y_pred)*3 + loss_bg(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_init = 'glorot_uniform'\n",
    "bias_init = 'zeros'\n",
    "\n",
    "# kernel_init = initializers.he_uniform()\n",
    "# bias_init = initializers.he_uniform()\n",
    "kernel_regul = regularizers.l2(1)\n",
    "activity_regul = regularizers.l2(1)\n",
    "\n",
    "learn_rate = 0.004\n",
    "\n",
    "# ENCODER\n",
    "def encoder_layer(img_in, filters, kernel_size, bn=True, resid=True):\n",
    "    # conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding='same')(img_in)\n",
    "    conv = Conv2D(filters, (kernel_size, kernel_size), padding=\"same\",\n",
    "       strides=1,kernel_initializer='glorot_uniform')(img_in)\n",
    "    if bn:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "\n",
    "    conv = Conv2D(filters, (kernel_size, kernel_size), padding=\"same\",\n",
    "       strides=1,kernel_initializer='glorot_uniform')(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    return conv\n",
    "\n",
    "# DECODER\n",
    "def decoder_layer(img_in, filters, kernel_size, bn=True, resid=True):\n",
    "    conv = Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(img_in)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "\n",
    "    conv = Conv2D(filters=filters//2, kernel_size=(3, 3), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(conv)\n",
    "    conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "\n",
    "    conv = UpSampling2D(size = (2,2))(conv)\n",
    "    return conv\n",
    "\n",
    "\n",
    "def build_unet(): \n",
    "    history_traffic_volume = Input(shape=history_volume_shape)\n",
    "    \n",
    "    conv1 = encoder_layer(history_traffic_volume, 32, 3, bn=False)\n",
    "    pool1 = AveragePooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = encoder_layer(pool1, 64, 3, bn=True)\n",
    "    pool2 = AveragePooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = encoder_layer(pool2, 128, 3, bn=True)\n",
    "    pool3 = AveragePooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    #         conv4 = encoder_layer(pool3, 256, 3, bn=True)\n",
    "    #         pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = decoder_layer(pool3, 256, 3, bn=True)\n",
    "    merge1 = Concatenate()([conv3,conv5])\n",
    "\n",
    "    conv6 = decoder_layer(merge1, 128, 3, bn=True)\n",
    "    merge2 = Concatenate()([conv2,conv6])\n",
    "\n",
    "    conv7 = decoder_layer(merge2, 64, 3, bn=True)\n",
    "    merge3 = Concatenate()([conv1,conv7])\n",
    "\n",
    "    #         conv8 = decoder_layer(merge3, 32, 3, bn=True)\n",
    "    #         merge4 = Concatenate()([conv1,conv8])\n",
    "\n",
    "    conv9 = encoder_layer(merge3, 32, 3, bn=False)\n",
    "\n",
    "\n",
    "    model_output =  Conv2D(1, 1, activation = 'relu', kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(conv9)\n",
    "\n",
    "    # Setup the model inputs / outputs\n",
    "    model = Model(inputs=history_traffic_volume, outputs=model_output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer = Adam(lr=learn_rate),\n",
    "        loss=loss_hole\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_masked = deepcopy(X_test)\n",
    "test_true_volume = deepcopy(X_test[:, :, :, :1])\n",
    "\n",
    "test_length = len(X_test)\n",
    "test_mask = np.stack([rand_mask for _ in range(test_length)], axis=0)\n",
    "\n",
    "test_traffic_mean = X_test[test_mask==1].mean()\n",
    "test_masked[test_mask==0] = test_traffic_mean\n",
    "test_history_volume = deepcopy(test_masked)\n",
    "\n",
    "rand_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_step = []\n",
    "l2_validation = []\n",
    "\n",
    "unet = build_unet()\n",
    "\n",
    "\n",
    "def train(train_matrix, epochs, batch_size=MAX_BATCH_SIZE, learn_rate=0.01):\n",
    "\n",
    "    min_mse = 999\n",
    "    start_time = datetime.now()\n",
    "    print(\"train start \"+str(start_time))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "#         if epoch>=110 and epoch % 5 == 0 and epoch != 0:\n",
    "#             unet_lr = K.get_value(unet.optimizer.lr)\n",
    "#             if unet_lr>0.002:\n",
    "#                 K.set_value(unet.optimizer.lr, unet_lr*0.9)\n",
    "                \n",
    "        for batch_i, (true_volume, history_volume) in enumerate(load_data(train_matrix,batch_size)):\n",
    "            # true_volume 真实待预测路网交通量  history_volume 路网交通量历史数据\n",
    "            #  训练 unet\n",
    "            #  训练 Generator\n",
    "            g_loss = unet.train_on_batch(history_volume, true_volume)\n",
    "\n",
    "\n",
    "        elapsed_time = datetime.now() - start_time\n",
    "        # Plot the progress\n",
    "        y_pred = unet.predict(test_history_volume)\n",
    "        \n",
    "        y_true = (1-test_mask[:,:,:,:1])*test_true_volume\n",
    "        y_pred = (1-test_mask[:,:,:,:1])*y_pred\n",
    "        l2_epoch_validation = l2(y_true, y_pred)\n",
    "        l1_epoch_validation = l1(y_true, y_pred)\n",
    "        \n",
    "        y_pred[y_true==0] += 1\n",
    "        y_true[y_true==0] += 1\n",
    "        mape_epoch_validation = mape(y_true, y_pred)\n",
    "        \n",
    "        lr_step.append(K.get_value(unet.optimizer.lr))\n",
    "        l2_validation.append(l2_epoch_validation)\n",
    "        if epoch%1==0:\n",
    "#             print(\"unet lr:\"+ str(K.get_value(unet.optimizer.lr)))\n",
    "            print (\"[Epoch %d/%d]  [mse: %f] [mae: %f] [mape: %f] [G loss: %f] time: %s\" % (epoch+1, epochs,\n",
    "                                                                    l2_epoch_validation,\n",
    "                                                                    l1_epoch_validation,\n",
    "                                                                    mape_epoch_validation,\n",
    "                                                                    g_loss,\n",
    "                                                                    elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start 2019-11-12 10:37:57.433384\n",
      "[Epoch 1/200]  [mse: 354.103258] [mae: 252.680846] [mape: 66.777459] [G loss: 97591016.000000] time: 0:00:28.263743\n",
      "[Epoch 2/200]  [mse: 107.636519] [mae: 73.314763] [mape: 30.847518] [G loss: 35998100.000000] time: 0:00:48.408834\n",
      "[Epoch 3/200]  [mse: 71.637755] [mae: 50.131431] [mape: 37.995179] [G loss: 10241592.000000] time: 0:01:07.835396\n",
      "[Epoch 4/200]  [mse: 112.249642] [mae: 71.780565] [mape: 33.230458] [G loss: 5397094.000000] time: 0:01:26.949269\n",
      "[Epoch 5/200]  [mse: 32.096104] [mae: 22.195409] [mape: 14.662380] [G loss: 2895571.000000] time: 0:01:45.870120\n",
      "[Epoch 6/200]  [mse: 35.335972] [mae: 25.616647] [mape: 16.432316] [G loss: 1807227.750000] time: 0:02:04.960939\n",
      "[Epoch 7/200]  [mse: 29.259923] [mae: 20.682144] [mape: 12.565797] [G loss: 1421400.500000] time: 0:02:24.105709\n",
      "[Epoch 8/200]  [mse: 32.535919] [mae: 23.984534] [mape: 13.907191] [G loss: 943898.875000] time: 0:02:43.245106\n",
      "[Epoch 9/200]  [mse: 25.516382] [mae: 17.954514] [mape: 11.484702] [G loss: 875890.812500] time: 0:03:02.330946\n",
      "[Epoch 10/200]  [mse: 37.914036] [mae: 28.112029] [mape: 14.527021] [G loss: 718933.062500] time: 0:03:21.678307\n",
      "[Epoch 11/200]  [mse: 44.584605] [mae: 34.579111] [mape: 17.208073] [G loss: 577211.375000] time: 0:03:40.940533\n",
      "[Epoch 12/200]  [mse: 36.619995] [mae: 27.111070] [mape: 12.787114] [G loss: 473471.750000] time: 0:04:00.403252\n",
      "[Epoch 13/200]  [mse: 31.750033] [mae: 23.116929] [mape: 11.799300] [G loss: 428691.187500] time: 0:04:19.603254\n",
      "[Epoch 14/200]  [mse: 54.275445] [mae: 37.887781] [mape: 20.176646] [G loss: 730029.937500] time: 0:04:38.786026\n",
      "[Epoch 15/200]  [mse: 23.617771] [mae: 16.981506] [mape: 10.447452] [G loss: 428047.718750] time: 0:04:57.717136\n",
      "[Epoch 16/200]  [mse: 28.044475] [mae: 20.520276] [mape: 11.151361] [G loss: 335919.062500] time: 0:05:16.884106\n",
      "[Epoch 17/200]  [mse: 31.047800] [mae: 22.061218] [mape: 10.513929] [G loss: 288047.375000] time: 0:05:36.178251\n",
      "[Epoch 18/200]  [mse: 31.460475] [mae: 22.106006] [mape: 10.274763] [G loss: 254935.781250] time: 0:05:55.261566\n",
      "[Epoch 19/200]  [mse: 30.493889] [mae: 21.364954] [mape: 10.320124] [G loss: 232475.734375] time: 0:06:14.639005\n",
      "[Epoch 20/200]  [mse: 30.744475] [mae: 21.409258] [mape: 10.058350] [G loss: 232580.609375] time: 0:06:33.829033\n",
      "[Epoch 21/200]  [mse: 21.589429] [mae: 15.134827] [mape: 10.153752] [G loss: 279343.000000] time: 0:06:53.476931\n",
      "[Epoch 22/200]  [mse: 32.419330] [mae: 23.126538] [mape: 11.086609] [G loss: 242722.375000] time: 0:07:12.327981\n",
      "[Epoch 23/200]  [mse: 29.967478] [mae: 21.209469] [mape: 10.957390] [G loss: 205679.609375] time: 0:07:31.117119\n",
      "[Epoch 24/200]  [mse: 26.034045] [mae: 18.303300] [mape: 10.321110] [G loss: 205712.656250] time: 0:07:50.744385\n",
      "[Epoch 25/200]  [mse: 20.334896] [mae: 14.504208] [mape: 10.047097] [G loss: 232937.468750] time: 0:08:09.790948\n",
      "[Epoch 26/200]  [mse: 26.657111] [mae: 18.586596] [mape: 10.202690] [G loss: 203250.703125] time: 0:08:29.056843\n",
      "[Epoch 27/200]  [mse: 27.425627] [mae: 19.031717] [mape: 10.231573] [G loss: 210207.875000] time: 0:08:48.612895\n",
      "[Epoch 28/200]  [mse: 24.004102] [mae: 17.349768] [mape: 11.096104] [G loss: 184321.421875] time: 0:09:07.514627\n",
      "[Epoch 29/200]  [mse: 27.721083] [mae: 19.475537] [mape: 11.902649] [G loss: 252338.718750] time: 0:09:26.828985\n",
      "[Epoch 30/200]  [mse: 24.967997] [mae: 17.718543] [mape: 10.320447] [G loss: 198660.500000] time: 0:09:45.799688\n",
      "[Epoch 31/200]  [mse: 26.526112] [mae: 18.907854] [mape: 10.581806] [G loss: 184695.671875] time: 0:10:04.733252\n",
      "[Epoch 32/200]  [mse: 25.216603] [mae: 17.966405] [mape: 10.293759] [G loss: 176410.531250] time: 0:10:23.867590\n",
      "[Epoch 33/200]  [mse: 26.639294] [mae: 18.788991] [mape: 10.331050] [G loss: 174765.843750] time: 0:10:43.253016\n",
      "[Epoch 34/200]  [mse: 19.991527] [mae: 14.349669] [mape: 9.749230] [G loss: 189660.140625] time: 0:11:02.785730\n",
      "[Epoch 35/200]  [mse: 29.122693] [mae: 21.131799] [mape: 11.052243] [G loss: 184526.875000] time: 0:11:22.343729\n",
      "[Epoch 36/200]  [mse: 32.940603] [mae: 23.356433] [mape: 11.163390] [G loss: 199509.718750] time: 0:11:41.490053\n",
      "[Epoch 37/200]  [mse: 26.708962] [mae: 19.019493] [mape: 10.639707] [G loss: 180464.078125] time: 0:12:00.204714\n",
      "[Epoch 38/200]  [mse: 26.811346] [mae: 19.308861] [mape: 10.993350] [G loss: 186013.078125] time: 0:12:18.879112\n",
      "[Epoch 39/200]  [mse: 20.310949] [mae: 14.668072] [mape: 10.320713] [G loss: 209433.437500] time: 0:12:37.603343\n",
      "[Epoch 40/200]  [mse: 25.234072] [mae: 18.320841] [mape: 10.799251] [G loss: 173222.921875] time: 0:12:56.337053\n",
      "[Epoch 41/200]  [mse: 30.421717] [mae: 21.508739] [mape: 10.644700] [G loss: 175078.203125] time: 0:13:15.089341\n",
      "[Epoch 42/200]  [mse: 38.688678] [mae: 27.268901] [mape: 11.811902] [G loss: 209111.906250] time: 0:13:33.788294\n",
      "[Epoch 43/200]  [mse: 37.592341] [mae: 27.539855] [mape: 16.948615] [G loss: 509497.468750] time: 0:13:52.485414\n",
      "[Epoch 44/200]  [mse: 20.464786] [mae: 14.348659] [mape: 9.033927] [G loss: 171107.625000] time: 0:14:11.159239\n",
      "[Epoch 45/200]  [mse: 22.279042] [mae: 15.490740] [mape: 8.850869] [G loss: 157451.703125] time: 0:14:29.911318\n",
      "[Epoch 46/200]  [mse: 22.391878] [mae: 15.584690] [mape: 8.874659] [G loss: 154240.375000] time: 0:14:48.648945\n",
      "[Epoch 47/200]  [mse: 21.416152] [mae: 14.893947] [mape: 8.711129] [G loss: 154883.812500] time: 0:15:07.384794\n",
      "[Epoch 48/200]  [mse: 22.355087] [mae: 15.642734] [mape: 8.960896] [G loss: 154884.015625] time: 0:15:26.186262\n",
      "[Epoch 49/200]  [mse: 20.971143] [mae: 14.731295] [mape: 8.907891] [G loss: 155409.453125] time: 0:15:44.968458\n",
      "[Epoch 50/200]  [mse: 19.775423] [mae: 13.965988] [mape: 8.929683] [G loss: 161504.343750] time: 0:16:04.111462\n",
      "[Epoch 51/200]  [mse: 22.608170] [mae: 15.944891] [mape: 9.289339] [G loss: 152819.328125] time: 0:16:22.838138\n",
      "[Epoch 52/200]  [mse: 24.448260] [mae: 17.034819] [mape: 8.940864] [G loss: 155006.890625] time: 0:16:42.013575\n",
      "[Epoch 53/200]  [mse: 27.599238] [mae: 19.109358] [mape: 9.368334] [G loss: 186783.515625] time: 0:17:01.234538\n",
      "[Epoch 54/200]  [mse: 29.470636] [mae: 20.655076] [mape: 10.227089] [G loss: 190217.093750] time: 0:17:20.324456\n",
      "[Epoch 55/200]  [mse: 24.325887] [mae: 17.156726] [mape: 11.175911] [G loss: 177586.562500] time: 0:17:39.511026\n",
      "[Epoch 56/200]  [mse: 18.601232] [mae: 12.745421] [mape: 7.720719] [G loss: 158729.046875] time: 0:17:58.718124\n",
      "[Epoch 57/200]  [mse: 17.811214] [mae: 12.388730] [mape: 8.118117] [G loss: 156395.546875] time: 0:18:17.722104\n",
      "[Epoch 58/200]  [mse: 18.727095] [mae: 13.010160] [mape: 8.172370] [G loss: 145944.468750] time: 0:18:36.515972\n",
      "[Epoch 59/200]  [mse: 17.948418] [mae: 12.405042] [mape: 8.025358] [G loss: 145760.968750] time: 0:18:55.192539\n",
      "[Epoch 60/200]  [mse: 18.865435] [mae: 13.086350] [mape: 8.238262] [G loss: 147095.812500] time: 0:19:13.875283\n",
      "[Epoch 61/200]  [mse: 18.868588] [mae: 13.058026] [mape: 8.301397] [G loss: 143444.750000] time: 0:19:32.625611\n",
      "[Epoch 62/200]  [mse: 20.484086] [mae: 14.217938] [mape: 8.470321] [G loss: 139656.968750] time: 0:19:51.356191\n",
      "[Epoch 63/200]  [mse: 22.393063] [mae: 15.789388] [mape: 8.875450] [G loss: 137476.296875] time: 0:20:10.063648\n",
      "[Epoch 64/200]  [mse: 19.229730] [mae: 13.472221] [mape: 8.831283] [G loss: 143170.218750] time: 0:20:28.768794\n",
      "[Epoch 65/200]  [mse: 21.118788] [mae: 14.465785] [mape: 7.922650] [G loss: 152714.078125] time: 0:20:47.486874\n",
      "[Epoch 66/200]  [mse: 20.552381] [mae: 14.500960] [mape: 9.555340] [G loss: 156638.156250] time: 0:21:06.182926\n",
      "[Epoch 67/200]  [mse: 28.692305] [mae: 19.843078] [mape: 9.989943] [G loss: 174173.953125] time: 0:21:24.928238\n",
      "[Epoch 68/200]  [mse: 16.875565] [mae: 11.773392] [mape: 8.238704] [G loss: 139145.468750] time: 0:21:43.916136\n",
      "[Epoch 69/200]  [mse: 25.315001] [mae: 17.990319] [mape: 9.196045] [G loss: 144411.375000] time: 0:22:02.817766\n",
      "[Epoch 70/200]  [mse: 21.393149] [mae: 14.796263] [mape: 8.521453] [G loss: 139436.906250] time: 0:22:21.902363\n",
      "[Epoch 71/200]  [mse: 25.062350] [mae: 17.187598] [mape: 8.065262] [G loss: 142024.000000] time: 0:22:41.063444\n",
      "[Epoch 72/200]  [mse: 26.448645] [mae: 18.262694] [mape: 8.174411] [G loss: 140319.406250] time: 0:23:00.179833\n",
      "[Epoch 73/200]  [mse: 23.806725] [mae: 16.322720] [mape: 8.125673] [G loss: 138205.609375] time: 0:23:19.172143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 74/200]  [mse: 22.393790] [mae: 15.498307] [mape: 8.038994] [G loss: 132656.968750] time: 0:23:38.461694\n",
      "[Epoch 75/200]  [mse: 18.631820] [mae: 12.761283] [mape: 7.495709] [G loss: 143246.218750] time: 0:23:58.043893\n",
      "[Epoch 76/200]  [mse: 24.308584] [mae: 16.859280] [mape: 8.537374] [G loss: 141629.671875] time: 0:24:17.279481\n",
      "[Epoch 77/200]  [mse: 19.215401] [mae: 13.389583] [mape: 8.938425] [G loss: 164417.500000] time: 0:24:36.600576\n",
      "[Epoch 78/200]  [mse: 21.312246] [mae: 15.038387] [mape: 8.538922] [G loss: 128792.132812] time: 0:24:55.708182\n",
      "[Epoch 79/200]  [mse: 20.001080] [mae: 13.725565] [mape: 7.558139] [G loss: 148299.250000] time: 0:25:14.758909\n",
      "[Epoch 80/200]  [mse: 26.414958] [mae: 17.982257] [mape: 8.125639] [G loss: 146366.484375] time: 0:25:33.693737\n",
      "[Epoch 81/200]  [mse: 23.562307] [mae: 16.793516] [mape: 9.342683] [G loss: 129748.140625] time: 0:25:53.005068\n",
      "[Epoch 82/200]  [mse: 18.141360] [mae: 12.906994] [mape: 9.786544] [G loss: 126853.304688] time: 0:26:11.822651\n",
      "[Epoch 83/200]  [mse: 24.783646] [mae: 17.239262] [mape: 8.769200] [G loss: 137505.671875] time: 0:26:30.551722\n",
      "[Epoch 84/200]  [mse: 26.698139] [mae: 18.559442] [mape: 9.617076] [G loss: 136794.687500] time: 0:26:49.799968\n",
      "[Epoch 85/200]  [mse: 22.462428] [mae: 15.886722] [mape: 8.941129] [G loss: 124409.578125] time: 0:27:09.119834\n",
      "[Epoch 86/200]  [mse: 23.454541] [mae: 16.155477] [mape: 8.282752] [G loss: 132362.484375] time: 0:27:28.186048\n",
      "[Epoch 87/200]  [mse: 18.258342] [mae: 12.843279] [mape: 8.915138] [G loss: 126979.070312] time: 0:27:47.188489\n",
      "[Epoch 88/200]  [mse: 28.550344] [mae: 20.148627] [mape: 10.070115] [G loss: 151967.890625] time: 0:28:06.222685\n",
      "[Epoch 89/200]  [mse: 19.084353] [mae: 13.526953] [mape: 10.386254] [G loss: 149229.906250] time: 0:28:25.696359\n",
      "[Epoch 90/200]  [mse: 19.264145] [mae: 14.006153] [mape: 10.334140] [G loss: 132864.234375] time: 0:28:45.065212\n",
      "[Epoch 91/200]  [mse: 22.422870] [mae: 16.120047] [mape: 9.966513] [G loss: 125484.437500] time: 0:29:04.976546\n",
      "[Epoch 92/200]  [mse: 22.794201] [mae: 16.370551] [mape: 10.009889] [G loss: 122099.453125] time: 0:29:24.210399\n",
      "[Epoch 93/200]  [mse: 22.378224] [mae: 16.062124] [mape: 9.926885] [G loss: 119945.226562] time: 0:29:43.805042\n",
      "[Epoch 94/200]  [mse: 21.868210] [mae: 15.645776] [mape: 9.798442] [G loss: 118112.679688] time: 0:30:03.388240\n",
      "[Epoch 95/200]  [mse: 21.886553] [mae: 15.621255] [mape: 9.573685] [G loss: 116244.750000] time: 0:30:22.235413\n",
      "[Epoch 96/200]  [mse: 21.985179] [mae: 15.708046] [mape: 9.834592] [G loss: 115721.804688] time: 0:30:41.064164\n",
      "[Epoch 97/200]  [mse: 24.994548] [mae: 17.828162] [mape: 9.370353] [G loss: 118445.140625] time: 0:30:59.924782\n",
      "[Epoch 98/200]  [mse: 20.607453] [mae: 14.558132] [mape: 9.543307] [G loss: 116878.031250] time: 0:31:19.359606\n",
      "[Epoch 99/200]  [mse: 27.364487] [mae: 19.338114] [mape: 9.211681] [G loss: 124173.789062] time: 0:31:38.584549\n",
      "[Epoch 100/200]  [mse: 21.345563] [mae: 14.986301] [mape: 8.870387] [G loss: 114494.046875] time: 0:31:57.957744\n",
      "[Epoch 101/200]  [mse: 21.292192] [mae: 15.001012] [mape: 10.488866] [G loss: 154064.859375] time: 0:32:17.341905\n",
      "[Epoch 102/200]  [mse: 26.502747] [mae: 19.219444] [mape: 10.369736] [G loss: 134701.015625] time: 0:32:36.385056\n",
      "[Epoch 103/200]  [mse: 26.645250] [mae: 19.128368] [mape: 10.077652] [G loss: 127945.906250] time: 0:32:55.759541\n",
      "[Epoch 104/200]  [mse: 26.410155] [mae: 18.846002] [mape: 9.926786] [G loss: 122786.164062] time: 0:33:15.087830\n",
      "[Epoch 105/200]  [mse: 25.991298] [mae: 18.457252] [mape: 9.617542] [G loss: 120522.859375] time: 0:33:34.165450\n",
      "[Epoch 106/200]  [mse: 24.919452] [mae: 17.818289] [mape: 9.786289] [G loss: 116359.851562] time: 0:33:53.273243\n",
      "[Epoch 107/200]  [mse: 24.572668] [mae: 17.271817] [mape: 9.309068] [G loss: 116103.593750] time: 0:34:12.596689\n",
      "[Epoch 108/200]  [mse: 22.276812] [mae: 15.792097] [mape: 9.443545] [G loss: 111814.906250] time: 0:34:32.168767\n",
      "[Epoch 109/200]  [mse: 23.368599] [mae: 16.591606] [mape: 9.624308] [G loss: 113485.812500] time: 0:34:51.902212\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-3f03ee3fa4c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_BATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-9a1242feb061>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_matrix, epochs, batch_size, learn_rate)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;31m#  训练 unet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;31m#  训练 Generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mg_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_volume\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_volume\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(X_train, epochs=200, batch_size=MAX_BATCH_SIZE, learn_rate=learn_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet.save_weights('./model/dataRecorvey20191109/runet_10_rmse11.97.h5')\n",
    "# unet.load_weights('./model/RUnet/unet_60epoch_18rmse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_masked_tmp = deepcopy(X_test)\n",
    "# test_true_volume_tmp = deepcopy(X_test[:, :, :, :1])\n",
    "\n",
    "# test_length_tmp = len(X_test)\n",
    "# rand_mask_tmp = random_mask(matrix_length, matrix_length, size=rand_size, channels=channel_num, smooth_time=smooth_time, type=mask_type, block_size=block_size)\n",
    "# test_mask_tmp = np.stack([rand_mask_tmp for _ in range(test_length_tmp)], axis=0)\n",
    "\n",
    "# test_traffic_mean_tmp = X_test[test_mask_tmp==1].mean()\n",
    "# test_masked_tmp[test_mask_tmp==0] = test_traffic_mean_tmp\n",
    "# test_history_volume_tmp = deepcopy(test_masked_tmp)\n",
    "\n",
    "\n",
    "# y_pred = unet.predict(test_history_volume_tmp)\n",
    "\n",
    "# # 仅对缺失数据进行l2评价。（对预测来说既对第一层进行评价，验证）\n",
    "# y_true = (1-test_mask[:,:,:,:1])*test_true_volume_tmp\n",
    "# y_pred = (1-test_mask[:,:,:,:1])*y_pred\n",
    "\n",
    "\n",
    "y_pred = unet.predict(test_history_volume)\n",
    "\n",
    "# 仅对缺失数据进行l2评价。（对预测来说既对第一层进行评价，验证）\n",
    "y_true = (1-test_mask[:,:,:,:1])*test_true_volume\n",
    "y_pred = (1-test_mask[:,:,:,:1])*y_pred\n",
    "\n",
    "l2(y_true, y_pred), l1(y_true, y_pred)\n",
    "# (13.612251463372885, 7.896321495663051)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_true==0] += 1\n",
    "y_true[y_true==0] += 1\n",
    "\n",
    "mape(y_true, y_pred)\n",
    "# 5.013244341615349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_model = build_unet()\n",
    "min_model.load_weights('./model/dataRecorvey20191109/tmp/min_runet.h5')\n",
    "\n",
    "y_pred = min_model.predict(test_history_volume)\n",
    "\n",
    "# 仅对缺失数据进行l2评价。（对预测来说既对第一层进行评价，验证）\n",
    "y_true = (1-test_mask[:,:,:,:1])*test_true_volume\n",
    "y_pred = (1-test_mask[:,:,:,:1])*y_pred\n",
    "\n",
    "l2(y_true, y_pred), l1(y_true, y_pred)\n",
    "# (11.085728026573435, 6.957813774885677)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[y_true==0] += 1\n",
    "y_true[y_true==0] += 1\n",
    "\n",
    "mape(y_true, y_pred)\n",
    "# 4.429743492502313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posX = 0\n",
    "posY = 2\n",
    "startX = 500\n",
    "gapX = 200\n",
    "\n",
    "y = test_true_volume[:, posX, posY, :][startX: startX+gapX]\n",
    "\n",
    "yf = y_pred[:, posX, posY, :][startX: startX+gapX]\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "fig, ax = plt.subplots(figsize=(25, 8))\n",
    "lines = plt.plot(x, yf, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "start_time = 1600\n",
    "y = y_true.reshape(-1,)[start_time: start_time+100]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[start_time: start_time+100]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 10))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi = l2_validation\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "lines = plt.plot(xi, yi, 'k^--', linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lr_step\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "lines = plt.plot(x, y, 'ko-', linewidth=1, markersize=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 3600\n",
    "y = y_true.reshape(-1,)[start_time: start_time+100]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[start_time: start_time+100]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 0\n",
    "y = y_true.reshape(-1,)[start_time: start_time+100]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[start_time: start_time+100]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_true_volume"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
