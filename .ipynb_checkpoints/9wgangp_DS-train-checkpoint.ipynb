{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, concatenate, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, Add, Subtract\n",
    "from keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D ,AveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, Nadam, RMSprop\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.engine.topology import Layer\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import gc\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import load_model  \n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载  预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交通矩阵为 matrix_length*matrix_length\n",
    "matrix_length = 32\n",
    "\n",
    "matrix_df = pd.read_csv('./data/trafficV_M.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createTrainArray(week_history_num=0, minute_history_num=0):\n",
    "#     week_delta_list = [pd.Timedelta(i+1, unit='W') for i in range(week_history_num)]\n",
    "#     minute_delta_list = [pd.Timedelta((i+1)*15, unit='m') for i in range(minute_history_num)]\n",
    "#     # 参考历史数据时间点list\n",
    "#     delta_list = week_delta_list+minute_delta_list\n",
    "#     print(delta_list)\n",
    "    \n",
    "#     set_up_time = pd.Timedelta(week_history_num, unit='W')\n",
    "#     # 根据历史数据选取多少，重新构建数据集\n",
    "#     # 相当于去除最开始week_history_num个周的数据，因为这些数据无法找到更前的数据\n",
    "#     train_df = matrix_df.truncate(before=matrix_df.index.min() + set_up_time)\n",
    "    \n",
    "#     train_ago_array_tuple = tuple([np.array(matrix_df.loc[train_df.index - i]).reshape(-1, matrix_length, matrix_length, 1) for i in delta_list])\n",
    "#     train_df = np.array(train_df).reshape(-1, matrix_length, matrix_length, 1)\n",
    "#     # concatenate保持 待修复数据在前，参考历史数据在后。与random_mask函数生成mask相一致\n",
    "#     train_array = np.concatenate((train_df,)+train_ago_array_tuple, axis=3)\n",
    "#     print(train_array.shape)\n",
    "#     return train_array\n",
    "\n",
    "\n",
    "\n",
    "def createTrainArray(week_history_num=0, minute_history_num=0):\n",
    "    week_delta_list = [pd.Timedelta(week_history_num-i, unit='W') for i in range(week_history_num)]\n",
    "    minute_delta_list = [pd.Timedelta((minute_history_num-i)*15, unit='m') for i in range(minute_history_num)]\n",
    "    # 参考历史数据时间点list\n",
    "    delta_list = minute_delta_list+week_delta_list\n",
    "    print(delta_list)\n",
    "    \n",
    "    set_up_time = pd.Timedelta(week_history_num, unit='W')\n",
    "    # 根据历史数据选取多少，重新构建数据集\n",
    "    # 相当于去除最开始week_history_num个周的数据，因为这些数据无法找到更前的数据\n",
    "    train_df = matrix_df.truncate(before=matrix_df.index.min() + set_up_time)\n",
    "    \n",
    "    train_ago_array_tuple = tuple([np.array(matrix_df.loc[train_df.index - i]).reshape(-1, matrix_length, matrix_length, 1) for i in delta_list])\n",
    "    train_df = np.array(train_df).reshape(-1, matrix_length, matrix_length, 1)\n",
    "    # concatenate保持 待修复数据在前，参考历史数据在后。与random_mask函数生成mask相一致\n",
    "    train_array = np.concatenate((train_df,)+train_ago_array_tuple, axis=3)\n",
    "    print(train_array.shape)\n",
    "    return train_array\n",
    "\n",
    "\n",
    "def normalization(matrix):\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(matrix.shape[-1]):\n",
    "            cur_time = matrix[i][:, :, j]\n",
    "#             mean_val = cur_time.mean()\n",
    "            mx = cur_time.max()\n",
    "            mn = cur_time.min()\n",
    "            matrix[i][:, :, j] = np.divide((cur_time-mn), (mx-mn))\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timedelta('0 days 00:45:00'), Timedelta('0 days 00:30:00'), Timedelta('0 days 00:15:00'), Timedelta('14 days 00:00:00'), Timedelta('7 days 00:00:00')]\n",
      "(16032, 32, 32, 6)\n"
     ]
    }
   ],
   "source": [
    "week_history_num = 2\n",
    "minute_history_num = 3\n",
    "\n",
    "channel_num = week_history_num +minute_history_num +1\n",
    "smooth_time = channel_num-1\n",
    "\n",
    "# train_array为(16704, 32, 32, 3)，16704个矩阵，32*32采集点，3从上到下为当前时间，上一周，上一15min\n",
    "train_array = createTrainArray(week_history_num, minute_history_num)\n",
    "X_train, X_test = train_test_split(train_array, test_size = 0.1, random_state=42, shuffle=False)\n",
    "# X_train, X_val = train_test_split(train_array, test_size = 0.1, random_state=42, shuffle=False) # 不shuffle可用于查看数据正确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14428, 32, 32, 6), (1604, 32, 32, 6))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_BATCH_SIZE = 64\n",
    "epoch_steps = X_train.shape[0] // MAX_BATCH_SIZE\n",
    "test_steps = X_test.shape[0] // MAX_BATCH_SIZE\n",
    "epoch_steps, test_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "def load_data(volume_matrix, batch_size=MAX_BATCH_SIZE):\n",
    "    n_batches=batch_size\n",
    "    len_of_matrix = len(volume_matrix)\n",
    "\n",
    "    batch_i = 0\n",
    "    while ((batch_i+1)*batch_size < len_of_matrix):\n",
    "        batch_matrix = volume_matrix[batch_i*batch_size: (batch_i+1)*batch_size]\n",
    "        true_volume, history_volume = batch_matrix[:, :, :, :1], batch_matrix[:, :, :, 1:]\n",
    "#         history_volume = normalization(history_volume)\n",
    "        batch_i+=1\n",
    "\n",
    "        yield true_volume, history_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def l2(y_true, y_pred):\n",
    "    return math.sqrt(np.sum(np.mean(np.square(y_true - y_pred), axis=0))/1024)\n",
    "\n",
    "def l1(y_true, y_pred):\n",
    "    return np.sum(np.mean(np.abs(y_true - y_pred), axis=0))/(matrix_length*matrix_length)\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return np.sum(np.mean((np.abs(y_true - y_pred)/y_true)*100, axis=0))/(matrix_length*matrix_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算D输出valid大小（PatchGAN）\n",
    "patch = 32\n",
    "disc_patch = (patch, patch, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (3, 3)\n",
    "g_filters_base = 32\n",
    "DropoutRatio = 0\n",
    "learn_rate_g = 0.0002\n",
    "learn_rate_d = 0.0008\n",
    "learn_rate_c = 0.0002\n",
    "\n",
    "# channels = 3\n",
    "matrix_shape = (matrix_length, matrix_length, channel_num)\n",
    "true_volume_shape = (matrix_length, matrix_length, 1)\n",
    "history_volume_shape = (matrix_length, matrix_length, channel_num-1)\n",
    "\n",
    "kernel_init = 'glorot_uniform'\n",
    "bias_init = 'zeros'\n",
    "kernel_regul = regularizers.l2(1)\n",
    "activity_regul = regularizers.l2(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet block\n",
    "def identity_block(X, filters, f):\n",
    "\n",
    "    F1, F2 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(filters=F1, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(X)\n",
    "\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(X)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "# ENCODER\n",
    "def encoder_layer(img_in, filters, kernel_size, bn=True, resid=True):\n",
    "    # conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding='same')(img_in)\n",
    "    conv = img_in\n",
    "    if bn:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "#             conv = MaxPooling2D((2, 2))(conv)\n",
    "\n",
    "\n",
    "    if resid:\n",
    "        conv = identity_block(conv, (filters, filters), kernel_size)\n",
    "\n",
    "    return conv\n",
    "\n",
    "# DECODER\n",
    "def decoder_layer(img_in, e_conv, filters, kernel_size, bn=True, resid=True):\n",
    "    # up_img = UpSampling2D(size=(2,2))(img_in)\n",
    "    up_img = img_in\n",
    "    concat_img = Concatenate(axis=3)([e_conv,up_img])\n",
    "    conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding='same',\n",
    "                  kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(concat_img)\n",
    "    if bn:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    conv = LeakyReLU(alpha=0)(conv)\n",
    "\n",
    "    if resid:\n",
    "        conv = identity_block(conv, (filters, filters), kernel_size)\n",
    "    return conv\n",
    "\n",
    "\n",
    "\n",
    "def build_generator():      \n",
    "\n",
    "    # INPUTS\n",
    "    history_traffic_volume = Input(shape=history_volume_shape)\n",
    "\n",
    "    # kernel_init = initializers.he_normal()\n",
    "    # bias_init = initializers.he_normal()\n",
    "    kernel_init = 'glorot_uniform'\n",
    "    bias_init = 'zeros'\n",
    "\n",
    "#         kernel_init = initializers.he_uniform()\n",
    "#         bias_init = 'Orthogonal'\n",
    "    kernel_regul = regularizers.l2(1)\n",
    "    activity_regul = regularizers.l2(1)\n",
    "\n",
    "    filters_base = 32\n",
    "    e_conv1_head = Conv2D(filters=filters_base, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(history_traffic_volume)\n",
    "#         e_conv1_head = Conv2D(filters=filters_base*1, kernel_size=3, strides=1, padding='same',\n",
    "#                               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "#                       kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv1_head)\n",
    "    e_conv1_tail = AveragePooling2D((2, 2))(e_conv1_head)\n",
    "#     e_conv1_tail = Dropout(DropoutRatio/2)(e_conv1_tail)\n",
    "    e_conv1 = encoder_layer(e_conv1_tail, filters_base, 3, bn=False)\n",
    "\n",
    "    e_conv2_head = Conv2D(filters=filters_base*2, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv1)\n",
    "    e_conv2_tail = AveragePooling2D((2, 2))(e_conv2_head)\n",
    "#     e_conv2_tail = Dropout(DropoutRatio)(e_conv2_tail)\n",
    "    e_conv2 = encoder_layer(e_conv2_tail, filters_base*2, 3)\n",
    "\n",
    "    e_conv3_head = Conv2D(filters=filters_base*4, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv2)\n",
    "    e_conv3_tail = AveragePooling2D((2, 2))(e_conv3_head)\n",
    "    \n",
    "    # 加drop引入噪声\n",
    "#     e_conv3_tail = Dropout(DropoutRatio)(e_conv3_tail)\n",
    "    \n",
    "    d_conv3_head = encoder_layer(e_conv3_tail, filters_base*4, 3)\n",
    "    resid1 = Subtract()([e_conv3_tail, d_conv3_head])\n",
    "    d_conv3_tail = UpSampling2D(size=(2, 2))(resid1)\n",
    "#     d_conv3_tail = Dropout(DropoutRatio)(d_conv3_tail)\n",
    "\n",
    "\n",
    "    d_conv4_head = decoder_layer(d_conv3_tail, e_conv3_head, filters_base*2, 3)\n",
    "    resid2 = Subtract()([d_conv4_head, e_conv2_tail])\n",
    "    d_conv4_tail = UpSampling2D(size=(2, 2))(resid2)\n",
    "#     d_conv4_tail = Dropout(DropoutRatio)(d_conv4_tail)\n",
    "\n",
    "\n",
    "    d_conv5_head = decoder_layer(d_conv4_tail, e_conv2_head, filters_base*1, 3)\n",
    "    resid3 = Subtract()([d_conv5_head, e_conv1_tail])\n",
    "    d_conv5_tail = UpSampling2D(size=(2, 2))(resid3)\n",
    "#     d_conv5_tail = Dropout(DropoutRatio)(d_conv5_tail)\n",
    "\n",
    "    d_conv6_head = decoder_layer(d_conv5_tail, e_conv1_head, filters_base//2, 3, bn=False)\n",
    "\n",
    "\n",
    "    outputs = Conv2D(1, 1, activation = 'relu', kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(d_conv6_head)\n",
    "\n",
    "    # Setup the model inputs / outputs\n",
    "    model = Model(inputs=history_traffic_volume, outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer = Adam(lr=learn_rate_g),\n",
    "        loss='mse'\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples):\n",
    "    \"\"\"\n",
    "    Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "    \"\"\"\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    # compute the euclidean norm by squaring ...\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    #   ... summing over the rows ...\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    #   ... and sqrt\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "    gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "    # return the mean as loss over all the batch samples\n",
    "    return K.mean(gradient_penalty)\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "def neg_wasserstein_loss(y_true, y_pred):\n",
    "    return -K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_filters_base = 32\n",
    "# Input shape\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator():\n",
    "    def d_layer(layer_input, filters, f_size=3, bn=True, stride=1):\n",
    "        \"\"\"Discriminator layer\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=stride, padding='same', kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(layer_input)\n",
    "        if bn:\n",
    "            d = BatchNormalization()(d)\n",
    "        d = LeakyReLU(alpha=0.1)(d)\n",
    "        return d\n",
    "    \n",
    "    matrix_A = Input(shape=true_volume_shape)\n",
    "    matrix_B = Input(shape=history_volume_shape)\n",
    "\n",
    "    # Concatenate image and conditioning image生成输入对象\n",
    "    combined_matrix = Concatenate(axis=-1)([matrix_A, matrix_B])\n",
    "\n",
    "    d1 = d_layer(combined_matrix, d_filters_base, bn=False)\n",
    "    d2 = d_layer(d1, d_filters_base*2)\n",
    "#     d2 = AveragePooling2D((2, 2))(d2)\n",
    "    d3 = d_layer(d2, d_filters_base*4)\n",
    "#     d3 = AveragePooling2D((2, 2))(d3)\n",
    "    d4 = d_layer(d3, d_filters_base*8)\n",
    "#     d4 = AveragePooling2D((2, 2))(d4)\n",
    "    d4 = d_layer(d4, d_filters_base*4)\n",
    "    d5 = d_layer(d4, d_filters_base*2)\n",
    "    d6 = d_layer(d5, d_filters_base*1)\n",
    "    \n",
    "    validity = Conv2D(1, kernel_size=3, strides=1, padding='same')(d6)\n",
    "    model = Model([matrix_A, matrix_B], validity)\n",
    "    model.compile(optimizer=Adam(lr=learn_rate_d), loss=wasserstein_loss, metrics=['mse'])   #binary_crossentropy\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((64, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "    \n",
    "    \n",
    "class GradNorm(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GradNorm, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(GradNorm, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        vaild_interpolated, interpolation_volume = inputs\n",
    "        grads = K.gradients(vaild_interpolated, interpolation_volume)\n",
    "        assert len(grads) == 1\n",
    "        grad = grads[0]\n",
    "#         a = K.sqrt(K.sum(K.batch_flatten(K.square(grad)), axis=1, keepdims=True))\n",
    "        return grad\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (MAX_BATCH_SIZE,) + true_volume_shape\n",
    "    \n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        return (MAX_BATCH_SIZE,) + true_volume_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "discriminator = build_discriminator()\n",
    "generator = build_generator()\n",
    "\n",
    "\n",
    "true_volume = Input(shape=true_volume_shape)\n",
    "history_volume = Input(shape=history_volume_shape)\n",
    "interpolation_volume = Input(shape=true_volume_shape)\n",
    "\n",
    "forecast_volume = generator(history_volume)\n",
    "\n",
    "discriminator.trainable = False\n",
    "true_vaild = discriminator([true_volume, history_volume])\n",
    "fake_vaild = discriminator([forecast_volume, history_volume])\n",
    "\n",
    "# gp = gradient_penalty_loss(true_volume, forecast_volume, interpolation_volume)\n",
    "norm = GradNorm()([discriminator([interpolation_volume, history_volume]), interpolation_volume])\n",
    "\n",
    "combined = Model(inputs=[true_volume, history_volume, interpolation_volume],\n",
    "                    outputs=[true_vaild, fake_vaild, norm, forecast_volume])\n",
    "combined.compile(loss=[wasserstein_loss,\n",
    "                        neg_wasserstein_loss,\n",
    "                       'mse',\n",
    "                        'mse'],\n",
    "                        optimizer=RMSprop(lr=learn_rate_c),\n",
    "                        loss_weights=[1, 1, 5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_step = []\n",
    "l2_validation = []\n",
    "\n",
    "def train(train_matrix, epochs, batch_size=MAX_BATCH_SIZE, learn_rate=0.01):\n",
    "\n",
    "    min_mse = 999\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(\"train start \"+str(start_time))\n",
    "\n",
    "    # Adversarial loss ground truths\n",
    "#     valid = np.ones((MAX_BATCH_SIZE,) + disc_patch)+np.random.rand(MAX_BATCH_SIZE, patch, patch, 1)/5\n",
    "#     fake = np.zeros((MAX_BATCH_SIZE,) + disc_patch)+np.random.rand(MAX_BATCH_SIZE, patch, patch, 1)/5\n",
    "    valid = np.ones((MAX_BATCH_SIZE,) + disc_patch)\n",
    "    fake = -np.ones((MAX_BATCH_SIZE,) + disc_patch)\n",
    "    dummy = np.zeros((MAX_BATCH_SIZE,) + true_volume_shape)\n",
    "\n",
    "    #　周期修改学习率　https://zhuanlan.zhihu.com/p/52084949\n",
    "    for epoch in range(epochs):\n",
    "#         if epoch>=100 and epoch % 5 == 0 and epoch != 0:\n",
    "#             generator_lr = K.get_value(generator.optimizer.lr)\n",
    "#             discriminator_lr = K.get_value(discriminator.optimizer.lr)\n",
    "#             combined_lr = K.get_value(combined.optimizer.lr)\n",
    "#             if generator_lr>0.0001:\n",
    "#                 K.set_value(generator.optimizer.lr, generator_lr*0.9)\n",
    "#             if discriminator_lr>0.0005:\n",
    "#                 K.set_value(discriminator.optimizer.lr, discriminator_lr*0.9)\n",
    "#             if combined_lr>0.0001:\n",
    "#                 K.set_value(combined.optimizer.lr, combined_lr*0.9)\n",
    "\n",
    "        for batch_i, (true_volume, history_volume) in enumerate(load_data(train_matrix,batch_size)):\n",
    "            # true_volume 真实待预测路网交通量  history_volume 路网交通量历史数据\n",
    "            #  训练 Discriminator\n",
    "\n",
    "            # 根据历史数据生成预测数据\n",
    "            forecast_volume = generator.predict(history_volume)\n",
    "\n",
    "            # 训练 the discriminators (original images = real / generated = Fake)\n",
    "            discriminator.trainable = True\n",
    "            d_loss_real = discriminator.train_on_batch([true_volume, history_volume], valid)\n",
    "            d_loss_fake = discriminator.train_on_batch([forecast_volume, history_volume], fake)\n",
    "            discriminator.trainable = False\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            \n",
    "\n",
    "            epsilon = np.random.uniform(0, 1, size=(MAX_BATCH_SIZE,1,1,1))\n",
    "            interpolation_volume = epsilon*true_volume + (1-epsilon)*forecast_volume\n",
    "            #  训练 Generator\n",
    "            g_loss = combined.train_on_batch([true_volume, history_volume, interpolation_volume], [valid, fake, dummy, true_volume])\n",
    "\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "        # Plot the progress\n",
    "        y_pred = generator.predict(X_test[:, :, :, 1:])\n",
    "        y_true = X_test[:, :, :, :1]\n",
    "\n",
    "        l2_epoch_validation = l2(y_true, y_pred)\n",
    "        l1_epoch_validation = l1(y_true, y_pred)\n",
    "        \n",
    "        y_pred[y_true==0] += 1\n",
    "        y_true[y_true==0] += 1\n",
    "        mape_epoch_validation = mape(y_true, y_pred)\n",
    "        \n",
    "#         lr_step.append(K.get_value(discriminator.optimizer.lr))\n",
    "#         if(l2_epoch_validation<12 and l2_epoch_validation>11.8 and l2_epoch_validation < min_mse):\n",
    "#             generator.save_weights('./model/wganpg/tmp/min_generator_wganpg.h5')\n",
    "#             discriminator.save_weights('./model/wganpg/tmp/min_discriminator_wganpg.h5')\n",
    "#             combined.save_weights('./model/wganpg/tmp/min_combined_wganpg.h5')\n",
    "#             min_mse = l2_epoch_validation\n",
    "            \n",
    "        l2_validation.append(l2_epoch_validation)\n",
    "        if epoch%1==0:\n",
    "#             print(\"unet lr:\"+ str(K.get_value(unet.optimizer.lr)))\n",
    "            print (\"[Epoch %d/%d]  [D loss: %f, mse: %f] [mae: %f] [mape: %f] [G loss: %f] time: %s\" % (epoch+1, epochs,\n",
    "                                                                    d_loss[0], l2_epoch_validation,\n",
    "                                                                    l1_epoch_validation,\n",
    "                                                                    mape_epoch_validation,\n",
    "                                                                    g_loss[0],\n",
    "                                                                    elapsed_time))\n",
    "        # If at show interval => show generated image samples\n",
    "#             if epoch % show_interval == 0:\n",
    "#                     show_images(dataset_name,epoch, batch_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start 2020-03-13 16:58:52.848398\n",
      "[Epoch 1/200]  [D loss: 1.646344, mse: 53.245445] [mae: 36.763697] [mape: 17.399804] [G loss: 29459.822266] time: 0:01:48.357559\n",
      "[Epoch 2/200]  [D loss: 5.655570, mse: 50.665678] [mae: 36.123093] [mape: 16.996357] [G loss: 25395.671875] time: 0:03:11.478188\n",
      "[Epoch 3/200]  [D loss: 0.667408, mse: 49.113445] [mae: 35.483998] [mape: 16.137264] [G loss: 24231.757812] time: 0:04:33.904138\n",
      "[Epoch 4/200]  [D loss: 0.297146, mse: 63.571240] [mae: 47.675323] [mape: 21.060220] [G loss: 37117.640625] time: 0:05:56.343403\n",
      "[Epoch 5/200]  [D loss: 0.875107, mse: 22.281084] [mae: 13.489203] [mape: 7.424766] [G loss: 6451.174805] time: 0:07:18.482981\n",
      "[Epoch 6/200]  [D loss: 0.212064, mse: 21.198098] [mae: 12.837059] [mape: 6.936449] [G loss: 5801.544434] time: 0:08:42.900880\n",
      "[Epoch 7/200]  [D loss: 0.172886, mse: 61.072802] [mae: 45.644587] [mape: 18.149876] [G loss: 14945.445312] time: 0:10:07.967855\n",
      "[Epoch 8/200]  [D loss: 0.235722, mse: 54.138429] [mae: 40.206087] [mape: 15.900884] [G loss: 13065.304688] time: 0:11:31.047396\n",
      "[Epoch 9/200]  [D loss: 0.753207, mse: 36.666957] [mae: 26.222512] [mape: 11.077892] [G loss: 6836.595215] time: 0:12:54.992625\n",
      "[Epoch 10/200]  [D loss: 0.312669, mse: 32.225974] [mae: 22.636825] [mape: 9.639012] [G loss: 5786.512695] time: 0:14:17.739481\n",
      "[Epoch 11/200]  [D loss: 0.565439, mse: 29.791314] [mae: 20.576177] [mape: 8.723319] [G loss: 5133.312988] time: 0:15:41.361811\n",
      "[Epoch 12/200]  [D loss: 1.671096, mse: 23.073485] [mae: 15.022188] [mape: 6.827246] [G loss: 4305.666016] time: 0:17:05.160405\n",
      "[Epoch 13/200]  [D loss: 0.498880, mse: 23.370796] [mae: 15.295525] [mape: 6.781531] [G loss: 4176.718750] time: 0:18:28.476523\n",
      "[Epoch 14/200]  [D loss: 0.641946, mse: 17.898062] [mae: 10.884572] [mape: 5.751038] [G loss: 3848.878662] time: 0:19:51.863816\n",
      "[Epoch 15/200]  [D loss: 0.434171, mse: 22.814223] [mae: 14.985454] [mape: 6.638691] [G loss: 3961.623291] time: 0:21:15.469212\n",
      "[Epoch 16/200]  [D loss: 0.552830, mse: 22.373506] [mae: 14.669113] [mape: 6.494324] [G loss: 3819.947754] time: 0:22:39.139985\n",
      "[Epoch 17/200]  [D loss: 0.401157, mse: 26.632006] [mae: 18.215544] [mape: 7.542033] [G loss: 4126.835938] time: 0:24:04.449736\n",
      "[Epoch 18/200]  [D loss: 5.168516, mse: 19.161646] [mae: 12.039227] [mape: 5.773106] [G loss: 3491.098877] time: 0:25:29.483669\n",
      "[Epoch 19/200]  [D loss: 0.679143, mse: 27.349310] [mae: 18.580785] [mape: 7.460567] [G loss: 3882.484619] time: 0:26:54.422052\n",
      "[Epoch 20/200]  [D loss: 0.156254, mse: 15.911820] [mae: 9.633945] [mape: 5.408217] [G loss: 3339.597656] time: 0:28:17.215078\n",
      "[Epoch 21/200]  [D loss: 0.352169, mse: 23.672876] [mae: 15.653387] [mape: 6.653715] [G loss: 3644.979248] time: 0:29:40.332320\n",
      "[Epoch 22/200]  [D loss: 0.249689, mse: 23.501917] [mae: 15.597505] [mape: 6.683216] [G loss: 3597.437256] time: 0:31:02.599412\n",
      "[Epoch 23/200]  [D loss: 0.437612, mse: 18.798481] [mae: 12.233537] [mape: 6.984235] [G loss: 3557.275391] time: 0:32:25.191976\n",
      "[Epoch 24/200]  [D loss: 0.647542, mse: 18.575549] [mae: 11.756722] [mape: 5.613953] [G loss: 3263.355225] time: 0:33:49.224737\n",
      "[Epoch 25/200]  [D loss: 0.393361, mse: 24.016547] [mae: 16.551473] [mape: 8.332110] [G loss: 4189.031250] time: 0:35:12.080918\n",
      "[Epoch 26/200]  [D loss: 0.488455, mse: 15.204784] [mae: 9.239001] [mape: 5.062729] [G loss: 3147.729736] time: 0:36:34.792145\n",
      "[Epoch 27/200]  [D loss: 0.197864, mse: 15.023843] [mae: 9.131202] [mape: 4.995033] [G loss: 3076.044678] time: 0:37:58.212238\n",
      "[Epoch 28/200]  [D loss: 0.320836, mse: 16.259892] [mae: 10.043341] [mape: 5.067630] [G loss: 3000.911865] time: 0:39:21.669344\n",
      "[Epoch 29/200]  [D loss: 0.294730, mse: 15.041079] [mae: 9.180933] [mape: 4.989263] [G loss: 3016.145020] time: 0:40:44.371819\n",
      "[Epoch 30/200]  [D loss: 0.256724, mse: 15.094688] [mae: 9.331598] [mape: 5.065363] [G loss: 2979.635010] time: 0:42:07.303807\n",
      "[Epoch 31/200]  [D loss: 0.369473, mse: 14.859922] [mae: 9.107721] [mape: 4.922290] [G loss: 2902.644043] time: 0:43:31.021182\n",
      "[Epoch 32/200]  [D loss: 0.602279, mse: 14.640091] [mae: 8.908709] [mape: 4.848882] [G loss: 2833.543457] time: 0:44:53.429004\n",
      "[Epoch 33/200]  [D loss: 0.482707, mse: 14.699093] [mae: 8.962876] [mape: 4.825939] [G loss: 2820.500000] time: 0:46:15.795541\n",
      "[Epoch 34/200]  [D loss: 0.299301, mse: 14.425883] [mae: 8.769764] [mape: 4.787149] [G loss: 2748.435059] time: 0:47:38.441459\n",
      "[Epoch 35/200]  [D loss: 0.198328, mse: 14.850041] [mae: 9.025982] [mape: 4.734278] [G loss: 2743.476807] time: 0:49:01.010141\n",
      "[Epoch 36/200]  [D loss: 0.447871, mse: 14.131126] [mae: 8.585993] [mape: 4.639848] [G loss: 2701.524414] time: 0:50:23.600040\n",
      "[Epoch 37/200]  [D loss: 0.490739, mse: 15.090400] [mae: 9.496351] [mape: 4.987746] [G loss: 2683.494629] time: 0:51:46.194331\n",
      "[Epoch 38/200]  [D loss: 0.214947, mse: 13.968082] [mae: 8.480267] [mape: 4.571493] [G loss: 2587.339600] time: 0:53:09.139907\n",
      "[Epoch 39/200]  [D loss: 0.282111, mse: 14.321366] [mae: 8.781174] [mape: 4.715066] [G loss: 2610.368408] time: 0:54:32.446741\n",
      "[Epoch 40/200]  [D loss: 0.276320, mse: 13.920539] [mae: 8.456276] [mape: 4.556291] [G loss: 2599.615234] time: 0:55:55.464106\n",
      "[Epoch 41/200]  [D loss: 0.183539, mse: 13.741421] [mae: 8.405012] [mape: 4.561193] [G loss: 2559.877197] time: 0:57:18.552226\n",
      "[Epoch 42/200]  [D loss: 0.147697, mse: 13.809655] [mae: 8.408109] [mape: 4.500098] [G loss: 2527.868164] time: 0:58:41.524491\n",
      "[Epoch 43/200]  [D loss: 0.178335, mse: 14.619248] [mae: 9.132483] [mape: 4.682940] [G loss: 2461.449219] time: 1:00:04.172647\n",
      "[Epoch 44/200]  [D loss: 0.226805, mse: 18.224682] [mae: 12.275212] [mape: 5.828496] [G loss: 2970.733887] time: 1:01:27.076932\n",
      "[Epoch 45/200]  [D loss: 0.298795, mse: 13.473836] [mae: 8.167527] [mape: 4.387777] [G loss: 2443.621826] time: 1:02:51.628709\n",
      "[Epoch 46/200]  [D loss: 2.013546, mse: 13.404233] [mae: 8.200663] [mape: 4.393299] [G loss: 2446.833740] time: 1:04:16.428375\n",
      "[Epoch 47/200]  [D loss: 0.330360, mse: 13.499464] [mae: 8.283721] [mape: 4.422802] [G loss: 2400.799316] time: 1:05:39.931647\n",
      "[Epoch 48/200]  [D loss: 0.174377, mse: 13.795038] [mae: 8.424328] [mape: 4.454773] [G loss: 2394.371094] time: 1:07:03.568739\n",
      "[Epoch 49/200]  [D loss: 0.154267, mse: 13.494899] [mae: 8.240185] [mape: 4.369284] [G loss: 2348.156006] time: 1:08:27.262600\n",
      "[Epoch 50/200]  [D loss: 0.086142, mse: 13.660273] [mae: 8.366364] [mape: 4.336781] [G loss: 2352.690186] time: 1:09:51.124218\n",
      "[Epoch 51/200]  [D loss: 0.100372, mse: 13.064968] [mae: 7.987431] [mape: 4.313526] [G loss: 2317.595703] time: 1:11:14.782640\n",
      "[Epoch 52/200]  [D loss: 0.087617, mse: 12.825113] [mae: 7.823035] [mape: 4.151529] [G loss: 2293.942627] time: 1:12:38.715511\n",
      "[Epoch 53/200]  [D loss: 0.122925, mse: 12.760621] [mae: 7.699820] [mape: 4.212138] [G loss: 2281.245605] time: 1:14:02.582204\n",
      "[Epoch 54/200]  [D loss: 0.181297, mse: 20.896847] [mae: 14.218150] [mape: 6.073478] [G loss: 3177.190430] time: 1:15:26.734041\n",
      "[Epoch 55/200]  [D loss: 0.232337, mse: 12.848902] [mae: 7.738182] [mape: 4.114937] [G loss: 2241.352051] time: 1:16:50.858567\n",
      "[Epoch 56/200]  [D loss: 0.136650, mse: 13.157724] [mae: 8.100257] [mape: 4.243641] [G loss: 2341.142578] time: 1:18:14.872477\n",
      "[Epoch 57/200]  [D loss: 0.103877, mse: 13.038506] [mae: 8.037378] [mape: 4.150291] [G loss: 2193.550781] time: 1:19:38.587595\n",
      "[Epoch 58/200]  [D loss: 0.075630, mse: 13.847710] [mae: 8.629138] [mape: 4.408850] [G loss: 2337.536133] time: 1:21:02.768849\n",
      "[Epoch 59/200]  [D loss: 0.067520, mse: 12.473894] [mae: 7.493533] [mape: 4.021208] [G loss: 2117.802246] time: 1:22:26.725300\n",
      "[Epoch 60/200]  [D loss: 0.198096, mse: 12.594535] [mae: 7.688359] [mape: 4.143941] [G loss: 2206.331543] time: 1:23:50.922562\n",
      "[Epoch 61/200]  [D loss: 0.170476, mse: 14.049815] [mae: 9.048910] [mape: 4.755822] [G loss: 2496.023193] time: 1:25:14.645383\n",
      "[Epoch 62/200]  [D loss: 0.161103, mse: 17.985983] [mae: 12.259661] [mape: 5.515644] [G loss: 2495.539551] time: 1:26:38.554264\n",
      "[Epoch 63/200]  [D loss: 0.106017, mse: 18.709075] [mae: 12.850938] [mape: 5.733511] [G loss: 2635.685547] time: 1:28:02.635121\n",
      "[Epoch 64/200]  [D loss: 0.088615, mse: 17.251734] [mae: 11.471721] [mape: 5.118386] [G loss: 2658.903320] time: 1:29:26.396476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 65/200]  [D loss: 0.065727, mse: 12.035561] [mae: 7.220453] [mape: 3.940890] [G loss: 1990.071411] time: 1:30:50.089033\n",
      "[Epoch 66/200]  [D loss: 0.081313, mse: 16.374445] [mae: 11.048631] [mape: 5.339561] [G loss: 2140.545410] time: 1:32:14.077257\n",
      "[Epoch 67/200]  [D loss: 0.076303, mse: 12.197027] [mae: 7.296877] [mape: 3.889667] [G loss: 1913.024658] time: 1:33:37.472724\n",
      "[Epoch 68/200]  [D loss: 0.095313, mse: 12.594722] [mae: 7.717288] [mape: 4.079728] [G loss: 2023.907959] time: 1:35:01.117801\n",
      "[Epoch 69/200]  [D loss: 0.169015, mse: 11.985623] [mae: 7.324794] [mape: 3.971706] [G loss: 1920.548706] time: 1:36:24.967930\n",
      "[Epoch 70/200]  [D loss: 0.083257, mse: 13.288187] [mae: 8.246024] [mape: 4.089306] [G loss: 1913.522461] time: 1:37:49.106784\n",
      "[Epoch 71/200]  [D loss: 0.113401, mse: 12.735268] [mae: 7.735425] [mape: 3.957874] [G loss: 1882.559692] time: 1:39:12.742635\n",
      "[Epoch 72/200]  [D loss: 0.082792, mse: 12.022417] [mae: 7.221863] [mape: 3.889620] [G loss: 1873.556763] time: 1:40:36.637784\n",
      "[Epoch 73/200]  [D loss: 0.102769, mse: 13.107582] [mae: 8.289918] [mape: 4.288859] [G loss: 2139.699951] time: 1:42:00.650545\n",
      "[Epoch 74/200]  [D loss: 0.069023, mse: 13.784686] [mae: 8.801329] [mape: 4.516485] [G loss: 2009.296753] time: 1:43:24.606125\n",
      "[Epoch 75/200]  [D loss: 0.052912, mse: 12.235078] [mae: 7.359094] [mape: 3.873177] [G loss: 1820.798462] time: 1:44:48.471247\n",
      "[Epoch 76/200]  [D loss: 0.197244, mse: 12.162115] [mae: 7.262530] [mape: 3.864784] [G loss: 1884.861206] time: 1:46:12.447382\n",
      "[Epoch 77/200]  [D loss: 0.075830, mse: 11.783501] [mae: 6.941297] [mape: 3.724213] [G loss: 1780.692261] time: 1:47:36.126117\n",
      "[Epoch 78/200]  [D loss: 0.094629, mse: 11.842697] [mae: 7.005468] [mape: 3.746281] [G loss: 1770.215210] time: 1:49:00.143178\n",
      "[Epoch 79/200]  [D loss: 0.079917, mse: 11.876196] [mae: 7.026824] [mape: 3.743711] [G loss: 1754.867065] time: 1:50:24.206365\n",
      "[Epoch 80/200]  [D loss: 0.169515, mse: 20.675440] [mae: 14.710148] [mape: 6.608265] [G loss: 2338.421387] time: 1:51:48.108844\n",
      "[Epoch 81/200]  [D loss: 0.374522, mse: 11.879158] [mae: 6.993725] [mape: 3.675638] [G loss: 1739.479614] time: 1:53:12.022975\n",
      "[Epoch 82/200]  [D loss: 0.077585, mse: 12.921799] [mae: 8.137791] [mape: 4.076234] [G loss: 1826.763672] time: 1:54:36.092098\n",
      "[Epoch 83/200]  [D loss: 0.082896, mse: 14.230273] [mae: 9.251198] [mape: 4.505683] [G loss: 1974.586914] time: 1:56:00.054814\n",
      "[Epoch 84/200]  [D loss: 0.069136, mse: 12.501628] [mae: 7.537306] [mape: 3.939219] [G loss: 1794.660645] time: 1:57:24.177714\n",
      "[Epoch 85/200]  [D loss: 0.269498, mse: 13.905884] [mae: 8.623081] [mape: 4.203893] [G loss: 2309.440918] time: 1:58:47.953726\n",
      "[Epoch 86/200]  [D loss: 0.077763, mse: 13.080283] [mae: 8.202030] [mape: 4.325175] [G loss: 1872.475342] time: 2:00:11.926293\n",
      "[Epoch 87/200]  [D loss: 0.070117, mse: 12.423985] [mae: 7.384693] [mape: 3.809229] [G loss: 1706.364014] time: 2:01:35.588727\n",
      "[Epoch 88/200]  [D loss: 0.108158, mse: 12.896526] [mae: 8.043192] [mape: 4.121585] [G loss: 1854.371094] time: 2:02:59.611698\n",
      "[Epoch 89/200]  [D loss: 0.065406, mse: 12.345248] [mae: 7.530882] [mape: 3.954940] [G loss: 1778.488403] time: 2:04:23.629135\n",
      "[Epoch 90/200]  [D loss: 0.254628, mse: 11.993011] [mae: 7.068501] [mape: 3.744880] [G loss: 1646.021240] time: 2:05:47.434151\n",
      "[Epoch 91/200]  [D loss: 0.070443, mse: 16.030529] [mae: 10.742381] [mape: 5.122006] [G loss: 2153.558594] time: 2:07:11.335126\n",
      "[Epoch 92/200]  [D loss: 0.093401, mse: 11.855484] [mae: 7.163529] [mape: 3.699182] [G loss: 1623.379883] time: 2:08:35.082974\n",
      "[Epoch 93/200]  [D loss: 0.092387, mse: 15.319730] [mae: 9.986717] [mape: 4.557984] [G loss: 2095.610107] time: 2:09:58.728973\n",
      "[Epoch 94/200]  [D loss: 0.076344, mse: 12.293285] [mae: 7.543786] [mape: 3.812937] [G loss: 1670.567383] time: 2:11:22.743978\n",
      "[Epoch 95/200]  [D loss: 0.090365, mse: 11.648846] [mae: 6.929865] [mape: 3.651430] [G loss: 1593.933716] time: 2:12:46.693774\n",
      "[Epoch 96/200]  [D loss: 0.106400, mse: 11.806125] [mae: 6.917862] [mape: 3.667720] [G loss: 1611.727661] time: 2:14:10.603433\n",
      "[Epoch 97/200]  [D loss: 0.487562, mse: 11.426929] [mae: 6.754371] [mape: 3.562343] [G loss: 1624.680664] time: 2:15:34.518210\n",
      "[Epoch 98/200]  [D loss: 0.154173, mse: 15.001524] [mae: 9.986312] [mape: 4.925387] [G loss: 2490.924316] time: 2:16:58.370480\n",
      "[Epoch 99/200]  [D loss: 0.085273, mse: 12.042722] [mae: 7.369070] [mape: 3.728347] [G loss: 1580.932617] time: 2:18:22.706585\n",
      "[Epoch 100/200]  [D loss: 0.094269, mse: 12.119781] [mae: 7.461507] [mape: 3.700707] [G loss: 1608.021240] time: 2:19:46.140539\n",
      "[Epoch 101/200]  [D loss: 0.078739, mse: 19.451598] [mae: 13.707348] [mape: 6.244657] [G loss: 2229.691650] time: 2:21:10.005582\n",
      "[Epoch 102/200]  [D loss: 0.134124, mse: 11.387727] [mae: 6.740227] [mape: 3.682500] [G loss: 1576.045044] time: 2:22:33.884364\n",
      "[Epoch 103/200]  [D loss: 0.039226, mse: 11.506116] [mae: 6.877183] [mape: 3.621139] [G loss: 1603.314453] time: 2:23:57.584399\n",
      "[Epoch 104/200]  [D loss: 0.312823, mse: 12.780759] [mae: 7.949096] [mape: 3.913071] [G loss: 1609.513672] time: 2:25:21.254991\n",
      "[Epoch 105/200]  [D loss: 0.085565, mse: 11.511146] [mae: 6.884400] [mape: 3.588829] [G loss: 1527.615845] time: 2:26:45.287076\n",
      "[Epoch 106/200]  [D loss: 0.048092, mse: 12.299297] [mae: 7.399739] [mape: 3.728319] [G loss: 1562.867432] time: 2:28:09.182159\n",
      "[Epoch 107/200]  [D loss: 0.058693, mse: 13.016295] [mae: 8.028201] [mape: 3.978906] [G loss: 1650.530396] time: 2:29:32.727313\n",
      "[Epoch 108/200]  [D loss: 0.072265, mse: 11.989815] [mae: 7.171527] [mape: 3.800350] [G loss: 1677.156860] time: 2:30:56.830269\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-f2845cdbc646>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_BATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearn_rate_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-67-54ab97db16aa>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_matrix, epochs, batch_size, learn_rate)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0md_loss_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrue_volume\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_volume\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0md_loss_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mforecast_volume\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_volume\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_loss_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_loss_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(X_train, epochs=200, batch_size=MAX_BATCH_SIZE, learn_rate=learn_rate_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.save_weights('./model/wganpg/generator_167epoch_11rmse.h5')\n",
    "# discriminator.save_weights('./model/wganpg/discriminator_167epoch_11rmse.h5')\n",
    "# combined.save_weights('./model/wganpg/combined_167epoch_11rmse.h5')\n",
    "\n",
    "# generator.load_weights('./model/wganpg/DS_rmse11.85_final/min_generator_wganpg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = generator.predict(X_test[:, :, :, 1:])\n",
    "y_true = X_test[:, :, :, :1]\n",
    "\n",
    "l1(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# every_rmse = np.sqrt(np.mean(np.square(y_true - y_pred), axis=0))\n",
    "# every_mae = np.mean(np.abs(y_true - y_pred), axis=0)\n",
    "# every_mape = np.mean((np.abs(y_true - y_pred)/y_true), axis=0)\n",
    "\n",
    "# import pickle\n",
    "# with open('./data/实验结果数据/wganDs/wganDs_everyObservationr_rmse.data','wb') as f:\n",
    "#     pickle.dump(every_rmse,f)\n",
    "    \n",
    "# with open('./data/实验结果数据/wganDs/wganDs_everyObservationr_mae.data','wb') as f:\n",
    "#     pickle.dump(every_mae,f)\n",
    "    \n",
    "# with open('./data/实验结果数据/wganDs/wganDs_everyObservationr_mape.data','wb') as f:\n",
    "#     pickle.dump(every_mape,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.sqrt(np.sum(np.mean(np.square(y_true - y_pred), axis=0))/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2(y_true, y_pred+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = y_true.reshape(-1,)[1600:1700]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[1600:1700]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi = l2_validation\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "\n",
    "y = [i*10000 for i in lr_step]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "lines = plt.plot(x, y, 'ko-', xi, yi, 'k^--', linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lr_step\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "# fig, ax = plt.subplots(figsize=(6, 6))\n",
    "# lines = plt.plot(x, y, 'ko-', linewidth=1, markersize=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_true.reshape(-1,)[3600:3700]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[3600:3700]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 0\n",
    "y = y_true.reshape(-1,)[start_time: start_time+100]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[start_time: start_time+100]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
