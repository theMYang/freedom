{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, concatenate, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, Add, Subtract\n",
    "from keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D ,AveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, Nadam, RMSprop\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.engine.topology import Layer\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import gc\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import load_model  \n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载  预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交通矩阵为 matrix_length*matrix_length\n",
    "matrix_length = 32\n",
    "\n",
    "matrix_df = pd.read_csv('./data/trafficV_M.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createTrainArray(week_history_num=0, minute_history_num=0):\n",
    "#     week_delta_list = [pd.Timedelta(i+1, unit='W') for i in range(week_history_num)]\n",
    "#     minute_delta_list = [pd.Timedelta((i+1)*15, unit='m') for i in range(minute_history_num)]\n",
    "#     # 参考历史数据时间点list\n",
    "#     delta_list = week_delta_list+minute_delta_list\n",
    "#     print(delta_list)\n",
    "    \n",
    "#     set_up_time = pd.Timedelta(week_history_num, unit='W')\n",
    "#     # 根据历史数据选取多少，重新构建数据集\n",
    "#     # 相当于去除最开始week_history_num个周的数据，因为这些数据无法找到更前的数据\n",
    "#     train_df = matrix_df.truncate(before=matrix_df.index.min() + set_up_time)\n",
    "    \n",
    "#     train_ago_array_tuple = tuple([np.array(matrix_df.loc[train_df.index - i]).reshape(-1, matrix_length, matrix_length, 1) for i in delta_list])\n",
    "#     train_df = np.array(train_df).reshape(-1, matrix_length, matrix_length, 1)\n",
    "#     # concatenate保持 待修复数据在前，参考历史数据在后。与random_mask函数生成mask相一致\n",
    "#     train_array = np.concatenate((train_df,)+train_ago_array_tuple, axis=3)\n",
    "#     print(train_array.shape)\n",
    "#     return train_array\n",
    "\n",
    "\n",
    "\n",
    "def createTrainArray(week_history_num=0, minute_history_num=0):\n",
    "    week_delta_list = [pd.Timedelta(week_history_num-i, unit='W') for i in range(week_history_num)]\n",
    "    minute_delta_list = [pd.Timedelta((minute_history_num-i)*15, unit='m') for i in range(minute_history_num)]\n",
    "    # 参考历史数据时间点list\n",
    "    delta_list = minute_delta_list+week_delta_list\n",
    "    print(delta_list)\n",
    "    \n",
    "    set_up_time = pd.Timedelta(week_history_num, unit='W')\n",
    "    # 根据历史数据选取多少，重新构建数据集\n",
    "    # 相当于去除最开始week_history_num个周的数据，因为这些数据无法找到更前的数据\n",
    "    train_df = matrix_df.truncate(before=matrix_df.index.min() + set_up_time)\n",
    "    \n",
    "    train_ago_array_tuple = tuple([np.array(matrix_df.loc[train_df.index - i]).reshape(-1, matrix_length, matrix_length, 1) for i in delta_list])\n",
    "    train_df = np.array(train_df).reshape(-1, matrix_length, matrix_length, 1)\n",
    "    # concatenate保持 待修复数据在前，参考历史数据在后。与random_mask函数生成mask相一致\n",
    "    train_array = np.concatenate((train_df,)+train_ago_array_tuple, axis=3)\n",
    "    print(train_array.shape)\n",
    "    return train_array\n",
    "\n",
    "\n",
    "def normalization(matrix):\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(matrix.shape[-1]):\n",
    "            cur_time = matrix[i][:, :, j]\n",
    "#             mean_val = cur_time.mean()\n",
    "            mx = cur_time.max()\n",
    "            mn = cur_time.min()\n",
    "            matrix[i][:, :, j] = np.divide((cur_time-mn), (mx-mn))\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timedelta('0 days 00:45:00'), Timedelta('0 days 00:30:00'), Timedelta('0 days 00:15:00'), Timedelta('14 days 00:00:00'), Timedelta('7 days 00:00:00')]\n",
      "(16032, 32, 32, 6)\n"
     ]
    }
   ],
   "source": [
    "week_history_num = 2\n",
    "minute_history_num = 3\n",
    "\n",
    "channel_num = week_history_num +minute_history_num +1\n",
    "smooth_time = channel_num-1\n",
    "\n",
    "# train_array为(16704, 32, 32, 3)，16704个矩阵，32*32采集点，3从上到下为当前时间，上一周，上一15min\n",
    "train_array = createTrainArray(week_history_num, minute_history_num)\n",
    "X_train, X_test = train_test_split(train_array, test_size = 0.1, random_state=42, shuffle=False)\n",
    "# X_train, X_val = train_test_split(train_array, test_size = 0.1, random_state=42, shuffle=False) # 不shuffle可用于查看数据正确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14428, 32, 32, 6), (1604, 32, 32, 6))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_BATCH_SIZE = 64\n",
    "epoch_steps = X_train.shape[0] // MAX_BATCH_SIZE\n",
    "test_steps = X_test.shape[0] // MAX_BATCH_SIZE\n",
    "epoch_steps, test_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "def load_data(volume_matrix, batch_size=MAX_BATCH_SIZE):\n",
    "    n_batches=batch_size\n",
    "    len_of_matrix = len(volume_matrix)\n",
    "\n",
    "    batch_i = 0\n",
    "    while ((batch_i+1)*batch_size < len_of_matrix):\n",
    "        batch_matrix = volume_matrix[batch_i*batch_size: (batch_i+1)*batch_size]\n",
    "        true_volume, history_volume = batch_matrix[:, :, :, :1], batch_matrix[:, :, :, 1:]\n",
    "#         history_volume = normalization(history_volume)\n",
    "        batch_i+=1\n",
    "\n",
    "        yield true_volume, history_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def l2(y_true, y_pred):\n",
    "    return math.sqrt(np.sum(np.mean(np.square(y_true - y_pred), axis=0))/1024)\n",
    "\n",
    "def l1(y_true, y_pred):\n",
    "    return np.sum(np.mean(np.abs(y_true - y_pred), axis=0))/(matrix_length*matrix_length)\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return np.sum(np.mean((np.abs(y_true - y_pred)/y_true)*100, axis=0))/(matrix_length*matrix_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算D输出valid大小（PatchGAN）\n",
    "patch = 4\n",
    "disc_patch = (patch, patch, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (3, 3)\n",
    "g_filters_base = 32\n",
    "DropoutRatio = 0\n",
    "learn_rate_g = 0.0002\n",
    "learn_rate_d = 0.0008\n",
    "learn_rate_c = 0.0002\n",
    "\n",
    "# channels = 3\n",
    "matrix_shape = (matrix_length, matrix_length, channel_num)\n",
    "true_volume_shape = (matrix_length, matrix_length, 1)\n",
    "history_volume_shape = (matrix_length, matrix_length, channel_num-1)\n",
    "\n",
    "kernel_init = 'glorot_uniform'\n",
    "bias_init = 'zeros'\n",
    "kernel_regul = regularizers.l2(1)\n",
    "activity_regul = regularizers.l2(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet block\n",
    "def identity_block(X, filters, f):\n",
    "\n",
    "    F1, F2 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(filters=F1, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(X)\n",
    "\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(X)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "# ENCODER\n",
    "def encoder_layer(img_in, filters, kernel_size, bn=True, resid=True):\n",
    "    # conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding='same')(img_in)\n",
    "    conv = img_in\n",
    "    if bn:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "#             conv = MaxPooling2D((2, 2))(conv)\n",
    "\n",
    "\n",
    "    if resid:\n",
    "        conv = identity_block(conv, (filters, filters), kernel_size)\n",
    "\n",
    "    return conv\n",
    "\n",
    "# DECODER\n",
    "def decoder_layer(img_in, e_conv, filters, kernel_size, bn=True, resid=True):\n",
    "    # up_img = UpSampling2D(size=(2,2))(img_in)\n",
    "    up_img = img_in\n",
    "    concat_img = Concatenate(axis=3)([e_conv,up_img])\n",
    "    conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding='same',\n",
    "                  kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(concat_img)\n",
    "    if bn:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    conv = LeakyReLU(alpha=0)(conv)\n",
    "\n",
    "    if resid:\n",
    "        conv = identity_block(conv, (filters, filters), kernel_size)\n",
    "    return conv\n",
    "\n",
    "\n",
    "\n",
    "def build_generator():      \n",
    "\n",
    "    # INPUTS\n",
    "    history_traffic_volume = Input(shape=history_volume_shape)\n",
    "\n",
    "    # kernel_init = initializers.he_normal()\n",
    "    # bias_init = initializers.he_normal()\n",
    "    kernel_init = 'glorot_uniform'\n",
    "    bias_init = 'zeros'\n",
    "\n",
    "#         kernel_init = initializers.he_uniform()\n",
    "#         bias_init = 'Orthogonal'\n",
    "    kernel_regul = regularizers.l2(1)\n",
    "    activity_regul = regularizers.l2(1)\n",
    "\n",
    "    filters_base = 32\n",
    "    e_conv1_head = Conv2D(filters=filters_base, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(history_traffic_volume)\n",
    "#         e_conv1_head = Conv2D(filters=filters_base*1, kernel_size=3, strides=1, padding='same',\n",
    "#                               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "#                       kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv1_head)\n",
    "    e_conv1_tail = AveragePooling2D((2, 2))(e_conv1_head)\n",
    "#     e_conv1_tail = Dropout(DropoutRatio/2)(e_conv1_tail)\n",
    "    e_conv1 = encoder_layer(e_conv1_tail, filters_base, 3, bn=False)\n",
    "\n",
    "    e_conv2_head = Conv2D(filters=filters_base*2, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv1)\n",
    "    e_conv2_tail = AveragePooling2D((2, 2))(e_conv2_head)\n",
    "#     e_conv2_tail = Dropout(DropoutRatio)(e_conv2_tail)\n",
    "    e_conv2 = encoder_layer(e_conv2_tail, filters_base*2, 3)\n",
    "\n",
    "    e_conv3_head = Conv2D(filters=filters_base*4, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv2)\n",
    "    e_conv3_tail = AveragePooling2D((2, 2))(e_conv3_head)\n",
    "    \n",
    "    # 加drop引入噪声\n",
    "#     e_conv3_tail = Dropout(DropoutRatio)(e_conv3_tail)\n",
    "    \n",
    "    d_conv3_head = encoder_layer(e_conv3_tail, filters_base*4, 3)\n",
    "    resid1 = Subtract()([e_conv3_tail, d_conv3_head])\n",
    "    d_conv3_tail = UpSampling2D(size=(2, 2))(resid1)\n",
    "#     d_conv3_tail = Dropout(DropoutRatio)(d_conv3_tail)\n",
    "\n",
    "\n",
    "    d_conv4_head = decoder_layer(d_conv3_tail, e_conv3_head, filters_base*2, 3)\n",
    "    resid2 = Subtract()([d_conv4_head, e_conv2_tail])\n",
    "    d_conv4_tail = UpSampling2D(size=(2, 2))(resid2)\n",
    "#     d_conv4_tail = Dropout(DropoutRatio)(d_conv4_tail)\n",
    "\n",
    "\n",
    "    d_conv5_head = decoder_layer(d_conv4_tail, e_conv2_head, filters_base*1, 3)\n",
    "    resid3 = Subtract()([d_conv5_head, e_conv1_tail])\n",
    "    d_conv5_tail = UpSampling2D(size=(2, 2))(resid3)\n",
    "#     d_conv5_tail = Dropout(DropoutRatio)(d_conv5_tail)\n",
    "\n",
    "    d_conv6_head = decoder_layer(d_conv5_tail, e_conv1_head, filters_base//2, 3, bn=False)\n",
    "\n",
    "\n",
    "    outputs = Conv2D(1, 1, activation = 'relu', kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(d_conv6_head)\n",
    "\n",
    "    # Setup the model inputs / outputs\n",
    "    model = Model(inputs=history_traffic_volume, outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer = Adam(lr=learn_rate_g),\n",
    "        loss='mse'\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples):\n",
    "    \"\"\"\n",
    "    Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "    \"\"\"\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    # compute the euclidean norm by squaring ...\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    #   ... summing over the rows ...\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    #   ... and sqrt\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "    gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "    # return the mean as loss over all the batch samples\n",
    "    return K.mean(gradient_penalty)\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "def neg_wasserstein_loss(y_true, y_pred):\n",
    "    return -K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_filters_base = 32\n",
    "# Input shape\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator():\n",
    "    def d_layer(layer_input, filters, f_size=3, bn=True, stride=1):\n",
    "        \"\"\"Discriminator layer\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=stride, padding='same', kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(layer_input)\n",
    "        if bn:\n",
    "            d = BatchNormalization()(d)\n",
    "        d = LeakyReLU(alpha=0.1)(d)\n",
    "        return d\n",
    "    \n",
    "    matrix_A = Input(shape=true_volume_shape)\n",
    "    matrix_B = Input(shape=history_volume_shape)\n",
    "\n",
    "    # Concatenate image and conditioning image生成输入对象\n",
    "    combined_matrix = Concatenate(axis=-1)([matrix_A, matrix_B])\n",
    "\n",
    "    d1 = d_layer(combined_matrix, d_filters_base, bn=False)\n",
    "    d2 = d_layer(d1, d_filters_base*2, stride=2)\n",
    "#     d2 = AveragePooling2D((2, 2))(d2)\n",
    "    d3 = d_layer(d2, d_filters_base*4, stride=2)\n",
    "#     d3 = AveragePooling2D((2, 2))(d3)\n",
    "    d4 = d_layer(d3, d_filters_base*8, stride=2)\n",
    "#     d4 = AveragePooling2D((2, 2))(d4)\n",
    "    d4 = d_layer(d4, d_filters_base*4)\n",
    "    d5 = d_layer(d4, d_filters_base*2)\n",
    "    d6 = d_layer(d5, d_filters_base*1)\n",
    "    \n",
    "    validity = Conv2D(1, kernel_size=3, strides=1, padding='same')(d6)\n",
    "    model = Model([matrix_A, matrix_B], validity)\n",
    "    model.compile(optimizer=Adam(lr=learn_rate_d), loss=wasserstein_loss, metrics=['mse'])   #binary_crossentropy\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((64, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "    \n",
    "    \n",
    "class GradNorm(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GradNorm, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(GradNorm, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        vaild_interpolated, interpolation_volume = inputs\n",
    "        grads = K.gradients(vaild_interpolated, interpolation_volume)\n",
    "        assert len(grads) == 1\n",
    "        grad = grads[0]\n",
    "#         a = K.sqrt(K.sum(K.batch_flatten(K.square(grad)), axis=1, keepdims=True))\n",
    "        return grad\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (MAX_BATCH_SIZE,) + true_volume_shape\n",
    "    \n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        return (MAX_BATCH_SIZE,) + true_volume_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "discriminator = build_discriminator()\n",
    "generator = build_generator()\n",
    "\n",
    "\n",
    "true_volume = Input(shape=true_volume_shape)\n",
    "history_volume = Input(shape=history_volume_shape)\n",
    "interpolation_volume = Input(shape=true_volume_shape)\n",
    "\n",
    "forecast_volume = generator(history_volume)\n",
    "\n",
    "discriminator.trainable = False\n",
    "true_vaild = discriminator([true_volume, history_volume])\n",
    "fake_vaild = discriminator([forecast_volume, history_volume])\n",
    "\n",
    "# gp = gradient_penalty_loss(true_volume, forecast_volume, interpolation_volume)\n",
    "norm = GradNorm()([discriminator([interpolation_volume, history_volume]), interpolation_volume])\n",
    "\n",
    "combined = Model(inputs=[true_volume, history_volume, interpolation_volume],\n",
    "                    outputs=[true_vaild, fake_vaild, norm, forecast_volume])\n",
    "combined.compile(loss=[wasserstein_loss,\n",
    "                        neg_wasserstein_loss,\n",
    "                       'mse',\n",
    "                        'mse'],\n",
    "                        optimizer=RMSprop(lr=learn_rate_c),\n",
    "                        loss_weights=[1, 1, 5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_step = []\n",
    "l2_validation = []\n",
    "\n",
    "def train(train_matrix, epochs, batch_size=MAX_BATCH_SIZE, learn_rate=0.01):\n",
    "\n",
    "    min_mse = 999\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(\"train start \"+str(start_time))\n",
    "\n",
    "    # Adversarial loss ground truths\n",
    "#     valid = np.ones((MAX_BATCH_SIZE,) + disc_patch)+np.random.rand(MAX_BATCH_SIZE, patch, patch, 1)/5\n",
    "#     fake = np.zeros((MAX_BATCH_SIZE,) + disc_patch)+np.random.rand(MAX_BATCH_SIZE, patch, patch, 1)/5\n",
    "    valid = np.ones((MAX_BATCH_SIZE,) + disc_patch)\n",
    "    fake = -np.ones((MAX_BATCH_SIZE,) + disc_patch)\n",
    "    dummy = np.zeros((MAX_BATCH_SIZE,) + true_volume_shape)\n",
    "\n",
    "    #　周期修改学习率　https://zhuanlan.zhihu.com/p/52084949\n",
    "    for epoch in range(epochs):\n",
    "#         if epoch>=100 and epoch % 5 == 0 and epoch != 0:\n",
    "#             generator_lr = K.get_value(generator.optimizer.lr)\n",
    "#             discriminator_lr = K.get_value(discriminator.optimizer.lr)\n",
    "#             combined_lr = K.get_value(combined.optimizer.lr)\n",
    "#             if generator_lr>0.0001:\n",
    "#                 K.set_value(generator.optimizer.lr, generator_lr*0.9)\n",
    "#             if discriminator_lr>0.0005:\n",
    "#                 K.set_value(discriminator.optimizer.lr, discriminator_lr*0.9)\n",
    "#             if combined_lr>0.0001:\n",
    "#                 K.set_value(combined.optimizer.lr, combined_lr*0.9)\n",
    "\n",
    "        for batch_i, (true_volume, history_volume) in enumerate(load_data(train_matrix,batch_size)):\n",
    "            # true_volume 真实待预测路网交通量  history_volume 路网交通量历史数据\n",
    "            #  训练 Discriminator\n",
    "\n",
    "            # 根据历史数据生成预测数据\n",
    "            forecast_volume = generator.predict(history_volume)\n",
    "\n",
    "            # 训练 the discriminators (original images = real / generated = Fake)\n",
    "            discriminator.trainable = True\n",
    "            d_loss_real = discriminator.train_on_batch([true_volume, history_volume], valid)\n",
    "            d_loss_fake = discriminator.train_on_batch([forecast_volume, history_volume], fake)\n",
    "            discriminator.trainable = False\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            \n",
    "\n",
    "            epsilon = np.random.uniform(0, 1, size=(MAX_BATCH_SIZE,1,1,1))\n",
    "            interpolation_volume = epsilon*true_volume + (1-epsilon)*forecast_volume\n",
    "            #  训练 Generator\n",
    "            g_loss = combined.train_on_batch([true_volume, history_volume, interpolation_volume], [valid, fake, dummy, true_volume])\n",
    "\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "        # Plot the progress\n",
    "        y_pred = generator.predict(X_test[:, :, :, 1:])\n",
    "        y_true = X_test[:, :, :, :1]\n",
    "\n",
    "        l2_epoch_validation = l2(y_true, y_pred)\n",
    "        l1_epoch_validation = l1(y_true, y_pred)\n",
    "        \n",
    "        y_pred[y_true==0] += 1\n",
    "        y_true[y_true==0] += 1\n",
    "        mape_epoch_validation = mape(y_true, y_pred)\n",
    "        \n",
    "#         lr_step.append(K.get_value(discriminator.optimizer.lr))\n",
    "        if(l2_epoch_validation<12 and l2_epoch_validation>11.8 and l2_epoch_validation < min_mse):\n",
    "            generator.save_weights('./model/wganpg/tmp/min_generator_wganpg.h5')\n",
    "            discriminator.save_weights('./model/wganpg/tmp/min_discriminator_wganpg.h5')\n",
    "            combined.save_weights('./model/wganpg/tmp/min_combined_wganpg.h5')\n",
    "            min_mse = l2_epoch_validation\n",
    "            \n",
    "        l2_validation.append(l2_epoch_validation)\n",
    "        if epoch%1==0:\n",
    "#             print(\"unet lr:\"+ str(K.get_value(unet.optimizer.lr)))\n",
    "            print (\"[Epoch %d/%d]  [D loss: %f, mse: %f] [mae: %f] [mape: %f] [G loss: %f] time: %s\" % (epoch+1, epochs,\n",
    "                                                                    d_loss[0], l2_epoch_validation,\n",
    "                                                                    l1_epoch_validation,\n",
    "                                                                    mape_epoch_validation,\n",
    "                                                                    g_loss[0],\n",
    "                                                                    elapsed_time))\n",
    "        # If at show interval => show generated image samples\n",
    "#             if epoch % show_interval == 0:\n",
    "#                     show_images(dataset_name,epoch, batch_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start 2019-12-19 14:20:01.952898\n",
      "[Epoch 1/200]  [D loss: 0.027971, mse: 98.597841] [mae: 72.378989] [mape: 27.828802] [G loss: 52806.437500] time: 0:01:13.112279\n",
      "[Epoch 2/200]  [D loss: 0.057329, mse: 89.908787] [mae: 68.157712] [mape: 27.658230] [G loss: 37748.867188] time: 0:02:00.438635\n",
      "[Epoch 3/200]  [D loss: 0.082149, mse: 77.530782] [mae: 58.926374] [mape: 24.462034] [G loss: 21782.068359] time: 0:02:46.955080\n",
      "[Epoch 4/200]  [D loss: 0.095666, mse: 59.654702] [mae: 44.662285] [mape: 19.099515] [G loss: 13164.811523] time: 0:03:33.080898\n",
      "[Epoch 5/200]  [D loss: 0.059949, mse: 86.496748] [mae: 66.138495] [mape: 27.768284] [G loss: 41847.503906] time: 0:04:19.319547\n",
      "[Epoch 6/200]  [D loss: 0.052127, mse: 41.231260] [mae: 29.861638] [mape: 13.053326] [G loss: 7584.401367] time: 0:05:05.516249\n",
      "[Epoch 7/200]  [D loss: 0.154125, mse: 27.766565] [mae: 18.859818] [mape: 9.004929] [G loss: 6305.943359] time: 0:05:51.693510\n",
      "[Epoch 8/200]  [D loss: 0.382193, mse: 23.804871] [mae: 15.591168] [mape: 7.667286] [G loss: 5720.176758] time: 0:06:37.754514\n",
      "[Epoch 9/200]  [D loss: 0.174959, mse: 19.301906] [mae: 11.601456] [mape: 6.062756] [G loss: 5161.652832] time: 0:07:23.917653\n",
      "[Epoch 10/200]  [D loss: 0.021394, mse: 18.429441] [mae: 10.906976] [mape: 5.713783] [G loss: 4890.490234] time: 0:08:10.064829\n",
      "[Epoch 11/200]  [D loss: 0.066103, mse: 19.074143] [mae: 11.583031] [mape: 6.004590] [G loss: 4871.687500] time: 0:08:56.211755\n",
      "[Epoch 12/200]  [D loss: 0.259944, mse: 18.285696] [mae: 10.984434] [mape: 5.738780] [G loss: 4684.424316] time: 0:09:42.518083\n",
      "[Epoch 13/200]  [D loss: 0.123370, mse: 18.492050] [mae: 11.282009] [mape: 5.924400] [G loss: 4621.043457] time: 0:10:28.981599\n",
      "[Epoch 14/200]  [D loss: 0.032143, mse: 32.246192] [mae: 22.714193] [mape: 9.405365] [G loss: 5367.505859] time: 0:11:14.914273\n",
      "[Epoch 15/200]  [D loss: 0.110523, mse: 24.229541] [mae: 16.044953] [mape: 6.961192] [G loss: 4439.808594] time: 0:12:00.705819\n",
      "[Epoch 16/200]  [D loss: 0.287824, mse: 17.741152] [mae: 10.507900] [mape: 5.263878] [G loss: 4071.855713] time: 0:12:46.683235\n",
      "[Epoch 17/200]  [D loss: 0.082947, mse: 25.468550] [mae: 17.545324] [mape: 8.809950] [G loss: 5454.115234] time: 0:13:32.904448\n",
      "[Epoch 18/200]  [D loss: 0.055501, mse: 17.989665] [mae: 11.065741] [mape: 5.846819] [G loss: 4184.242676] time: 0:14:19.182464\n",
      "[Epoch 19/200]  [D loss: 0.007736, mse: 21.813319] [mae: 14.519636] [mape: 7.378157] [G loss: 4765.667969] time: 0:15:05.410390\n",
      "[Epoch 20/200]  [D loss: 0.111454, mse: 17.717331] [mae: 10.741279] [mape: 5.134528] [G loss: 3736.855957] time: 0:15:51.620687\n",
      "[Epoch 21/200]  [D loss: 0.482693, mse: 18.795753] [mae: 11.952824] [mape: 6.119939] [G loss: 4413.970703] time: 0:16:37.803449\n",
      "[Epoch 22/200]  [D loss: 0.011006, mse: 16.053571] [mae: 9.469055] [mape: 5.141467] [G loss: 3699.034668] time: 0:17:23.850919\n",
      "[Epoch 23/200]  [D loss: 0.021196, mse: 16.890687] [mae: 10.339020] [mape: 5.362991] [G loss: 3850.342041] time: 0:18:10.081635\n",
      "[Epoch 24/200]  [D loss: 0.000443, mse: 17.570682] [mae: 10.960393] [mape: 5.489133] [G loss: 4140.829102] time: 0:18:55.842031\n",
      "[Epoch 25/200]  [D loss: 0.000231, mse: 16.105159] [mae: 9.746768] [mape: 5.078918] [G loss: 3591.701172] time: 0:19:41.668482\n",
      "[Epoch 26/200]  [D loss: 0.000072, mse: 19.155311] [mae: 12.436012] [mape: 6.051984] [G loss: 4264.184570] time: 0:20:27.437009\n",
      "[Epoch 27/200]  [D loss: 0.000082, mse: 16.591308] [mae: 10.269944] [mape: 5.199659] [G loss: 3750.945312] time: 0:21:13.227781\n",
      "[Epoch 28/200]  [D loss: 0.000095, mse: 17.293808] [mae: 10.895734] [mape: 5.433127] [G loss: 3634.232178] time: 0:22:00.326618\n",
      "[Epoch 29/200]  [D loss: 0.000110, mse: 15.527938] [mae: 9.355554] [mape: 4.842821] [G loss: 3398.146729] time: 0:22:47.309118\n",
      "[Epoch 30/200]  [D loss: 0.000124, mse: 15.844872] [mae: 9.721500] [mape: 5.023991] [G loss: 3398.727295] time: 0:23:33.627021\n",
      "[Epoch 31/200]  [D loss: 0.000147, mse: 17.739941] [mae: 11.399230] [mape: 5.588434] [G loss: 3542.205566] time: 0:24:22.552114\n",
      "[Epoch 32/200]  [D loss: 0.000159, mse: 16.352416] [mae: 10.192477] [mape: 5.212963] [G loss: 3388.361816] time: 0:25:08.669751\n",
      "[Epoch 33/200]  [D loss: 0.000218, mse: 14.082517] [mae: 8.203837] [mape: 4.278998] [G loss: 3015.777100] time: 0:25:54.832696\n",
      "[Epoch 34/200]  [D loss: 0.127155, mse: 14.704605] [mae: 8.905144] [mape: 4.604004] [G loss: 3057.409424] time: 0:26:42.142468\n",
      "[Epoch 35/200]  [D loss: 0.000034, mse: 15.192039] [mae: 9.276647] [mape: 4.706949] [G loss: 3104.148438] time: 0:27:30.085783\n",
      "[Epoch 36/200]  [D loss: 0.000037, mse: 14.912263] [mae: 9.149573] [mape: 4.735851] [G loss: 2987.050293] time: 0:28:16.484506\n",
      "[Epoch 37/200]  [D loss: 0.000040, mse: 15.891241] [mae: 9.936587] [mape: 4.902670] [G loss: 3193.952637] time: 0:29:03.437571\n",
      "[Epoch 38/200]  [D loss: 0.000043, mse: 13.896199] [mae: 8.342784] [mape: 4.464456] [G loss: 2865.318115] time: 0:29:50.236021\n",
      "[Epoch 39/200]  [D loss: 0.000046, mse: 13.621430] [mae: 8.004658] [mape: 4.160730] [G loss: 2778.788086] time: 0:30:36.926214\n",
      "[Epoch 40/200]  [D loss: 0.000050, mse: 14.996909] [mae: 9.199625] [mape: 4.619430] [G loss: 2987.500977] time: 0:31:23.433745\n",
      "[Epoch 41/200]  [D loss: 0.000052, mse: 13.623131] [mae: 8.079638] [mape: 4.279307] [G loss: 2734.018555] time: 0:32:09.702847\n",
      "[Epoch 42/200]  [D loss: 0.000055, mse: 13.425371] [mae: 7.880498] [mape: 4.103303] [G loss: 2671.331055] time: 0:32:56.165597\n",
      "[Epoch 43/200]  [D loss: 0.000064, mse: 14.431489] [mae: 8.749919] [mape: 4.337200] [G loss: 2766.415283] time: 0:33:42.802567\n",
      "[Epoch 44/200]  [D loss: 0.000082, mse: 14.103871] [mae: 8.521472] [mape: 4.387506] [G loss: 2661.566406] time: 0:34:29.319460\n",
      "[Epoch 45/200]  [D loss: 0.000153, mse: 16.642073] [mae: 10.755913] [mape: 5.137651] [G loss: 2898.070312] time: 0:35:15.647832\n",
      "[Epoch 46/200]  [D loss: 0.000156, mse: 19.929601] [mae: 13.513578] [mape: 6.318867] [G loss: 3508.399170] time: 0:36:02.023675\n",
      "[Epoch 47/200]  [D loss: 0.000228, mse: 22.436270] [mae: 15.600259] [mape: 7.080243] [G loss: 3749.477051] time: 0:36:48.960926\n",
      "[Epoch 48/200]  [D loss: 0.000641, mse: 13.564592] [mae: 7.978324] [mape: 4.003112] [G loss: 2560.752441] time: 0:37:37.588512\n",
      "[Epoch 49/200]  [D loss: 0.000350, mse: 13.887683] [mae: 8.400751] [mape: 4.212713] [G loss: 2508.641602] time: 0:38:26.400584\n",
      "[Epoch 50/200]  [D loss: 0.000371, mse: 15.875837] [mae: 10.035273] [mape: 4.630752] [G loss: 2819.621826] time: 0:39:13.776697\n",
      "[Epoch 51/200]  [D loss: 0.000239, mse: 15.061130] [mae: 9.607030] [mape: 4.892155] [G loss: 2592.862061] time: 0:40:03.469015\n",
      "[Epoch 52/200]  [D loss: 0.000424, mse: 18.199059] [mae: 12.017845] [mape: 5.557956] [G loss: 3044.410645] time: 0:40:52.322768\n",
      "[Epoch 53/200]  [D loss: 0.000388, mse: 13.617124] [mae: 8.135405] [mape: 4.177388] [G loss: 2332.907715] time: 0:41:41.314985\n",
      "[Epoch 54/200]  [D loss: 0.000310, mse: 14.305061] [mae: 8.868694] [mape: 4.354524] [G loss: 2521.497070] time: 0:42:28.549462\n",
      "[Epoch 55/200]  [D loss: 0.000308, mse: 14.280158] [mae: 8.800790] [mape: 4.410951] [G loss: 2403.010010] time: 0:43:16.404274\n",
      "[Epoch 56/200]  [D loss: 0.000350, mse: 12.431716] [mae: 7.253082] [mape: 3.870336] [G loss: 2247.410889] time: 0:44:04.090829\n",
      "[Epoch 57/200]  [D loss: 0.000272, mse: 12.616216] [mae: 7.356979] [mape: 3.885016] [G loss: 2186.842529] time: 0:44:51.616726\n",
      "[Epoch 58/200]  [D loss: 0.090221, mse: 17.613333] [mae: 11.791015] [mape: 5.496454] [G loss: 2683.229248] time: 0:45:39.042013\n",
      "[Epoch 59/200]  [D loss: 0.000027, mse: 13.606993] [mae: 8.357245] [mape: 4.249673] [G loss: 2321.389404] time: 0:46:25.879731\n",
      "[Epoch 60/200]  [D loss: 0.000028, mse: 14.277920] [mae: 9.035904] [mape: 4.405848] [G loss: 2479.833008] time: 0:47:12.498723\n",
      "[Epoch 61/200]  [D loss: 0.000030, mse: 12.216433] [mae: 7.192854] [mape: 3.893502] [G loss: 2088.031006] time: 0:47:59.828894\n",
      "[Epoch 62/200]  [D loss: 0.000031, mse: 13.151404] [mae: 8.013341] [mape: 4.150142] [G loss: 2133.359863] time: 0:48:48.237641\n",
      "[Epoch 63/200]  [D loss: 0.000033, mse: 13.734935] [mae: 8.617441] [mape: 4.365315] [G loss: 2149.282715] time: 0:49:35.288993\n",
      "[Epoch 64/200]  [D loss: 0.000035, mse: 12.240459] [mae: 7.282417] [mape: 3.881789] [G loss: 2062.568359] time: 0:50:22.216470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 65/200]  [D loss: 0.000037, mse: 15.288219] [mae: 9.936101] [mape: 4.862856] [G loss: 2183.593506] time: 0:51:09.123601\n",
      "[Epoch 66/200]  [D loss: 0.000040, mse: 16.950028] [mae: 11.315978] [mape: 5.228427] [G loss: 2378.239502] time: 0:51:56.843562\n",
      "[Epoch 67/200]  [D loss: 0.000048, mse: 12.689250] [mae: 7.623331] [mape: 4.070551] [G loss: 2054.030762] time: 0:52:44.261995\n",
      "[Epoch 68/200]  [D loss: 0.000073, mse: 14.164448] [mae: 9.100208] [mape: 4.580002] [G loss: 2295.607910] time: 0:53:33.571348\n",
      "[Epoch 69/200]  [D loss: 0.000137, mse: 12.370127] [mae: 7.334489] [mape: 3.882409] [G loss: 1886.604126] time: 0:54:21.633827\n",
      "[Epoch 70/200]  [D loss: 0.000238, mse: 11.969821] [mae: 7.041535] [mape: 3.779058] [G loss: 1887.425049] time: 0:55:09.718927\n",
      "[Epoch 71/200]  [D loss: 0.000328, mse: 13.121954] [mae: 8.028861] [mape: 4.060763] [G loss: 1937.912476] time: 0:56:08.737839\n",
      "[Epoch 72/200]  [D loss: 0.000349, mse: 16.662087] [mae: 11.168385] [mape: 5.224902] [G loss: 2395.050049] time: 0:56:55.668892\n",
      "[Epoch 73/200]  [D loss: 0.000250, mse: 11.857722] [mae: 6.961082] [mape: 3.712010] [G loss: 1845.133911] time: 0:57:43.448247\n",
      "[Epoch 74/200]  [D loss: 0.000307, mse: 16.244658] [mae: 10.766171] [mape: 4.952679] [G loss: 2174.163086] time: 0:58:31.145482\n",
      "[Epoch 75/200]  [D loss: 0.000394, mse: 11.642699] [mae: 6.837083] [mape: 3.684525] [G loss: 1794.163086] time: 0:59:16.789682\n",
      "[Epoch 76/200]  [D loss: 0.000422, mse: 11.609397] [mae: 6.847371] [mape: 3.673381] [G loss: 1779.276733] time: 1:00:03.703788\n",
      "[Epoch 77/200]  [D loss: 0.000460, mse: 13.423222] [mae: 8.358529] [mape: 4.140661] [G loss: 1910.950562] time: 1:00:50.338763\n",
      "[Epoch 78/200]  [D loss: 0.000392, mse: 16.636515] [mae: 11.048077] [mape: 5.005415] [G loss: 2210.742920] time: 1:01:36.106291\n",
      "[Epoch 79/200]  [D loss: 0.000370, mse: 12.309496] [mae: 7.391591] [mape: 3.756175] [G loss: 1786.599365] time: 1:02:21.766067\n",
      "[Epoch 80/200]  [D loss: 0.000468, mse: 11.787676] [mae: 7.032684] [mape: 3.689944] [G loss: 1723.289062] time: 1:03:07.511058\n",
      "[Epoch 81/200]  [D loss: 0.000405, mse: 19.467753] [mae: 13.529323] [mape: 5.995414] [G loss: 2503.295166] time: 1:03:53.442374\n",
      "[Epoch 82/200]  [D loss: 0.000410, mse: 13.315853] [mae: 8.344831] [mape: 4.065290] [G loss: 1879.066528] time: 1:04:39.048952\n",
      "[Epoch 83/200]  [D loss: 0.000422, mse: 11.871946] [mae: 6.925490] [mape: 3.629876] [G loss: 1676.834595] time: 1:05:24.419703\n",
      "[Epoch 84/200]  [D loss: 0.479551, mse: 13.885502] [mae: 8.734063] [mape: 4.197889] [G loss: 1798.404053] time: 1:06:09.833430\n",
      "[Epoch 85/200]  [D loss: 0.000029, mse: 13.692172] [mae: 8.564270] [mape: 4.071085] [G loss: 1863.094971] time: 1:06:55.478362\n",
      "[Epoch 86/200]  [D loss: 0.000031, mse: 22.413193] [mae: 16.134733] [mape: 6.979299] [G loss: 3095.494873] time: 1:07:41.292802\n",
      "[Epoch 87/200]  [D loss: 0.000032, mse: 15.292913] [mae: 9.946029] [mape: 4.528247] [G loss: 2019.249390] time: 1:08:28.045991\n",
      "[Epoch 88/200]  [D loss: 0.000034, mse: 14.612176] [mae: 9.395966] [mape: 4.296947] [G loss: 1844.835449] time: 1:09:14.089851\n",
      "[Epoch 89/200]  [D loss: 0.000035, mse: 14.728548] [mae: 9.582756] [mape: 4.477325] [G loss: 2043.445801] time: 1:10:00.417992\n",
      "[Epoch 90/200]  [D loss: 0.000038, mse: 15.686214] [mae: 10.437827] [mape: 4.841923] [G loss: 1868.961670] time: 1:10:46.278344\n",
      "[Epoch 91/200]  [D loss: 0.000044, mse: 14.365455] [mae: 9.062385] [mape: 4.210262] [G loss: 1828.857056] time: 1:11:32.361312\n",
      "[Epoch 92/200]  [D loss: 0.000059, mse: 12.664770] [mae: 7.734042] [mape: 3.841383] [G loss: 1702.019653] time: 1:12:18.268292\n",
      "[Epoch 93/200]  [D loss: 0.000101, mse: 12.104753] [mae: 7.109473] [mape: 3.639154] [G loss: 1613.506226] time: 1:13:03.785652\n",
      "[Epoch 94/200]  [D loss: 0.000164, mse: 13.853153] [mae: 8.826443] [mape: 4.109391] [G loss: 1694.018921] time: 1:13:49.792611\n",
      "[Epoch 95/200]  [D loss: 0.000248, mse: 16.165778] [mae: 10.889974] [mape: 5.012886] [G loss: 2049.469971] time: 1:14:36.972694\n",
      "[Epoch 96/200]  [D loss: 0.000283, mse: 15.109716] [mae: 9.964049] [mape: 4.658650] [G loss: 1758.287109] time: 1:15:23.154701\n",
      "[Epoch 97/200]  [D loss: 0.000342, mse: 15.359697] [mae: 10.072289] [mape: 4.599564] [G loss: 1972.074951] time: 1:16:09.424992\n",
      "[Epoch 98/200]  [D loss: 0.000374, mse: 18.370292] [mae: 12.685371] [mape: 5.633977] [G loss: 2622.225098] time: 1:16:55.587397\n",
      "[Epoch 99/200]  [D loss: 0.000400, mse: 16.807781] [mae: 11.362418] [mape: 5.200101] [G loss: 1956.385742] time: 1:17:41.451904\n",
      "[Epoch 100/200]  [D loss: 0.000395, mse: 12.996197] [mae: 8.127006] [mape: 3.964268] [G loss: 1651.858032] time: 1:18:27.636338\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-f2845cdbc646>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_BATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearn_rate_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-29-250a3f5a9dd6>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_matrix, epochs, batch_size, learn_rate)\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;31m# 训练 the discriminators (original images = real / generated = Fake)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0md_loss_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrue_volume\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_volume\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[0md_loss_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mforecast_volume\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_volume\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2617\u001b[0m                 array_vals.append(\n\u001b[0;32m   2618\u001b[0m                     np.asarray(value,\n\u001b[1;32m-> 2619\u001b[1;33m                                dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n\u001b[0m\u001b[0;32m   2620\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2621\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \"\"\"\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(X_train, epochs=200, batch_size=MAX_BATCH_SIZE, learn_rate=learn_rate_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.save_weights('./model/wganpg/generator_167epoch_11rmse.h5')\n",
    "# discriminator.save_weights('./model/wganpg/discriminator_167epoch_11rmse.h5')\n",
    "# combined.save_weights('./model/wganpg/combined_167epoch_11rmse.h5')\n",
    "\n",
    "# generator.load_weights('./model/wganpg/DS_rmse11/generator_167epoch_11rmse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = generator.predict(X_test[:, :, :, 1:])\n",
    "y_true = X_test[:, :, :, :1]\n",
    "\n",
    "l2(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.sqrt(np.sum(np.mean(np.square(y_true - y_pred), axis=0))/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2(y_true, y_pred+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = y_true.reshape(-1,)[1600:1700]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[1600:1700]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi = l2_validation\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "\n",
    "y = [i*10000 for i in lr_step]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "lines = plt.plot(x, y, 'ko-', xi, yi, 'k^--', linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lr_step\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "# fig, ax = plt.subplots(figsize=(6, 6))\n",
    "# lines = plt.plot(x, y, 'ko-', linewidth=1, markersize=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_true.reshape(-1,)[3600:3700]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[3600:3700]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 0\n",
    "y = y_true.reshape(-1,)[start_time: start_time+100]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[start_time: start_time+100]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
