{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, concatenate, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, Add, Subtract\n",
    "from keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D ,AveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, Nadam, RMSprop\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import gc\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import load_model  \n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载  预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交通矩阵为 matrix_length*matrix_length\n",
    "matrix_length = 32\n",
    "\n",
    "matrix_df = pd.read_csv('./data/trafficV_M.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainArray(week_history_num=0, minute_history_num=0):\n",
    "    week_delta_list = [pd.Timedelta(i+1, unit='W') for i in range(week_history_num)]\n",
    "    minute_delta_list = [pd.Timedelta((i+1)*15, unit='m') for i in range(minute_history_num)]\n",
    "    # 参考历史数据时间点list\n",
    "    delta_list = week_delta_list+minute_delta_list\n",
    "    print(delta_list)\n",
    "    \n",
    "    set_up_time = pd.Timedelta(week_history_num, unit='W')\n",
    "    # 根据历史数据选取多少，重新构建数据集\n",
    "    # 相当于去除最开始week_history_num个周的数据，因为这些数据无法找到更前的数据\n",
    "    train_df = matrix_df.truncate(before=matrix_df.index.min() + set_up_time)\n",
    "    \n",
    "    train_ago_array_tuple = tuple([np.array(matrix_df.loc[train_df.index - i]).reshape(-1, matrix_length, matrix_length, 1) for i in delta_list])\n",
    "    train_df = np.array(train_df).reshape(-1, matrix_length, matrix_length, 1)\n",
    "    # concatenate保持 待修复数据在前，参考历史数据在后。与random_mask函数生成mask相一致\n",
    "    train_array = np.concatenate((train_df,)+train_ago_array_tuple, axis=3)\n",
    "    print(train_array.shape)\n",
    "    return train_array\n",
    "\n",
    "\n",
    "def normalization(matrix):\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(matrix.shape[-1]):\n",
    "            cur_time = matrix[i][:, :, j]\n",
    "#             mean_val = cur_time.mean()\n",
    "            mx = cur_time.max()\n",
    "            mn = cur_time.min()\n",
    "            matrix[i][:, :, j] = np.divide((cur_time-mn), (mx-mn))\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timedelta('7 days 00:00:00'), Timedelta('14 days 00:00:00'), Timedelta('0 days 00:15:00'), Timedelta('0 days 00:30:00'), Timedelta('0 days 00:45:00')]\n",
      "(16032, 32, 32, 6)\n"
     ]
    }
   ],
   "source": [
    "week_history_num = 2\n",
    "minute_history_num = 3\n",
    "\n",
    "channel_num = week_history_num +minute_history_num +1\n",
    "smooth_time = channel_num-1\n",
    "\n",
    "# train_array为(16704, 32, 32, 3)，16704个矩阵，32*32采集点，3从上到下为当前时间，上一周，上一15min\n",
    "train_array = createTrainArray(week_history_num, minute_history_num)\n",
    "X_train, X_test = train_test_split(train_array, test_size = 0.2, random_state=42, shuffle=False)\n",
    "# X_train, X_val = train_test_split(train_array, test_size = 0.1, random_state=42, shuffle=False) # 不shuffle可用于查看数据正确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12825, 32, 32, 6), (3207, 32, 32, 6))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_BATCH_SIZE = 32\n",
    "epoch_steps = X_train.shape[0] // MAX_BATCH_SIZE\n",
    "test_steps = X_test.shape[0] // MAX_BATCH_SIZE\n",
    "epoch_steps, test_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "def load_data(volume_matrix, batch_size=MAX_BATCH_SIZE):\n",
    "    n_batches=batch_size\n",
    "    len_of_matrix = len(volume_matrix)\n",
    "\n",
    "    batch_i = 0\n",
    "    while ((batch_i+1)*batch_size < len_of_matrix):\n",
    "        batch_matrix = volume_matrix[batch_i*batch_size: (batch_i+1)*batch_size]\n",
    "        true_volume, history_volume = batch_matrix[:, :, :, :1], batch_matrix[:, :, :, 1:]\n",
    "#         history_volume = normalization(history_volume)\n",
    "        batch_i+=1\n",
    "\n",
    "        yield true_volume, history_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def l2(y_true, y_pred):\n",
    "    return math.sqrt(np.sum(np.mean(np.square(y_true - y_pred), axis=0))/1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BatchActivate(x, bn=True):\n",
    "    if bn:\n",
    "        x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n",
    "    x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n",
    "    if activation == True:\n",
    "        x = BatchActivate(x)\n",
    "    return x\n",
    "\n",
    "def residual_block(blockInput, num_filters=16, batch_activate = False):\n",
    "    x = BatchActivate(blockInput)\n",
    "    x = convolution_block(x, num_filters, (3,3) )\n",
    "    x = convolution_block(x, num_filters, (3,3), activation=False)\n",
    "    x = Add()([x, blockInput])\n",
    "    if batch_activate:\n",
    "        x = BatchActivate(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_generator():      \n",
    "\n",
    "#     # INPUTS\n",
    "#     input_matrix = Input(shape=history_volume_shape)\n",
    "#     # kernel_init = initializers.he_normal()\n",
    "#     # bias_init = initializers.he_normal()\n",
    "#     kernel_init = 'glorot_uniform'\n",
    "#     bias_init = 'zeros'\n",
    "\n",
    "#     # kernel_init = initializers.he_uniform()\n",
    "#     # bias_init = initializers.he_uniform()\n",
    "#     kernel_regul = regularizers.l2(1)\n",
    "#     activity_regul = regularizers.l2(1)\n",
    "\n",
    "#     # ENCODER\n",
    "#     def encoder_layer(img_in, filters, kernel_size, bn=True, resid=True):\n",
    "#         # conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding='same')(img_in)\n",
    "#         conv = Conv2D(filters, (kernel_size, kernel_size), padding=\"same\",\n",
    "#            strides=1,kernel_initializer='glorot_uniform')(img_in)\n",
    "#         if bn:\n",
    "#             conv = BatchNormalization()(conv)\n",
    "# #         conv = Activation('relu')(conv)\n",
    "#         conv = LeakyReLU(alpha=0.1)(conv)\n",
    "\n",
    "#         conv = Conv2D(filters, (kernel_size, kernel_size), padding=\"same\",\n",
    "#            strides=1,kernel_initializer='glorot_uniform')(conv)\n",
    "#         conv = BatchNormalization()(conv)\n",
    "# #         conv = Activation('relu')(conv)\n",
    "#         conv = LeakyReLU(alpha=0.1)(conv)\n",
    "#         return conv\n",
    "\n",
    "#     # DECODER\n",
    "#     def decoder_layer(img_in, filters, kernel_size, bn=True, resid=True):\n",
    "#         conv = Conv2D(filters, (3, 3), padding=\"same\",\n",
    "#            strides=1,kernel_initializer='glorot_uniform')(img_in)\n",
    "#         conv = BatchNormalization()(conv)\n",
    "#         conv = LeakyReLU(alpha=0.1)(conv)\n",
    "# #         conv = Activation('relu')(conv)\n",
    "\n",
    "#         conv = Conv2D(filters//2, (3, 3), padding=\"same\",\n",
    "#            strides=1,kernel_initializer='glorot_uniform')(conv)\n",
    "# #             if bn:\n",
    "#         conv = BatchNormalization()(conv)\n",
    "# #         conv = Activation('relu')(conv)\n",
    "#         conv = LeakyReLU(alpha=0.1)(conv)\n",
    "\n",
    "#         conv = UpSampling2D(size = (2,2))(conv)\n",
    "#         return conv\n",
    "\n",
    "#     encoder_layer.counter = 0\n",
    "#     conv1 = encoder_layer(input_matrix, 32, 3, bn=False)\n",
    "#     pool1 = AveragePooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "#     conv2 = encoder_layer(pool1, 64, 3, bn=True)\n",
    "#     pool2 = AveragePooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "#     conv3 = encoder_layer(pool2, 128, 3, bn=True)\n",
    "#     pool3 = AveragePooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "# #         conv4 = encoder_layer(pool3, 256, 3, bn=True)\n",
    "# #         pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "#     conv5 = decoder_layer(pool3, 256, 3, bn=True)\n",
    "#     merge1 = Concatenate()([conv3,conv5])\n",
    "\n",
    "#     conv6 = decoder_layer(merge1, 128, 3, bn=True)\n",
    "#     merge2 = Concatenate()([conv2,conv6])\n",
    "\n",
    "#     conv7 = decoder_layer(merge2, 64, 3, bn=True)\n",
    "#     merge3 = Concatenate()([conv1,conv7])\n",
    "\n",
    "# #         conv8 = decoder_layer(merge3, 32, 3, bn=True)\n",
    "# #         merge4 = Concatenate()([conv1,conv8])\n",
    "\n",
    "#     conv9 = encoder_layer(merge3, 32, 3, bn=False)\n",
    "    \n",
    "#     model_output = Conv2D(1, (1, 1), \n",
    "#            use_bias=False, padding=\"same\",activation=\"linear\",\n",
    "#            strides=1,kernel_initializer='glorot_uniform',\n",
    "#            name='block9_conv3')(conv9)\n",
    "\n",
    "#     # Setup the model inputs / outputs\n",
    "#     model = Model(inputs=input_matrix, outputs=model_output)\n",
    "\n",
    "#     # Compile the model\n",
    "#     model.compile(optimizer = Adam(lr=learn_rate), loss='mse')\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (3, 3)\n",
    "g_filters_base = 32\n",
    "DropoutRatio = 0.5\n",
    "learn_rate_g = 0.001\n",
    "learn_rate_d = 0.001\n",
    "learn_rate_c = 0.001\n",
    "\n",
    "# channels = 3\n",
    "matrix_shape = (matrix_length, matrix_length, channel_num)\n",
    "true_volume_shape = (matrix_length, matrix_length, 1)\n",
    "history_volume_shape = (matrix_length, matrix_length, channel_num-1)\n",
    "\n",
    "kernel_init = 'glorot_uniform'\n",
    "bias_init = 'zeros'\n",
    "kernel_regul = regularizers.l2(1)\n",
    "activity_regul = regularizers.l2(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet block\n",
    "def identity_block(X, filters, f):\n",
    "\n",
    "    F1, F2 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(filters=F1, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(X)\n",
    "\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(X)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "# ENCODER\n",
    "def encoder_layer(img_in, filters, kernel_size, bn=True, resid=True):\n",
    "    # conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding='same')(img_in)\n",
    "    conv = img_in\n",
    "    if bn:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "#             conv = MaxPooling2D((2, 2))(conv)\n",
    "\n",
    "\n",
    "    if resid:\n",
    "        conv = identity_block(conv, (filters, filters), kernel_size)\n",
    "\n",
    "    return conv\n",
    "\n",
    "# DECODER\n",
    "def decoder_layer(img_in, e_conv, filters, kernel_size, bn=True, resid=True):\n",
    "    # up_img = UpSampling2D(size=(2,2))(img_in)\n",
    "    up_img = img_in\n",
    "    concat_img = Concatenate(axis=3)([e_conv,up_img])\n",
    "    conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding='same',\n",
    "                  kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(concat_img)\n",
    "    if bn:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    conv = LeakyReLU(alpha=0)(conv)\n",
    "\n",
    "    if resid:\n",
    "        conv = identity_block(conv, (filters, filters), kernel_size)\n",
    "    return conv\n",
    "\n",
    "\n",
    "\n",
    "def build_generator():      \n",
    "\n",
    "    # INPUTS\n",
    "    history_traffic_volume = Input(shape=history_volume_shape)\n",
    "\n",
    "    # kernel_init = initializers.he_normal()\n",
    "    # bias_init = initializers.he_normal()\n",
    "    kernel_init = 'glorot_uniform'\n",
    "    bias_init = 'zeros'\n",
    "\n",
    "#         kernel_init = initializers.he_uniform()\n",
    "#         bias_init = 'Orthogonal'\n",
    "    kernel_regul = regularizers.l2(1)\n",
    "    activity_regul = regularizers.l2(1)\n",
    "\n",
    "    filters_base = 32\n",
    "    e_conv1_head = Conv2D(filters=filters_base, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(history_traffic_volume)\n",
    "#         e_conv1_head = Conv2D(filters=filters_base*1, kernel_size=3, strides=1, padding='same',\n",
    "#                               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "#                       kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv1_head)\n",
    "    e_conv1_tail = AveragePooling2D((2, 2))(e_conv1_head)\n",
    "    e_conv1 = encoder_layer(e_conv1_tail, filters_base, 3, bn=False)\n",
    "\n",
    "    e_conv2_head = Conv2D(filters=filters_base*2, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv1)\n",
    "    e_conv2_tail = AveragePooling2D((2, 2))(e_conv2_head)\n",
    "    e_conv2 = encoder_layer(e_conv2_tail, filters_base*2, 3)\n",
    "\n",
    "    e_conv3_head = Conv2D(filters=filters_base*4, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv2)\n",
    "    e_conv3_tail = AveragePooling2D((2, 2))(e_conv3_head)\n",
    "    d_conv3_head = encoder_layer(e_conv3_tail, filters_base*4, 3)\n",
    "    resid1 = Subtract()([e_conv3_tail, d_conv3_head])\n",
    "    d_conv3_tail = UpSampling2D(size=(2, 2))(resid1)\n",
    "\n",
    "\n",
    "    d_conv4_head = decoder_layer(d_conv3_tail, e_conv3_head, filters_base*2, 3)\n",
    "    resid2 = Subtract()([d_conv4_head, e_conv2_tail])\n",
    "    d_conv4_tail = UpSampling2D(size=(2, 2))(resid2)\n",
    "\n",
    "\n",
    "    d_conv5_head = decoder_layer(d_conv4_tail, e_conv2_head, filters_base*1, 3)\n",
    "    resid3 = Subtract()([d_conv5_head, e_conv1_tail])\n",
    "    d_conv5_tail = UpSampling2D(size=(2, 2))(resid3)\n",
    "\n",
    "    d_conv6_head = decoder_layer(d_conv5_tail, e_conv1_head, filters_base//2, 3, bn=False)\n",
    "\n",
    "\n",
    "    outputs = Conv2D(1, 1, activation = 'relu', kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(d_conv6_head)\n",
    "\n",
    "    # Setup the model inputs / outputs\n",
    "    model = Model(inputs=history_traffic_volume, outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer = Adam(lr=learn_rate_g),\n",
    "        loss='mse'\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_filters_base = 32\n",
    "# Input shape\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator():\n",
    "    def d_layer(layer_input, filters, f_size=3, bn=True, stride=1):\n",
    "        \"\"\"Discriminator layer\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=stride, padding='same', kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(layer_input)\n",
    "        if bn:\n",
    "            d = BatchNormalization()(d)\n",
    "        d = LeakyReLU(alpha=0.1)(d)\n",
    "        return d\n",
    "    \n",
    "    matrix_A = Input(shape=true_volume_shape)\n",
    "    matrix_B = Input(shape=history_volume_shape)\n",
    "\n",
    "    # Concatenate image and conditioning image生成输入对象\n",
    "    combined_matrix = Concatenate(axis=-1)([matrix_A, matrix_B])\n",
    "\n",
    "    d1 = d_layer(combined_matrix, d_filters_base, bn=False)\n",
    "    d2 = d_layer(d1, d_filters_base*2, stride=2)\n",
    "#     d2 = AveragePooling2D((2, 2))(d2)\n",
    "    d3 = d_layer(d2, d_filters_base*4, stride=2)\n",
    "#     d3 = AveragePooling2D((2, 2))(d3)\n",
    "    d4 = d_layer(d3, d_filters_base*8, stride=2)\n",
    "#     d4 = AveragePooling2D((2, 2))(d4)\n",
    "    d4 = d_layer(d4, d_filters_base*4)\n",
    "    d5 = d_layer(d4, d_filters_base*2)\n",
    "    d6 = d_layer(d5, d_filters_base*1)\n",
    "    \n",
    "    validity = Conv2D(1, kernel_size=3, strides=1, padding='same')(d6)\n",
    "\n",
    "    model = Model([matrix_A, matrix_B], validity)\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=learn_rate_d), metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算D输出valid大小（PatchGAN）\n",
    "patch = 4\n",
    "disc_patch = (patch, patch, 1)\n",
    "\n",
    "# Number of filters in the first layer of G and D\n",
    "gf = 64\n",
    "df = 64\n",
    "\n",
    "optimizer = Adam(lr=learn_rate_c)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# Input images and their conditioning images\n",
    "true_volume = Input(shape=true_volume_shape)\n",
    "history_volume = Input(shape=history_volume_shape)\n",
    "\n",
    "# By conditioning on B generate a fake version of A\n",
    "forecast_volume = generator(history_volume)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# Discriminators determines validity of translated images / condition pairs\n",
    "valid_gan = discriminator([forecast_volume, history_volume])\n",
    "\n",
    "combined = Model(inputs=[true_volume, history_volume], outputs=[valid_gan, forecast_volume])\n",
    "combined.compile(loss=['mse', 'mse'], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_step = []\n",
    "l2_validation = []\n",
    "\n",
    "def train(train_matrix, epochs, batch_size=MAX_BATCH_SIZE, learn_rate=0.01):\n",
    "\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(\"train start \"+str(start_time))\n",
    "\n",
    "    # Adversarial loss ground truths\n",
    "#     valid = np.ones((MAX_BATCH_SIZE,) + disc_patch)+np.random.rand(MAX_BATCH_SIZE, patch, patch, 1)/5\n",
    "#     fake = np.zeros((MAX_BATCH_SIZE,) + disc_patch)+np.random.rand(MAX_BATCH_SIZE, patch, patch, 1)/5\n",
    "    valid = np.ones((MAX_BATCH_SIZE,) + disc_patch)\n",
    "    fake = np.zeros((MAX_BATCH_SIZE,) + disc_patch)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch>9 and epoch % 2 == 0 and epoch != 0:\n",
    "            if learn_rate>0.001:\n",
    "                if epoch%3==0:\n",
    "                    learn_rate = 0.9*learn_rate\n",
    "                else:\n",
    "                    learn_rate = 0.9*learn_rate\n",
    "                K.set_value(generator.optimizer.lr, learn_rate)\n",
    "                K.set_value(discriminator.optimizer.lr, learn_rate)\n",
    "\n",
    "        for batch_i, (true_volume, history_volume) in enumerate(load_data(train_matrix,batch_size)):\n",
    "            # true_volume 真实待预测路网交通量  history_volume 路网交通量历史数据\n",
    "            #  训练 Discriminator\n",
    "\n",
    "            # 根据历史数据生成预测数据\n",
    "            forecast_volume = generator.predict(history_volume)\n",
    "\n",
    "            # 训练 the discriminators (original images = real / generated = Fake)\n",
    "            discriminator.trainable = True\n",
    "            d_loss_real = discriminator.train_on_batch([true_volume, history_volume], valid)\n",
    "            d_loss_fake = discriminator.train_on_batch([forecast_volume, history_volume], fake)\n",
    "            discriminator.trainable = False\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "            #  训练 Generator\n",
    "            g_loss = combined.train_on_batch([true_volume, history_volume], [valid, true_volume])\n",
    "\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "        # Plot the progress\n",
    "        y_pred = generator.predict(X_test[:, :, :, 1:])\n",
    "        y_true = X_test[:, :, :, :1]\n",
    "\n",
    "        l2_epoch_validation = l2(y_true, y_pred)\n",
    "        lr_step.append(K.get_value(discriminator.optimizer.lr))\n",
    "        l2_validation.append(l2_epoch_validation)\n",
    "        if epoch%1==0:\n",
    "#             print(\"discriminator lr:\"+ str(K.get_value(discriminator.optimizer.lr)))\n",
    "            print (\"[Epoch %d/%d]  [D loss: %f, mse: %f] [G loss: %f] time: %s\" % (epoch+1, epochs,\n",
    "                                                                    d_loss[0], l2_epoch_validation,\n",
    "                                                                    g_loss[0],\n",
    "                                                                    elapsed_time))\n",
    "        # If at show interval => show generated image samples\n",
    "#             if epoch % show_interval == 0:\n",
    "#                     show_images(dataset_name,epoch, batch_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start 2019-10-18 19:57:42.946203\n",
      "[Epoch 1/50]  [D loss: 0.376286, mse: 40.684993] [G loss: 1888.836426] time: 0:01:08.368259\n",
      "[Epoch 2/50]  [D loss: 0.407982, mse: 32.136564] [G loss: 1101.591064] time: 0:01:59.266868\n",
      "[Epoch 3/50]  [D loss: 0.388119, mse: 27.097196] [G loss: 1010.129395] time: 0:02:50.095284\n",
      "[Epoch 4/50]  [D loss: 0.683596, mse: 26.986766] [G loss: 1011.568420] time: 0:03:37.665057\n",
      "[Epoch 5/50]  [D loss: 0.423903, mse: 25.654784] [G loss: 934.541931] time: 0:04:28.634224\n",
      "[Epoch 6/50]  [D loss: 0.321683, mse: 25.226533] [G loss: 891.392395] time: 0:05:19.253270\n",
      "[Epoch 7/50]  [D loss: 0.326933, mse: 25.005491] [G loss: 841.021301] time: 0:06:08.790092\n",
      "[Epoch 8/50]  [D loss: 0.502368, mse: 24.473125] [G loss: 907.756897] time: 0:06:57.826627\n",
      "[Epoch 9/50]  [D loss: 0.285719, mse: 26.371835] [G loss: 829.819275] time: 0:07:46.304919\n",
      "[Epoch 10/50]  [D loss: 0.278647, mse: 30.447972] [G loss: 696.636963] time: 0:08:37.839304\n",
      "[Epoch 11/50]  [D loss: 0.367626, mse: 28.273968] [G loss: 814.129517] time: 0:09:28.585898\n",
      "[Epoch 12/50]  [D loss: 0.315994, mse: 24.066962] [G loss: 822.632690] time: 0:10:19.233259\n",
      "[Epoch 13/50]  [D loss: 0.256683, mse: 26.330164] [G loss: 749.670410] time: 0:11:10.099164\n",
      "[Epoch 14/50]  [D loss: 0.284000, mse: 23.007020] [G loss: 754.683228] time: 0:12:00.874434\n",
      "[Epoch 15/50]  [D loss: 0.288588, mse: 22.566072] [G loss: 717.555176] time: 0:12:50.702914\n",
      "[Epoch 16/50]  [D loss: 0.290887, mse: 23.944369] [G loss: 703.240601] time: 0:13:39.014694\n",
      "[Epoch 17/50]  [D loss: 0.324371, mse: 22.296268] [G loss: 821.351624] time: 0:14:30.894554\n",
      "[Epoch 18/50]  [D loss: 0.339400, mse: 24.646348] [G loss: 903.675659] time: 0:15:21.616131\n",
      "[Epoch 19/50]  [D loss: 0.256134, mse: 26.136699] [G loss: 853.521423] time: 0:16:11.929423\n",
      "[Epoch 20/50]  [D loss: 0.315079, mse: 23.699027] [G loss: 795.096375] time: 0:17:00.530157\n",
      "[Epoch 21/50]  [D loss: 0.257898, mse: 21.352725] [G loss: 737.125305] time: 0:17:48.374125\n",
      "[Epoch 22/50]  [D loss: 0.265168, mse: 23.320228] [G loss: 669.888794] time: 0:18:41.370451\n",
      "[Epoch 23/50]  [D loss: 0.290782, mse: 22.133549] [G loss: 700.729004] time: 0:19:32.691072\n",
      "[Epoch 24/50]  [D loss: 0.250562, mse: 42.698619] [G loss: 647.751526] time: 0:20:23.905988\n",
      "[Epoch 25/50]  [D loss: 0.250678, mse: 77.870612] [G loss: 2774.552002] time: 0:21:15.955393\n",
      "[Epoch 26/50]  [D loss: 0.250785, mse: 21.962715] [G loss: 767.843933] time: 0:22:06.871009\n",
      "[Epoch 27/50]  [D loss: 0.250868, mse: 20.679489] [G loss: 710.576660] time: 0:22:56.939452\n",
      "[Epoch 28/50]  [D loss: 0.250936, mse: 20.449518] [G loss: 711.452515] time: 0:23:47.217390\n",
      "[Epoch 29/50]  [D loss: 0.251027, mse: 21.832032] [G loss: 754.678223] time: 0:24:37.381661\n",
      "[Epoch 30/50]  [D loss: 0.251049, mse: 27.168522] [G loss: 762.906433] time: 0:25:28.312665\n",
      "[Epoch 31/50]  [D loss: 0.573506, mse: 21.748004] [G loss: 804.113892] time: 0:26:17.952592\n",
      "[Epoch 32/50]  [D loss: 0.250448, mse: 24.676202] [G loss: 727.229004] time: 0:27:07.186567\n",
      "[Epoch 33/50]  [D loss: 0.250448, mse: 23.876357] [G loss: 811.751099] time: 0:27:56.974375\n"
     ]
    }
   ],
   "source": [
    "train(X_train, epochs=50, batch_size=MAX_BATCH_SIZE, learn_rate=learn_rate_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.save_weights('/kaggle/working/generatorModel.h5')\n",
    "# generator.load_weights('/kaggle/working/generatorModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = generator.predict(X_test[:, :, :, 1:])\n",
    "y_true = X_test[:, :, :, :1]\n",
    "\n",
    "l2(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = y_true.reshape(-1,)[1600:1700]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[1600:1700]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi = l2_validation\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "\n",
    "y = [i*10000 for i in lr_step]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "lines = plt.plot(x, y, 'ko-', xi, yi, 'k^--', linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lr_step\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "# fig, ax = plt.subplots(figsize=(6, 6))\n",
    "# lines = plt.plot(x, y, 'ko-', linewidth=1, markersize=6)\n",
    "lr_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_true.reshape(-1,)[3600:3700]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[3600:3700]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 0\n",
    "y = y_true.reshape(-1,)[start_time: start_time+100]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[start_time: start_time+100]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
