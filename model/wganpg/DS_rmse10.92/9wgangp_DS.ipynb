{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import scipy\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, concatenate, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, Add, Subtract\n",
    "from keras.layers import Conv2D, Conv2DTranspose, MaxPooling2D ,AveragePooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, Nadam, RMSprop\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.engine.topology import Layer\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import gc\n",
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, TensorBoard\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import load_model  \n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载  预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交通矩阵为 matrix_length*matrix_length\n",
    "matrix_length = 32\n",
    "\n",
    "matrix_df = pd.read_csv('./data/trafficV_M.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createTrainArray(week_history_num=0, minute_history_num=0):\n",
    "#     week_delta_list = [pd.Timedelta(i+1, unit='W') for i in range(week_history_num)]\n",
    "#     minute_delta_list = [pd.Timedelta((i+1)*15, unit='m') for i in range(minute_history_num)]\n",
    "#     # 参考历史数据时间点list\n",
    "#     delta_list = week_delta_list+minute_delta_list\n",
    "#     print(delta_list)\n",
    "    \n",
    "#     set_up_time = pd.Timedelta(week_history_num, unit='W')\n",
    "#     # 根据历史数据选取多少，重新构建数据集\n",
    "#     # 相当于去除最开始week_history_num个周的数据，因为这些数据无法找到更前的数据\n",
    "#     train_df = matrix_df.truncate(before=matrix_df.index.min() + set_up_time)\n",
    "    \n",
    "#     train_ago_array_tuple = tuple([np.array(matrix_df.loc[train_df.index - i]).reshape(-1, matrix_length, matrix_length, 1) for i in delta_list])\n",
    "#     train_df = np.array(train_df).reshape(-1, matrix_length, matrix_length, 1)\n",
    "#     # concatenate保持 待修复数据在前，参考历史数据在后。与random_mask函数生成mask相一致\n",
    "#     train_array = np.concatenate((train_df,)+train_ago_array_tuple, axis=3)\n",
    "#     print(train_array.shape)\n",
    "#     return train_array\n",
    "\n",
    "\n",
    "\n",
    "def createTrainArray(week_history_num=0, minute_history_num=0):\n",
    "    week_delta_list = [pd.Timedelta(week_history_num-i, unit='W') for i in range(week_history_num)]\n",
    "    minute_delta_list = [pd.Timedelta((minute_history_num-i)*15, unit='m') for i in range(minute_history_num)]\n",
    "    # 参考历史数据时间点list\n",
    "    delta_list = minute_delta_list+week_delta_list\n",
    "    print(delta_list)\n",
    "    \n",
    "    set_up_time = pd.Timedelta(week_history_num, unit='W')\n",
    "    # 根据历史数据选取多少，重新构建数据集\n",
    "    # 相当于去除最开始week_history_num个周的数据，因为这些数据无法找到更前的数据\n",
    "    train_df = matrix_df.truncate(before=matrix_df.index.min() + set_up_time)\n",
    "    \n",
    "    train_ago_array_tuple = tuple([np.array(matrix_df.loc[train_df.index - i]).reshape(-1, matrix_length, matrix_length, 1) for i in delta_list])\n",
    "    train_df = np.array(train_df).reshape(-1, matrix_length, matrix_length, 1)\n",
    "    # concatenate保持 待修复数据在前，参考历史数据在后。与random_mask函数生成mask相一致\n",
    "    train_array = np.concatenate((train_df,)+train_ago_array_tuple, axis=3)\n",
    "    print(train_array.shape)\n",
    "    return train_array\n",
    "\n",
    "\n",
    "def normalization(matrix):\n",
    "    for i in range(len(matrix)):\n",
    "        for j in range(matrix.shape[-1]):\n",
    "            cur_time = matrix[i][:, :, j]\n",
    "#             mean_val = cur_time.mean()\n",
    "            mx = cur_time.max()\n",
    "            mn = cur_time.min()\n",
    "            matrix[i][:, :, j] = np.divide((cur_time-mn), (mx-mn))\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timedelta('0 days 00:45:00'), Timedelta('0 days 00:30:00'), Timedelta('0 days 00:15:00'), Timedelta('14 days 00:00:00'), Timedelta('7 days 00:00:00')]\n",
      "(16032, 32, 32, 6)\n"
     ]
    }
   ],
   "source": [
    "week_history_num = 2\n",
    "minute_history_num = 3\n",
    "\n",
    "channel_num = week_history_num +minute_history_num +1\n",
    "smooth_time = channel_num-1\n",
    "\n",
    "# train_array为(16704, 32, 32, 3)，16704个矩阵，32*32采集点，3从上到下为当前时间，上一周，上一15min\n",
    "train_array = createTrainArray(week_history_num, minute_history_num)\n",
    "X_train, X_test = train_test_split(train_array, test_size = 0.1, random_state=42, shuffle=False)\n",
    "# X_train, X_val = train_test_split(train_array, test_size = 0.1, random_state=42, shuffle=False) # 不shuffle可用于查看数据正确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14428, 32, 32, 6), (1604, 32, 32, 6))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_BATCH_SIZE = 64\n",
    "epoch_steps = X_train.shape[0] // MAX_BATCH_SIZE\n",
    "test_steps = X_test.shape[0] // MAX_BATCH_SIZE\n",
    "epoch_steps, test_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "def load_data(volume_matrix, batch_size=MAX_BATCH_SIZE):\n",
    "    n_batches=batch_size\n",
    "    len_of_matrix = len(volume_matrix)\n",
    "\n",
    "    batch_i = 0\n",
    "    while ((batch_i+1)*batch_size < len_of_matrix):\n",
    "        batch_matrix = volume_matrix[batch_i*batch_size: (batch_i+1)*batch_size]\n",
    "        true_volume, history_volume = batch_matrix[:, :, :, :1], batch_matrix[:, :, :, 1:]\n",
    "#         history_volume = normalization(history_volume)\n",
    "        batch_i+=1\n",
    "\n",
    "        yield true_volume, history_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def l2(y_true, y_pred):\n",
    "    return math.sqrt(np.sum(np.mean(np.square(y_true - y_pred), axis=0))/1024)\n",
    "\n",
    "def l1(y_true, y_pred):\n",
    "    return np.sum(np.mean(np.abs(y_true - y_pred), axis=0))/(matrix_length*matrix_length)\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    return np.sum(np.mean((np.abs(y_true - y_pred)/y_true)*100, axis=0))/(matrix_length*matrix_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算D输出valid大小（PatchGAN）\n",
    "patch = 4\n",
    "disc_patch = (patch, patch, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (3, 3)\n",
    "g_filters_base = 32\n",
    "DropoutRatio = 0\n",
    "learn_rate_g = 0.0002\n",
    "learn_rate_d = 0.0008\n",
    "learn_rate_c = 0.0002\n",
    "\n",
    "# channels = 3\n",
    "matrix_shape = (matrix_length, matrix_length, channel_num)\n",
    "true_volume_shape = (matrix_length, matrix_length, 1)\n",
    "history_volume_shape = (matrix_length, matrix_length, channel_num-1)\n",
    "\n",
    "kernel_init = 'glorot_uniform'\n",
    "bias_init = 'zeros'\n",
    "kernel_regul = regularizers.l2(1)\n",
    "activity_regul = regularizers.l2(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet block\n",
    "def identity_block(X, filters, f):\n",
    "\n",
    "    F1, F2 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(filters=F1, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(X)\n",
    "\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same',\n",
    "               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(X)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "# ENCODER\n",
    "def encoder_layer(img_in, filters, kernel_size, bn=True, resid=True):\n",
    "    # conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding='same')(img_in)\n",
    "    conv = img_in\n",
    "    if bn:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "#             conv = MaxPooling2D((2, 2))(conv)\n",
    "\n",
    "\n",
    "    if resid:\n",
    "        conv = identity_block(conv, (filters, filters), kernel_size)\n",
    "\n",
    "    return conv\n",
    "\n",
    "# DECODER\n",
    "def decoder_layer(img_in, e_conv, filters, kernel_size, bn=True, resid=True):\n",
    "    # up_img = UpSampling2D(size=(2,2))(img_in)\n",
    "    up_img = img_in\n",
    "    concat_img = Concatenate(axis=3)([e_conv,up_img])\n",
    "    conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=(1, 1), padding='same',\n",
    "                  kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "              kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(concat_img)\n",
    "    if bn:\n",
    "        conv = BatchNormalization()(conv)\n",
    "    conv = LeakyReLU(alpha=0)(conv)\n",
    "\n",
    "    if resid:\n",
    "        conv = identity_block(conv, (filters, filters), kernel_size)\n",
    "    return conv\n",
    "\n",
    "\n",
    "\n",
    "def build_generator():      \n",
    "\n",
    "    # INPUTS\n",
    "    history_traffic_volume = Input(shape=history_volume_shape)\n",
    "\n",
    "    # kernel_init = initializers.he_normal()\n",
    "    # bias_init = initializers.he_normal()\n",
    "    kernel_init = 'glorot_uniform'\n",
    "    bias_init = 'zeros'\n",
    "\n",
    "#         kernel_init = initializers.he_uniform()\n",
    "#         bias_init = 'Orthogonal'\n",
    "    kernel_regul = regularizers.l2(1)\n",
    "    activity_regul = regularizers.l2(1)\n",
    "\n",
    "    filters_base = 32\n",
    "    e_conv1_head = Conv2D(filters=filters_base, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(history_traffic_volume)\n",
    "#         e_conv1_head = Conv2D(filters=filters_base*1, kernel_size=3, strides=1, padding='same',\n",
    "#                               kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "#                       kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv1_head)\n",
    "    e_conv1_tail = AveragePooling2D((2, 2))(e_conv1_head)\n",
    "#     e_conv1_tail = Dropout(DropoutRatio/2)(e_conv1_tail)\n",
    "    e_conv1 = encoder_layer(e_conv1_tail, filters_base, 3, bn=False)\n",
    "\n",
    "    e_conv2_head = Conv2D(filters=filters_base*2, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv1)\n",
    "    e_conv2_tail = AveragePooling2D((2, 2))(e_conv2_head)\n",
    "#     e_conv2_tail = Dropout(DropoutRatio)(e_conv2_tail)\n",
    "    e_conv2 = encoder_layer(e_conv2_tail, filters_base*2, 3)\n",
    "\n",
    "    e_conv3_head = Conv2D(filters=filters_base*4, kernel_size=3, strides=1, padding='same',\n",
    "                          kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(e_conv2)\n",
    "    e_conv3_tail = AveragePooling2D((2, 2))(e_conv3_head)\n",
    "    \n",
    "    # 加drop引入噪声\n",
    "#     e_conv3_tail = Dropout(DropoutRatio)(e_conv3_tail)\n",
    "    \n",
    "    d_conv3_head = encoder_layer(e_conv3_tail, filters_base*4, 3)\n",
    "    resid1 = Subtract()([e_conv3_tail, d_conv3_head])\n",
    "    d_conv3_tail = UpSampling2D(size=(2, 2))(resid1)\n",
    "#     d_conv3_tail = Dropout(DropoutRatio)(d_conv3_tail)\n",
    "\n",
    "\n",
    "    d_conv4_head = decoder_layer(d_conv3_tail, e_conv3_head, filters_base*2, 3)\n",
    "    resid2 = Subtract()([d_conv4_head, e_conv2_tail])\n",
    "    d_conv4_tail = UpSampling2D(size=(2, 2))(resid2)\n",
    "#     d_conv4_tail = Dropout(DropoutRatio)(d_conv4_tail)\n",
    "\n",
    "\n",
    "    d_conv5_head = decoder_layer(d_conv4_tail, e_conv2_head, filters_base*1, 3)\n",
    "    resid3 = Subtract()([d_conv5_head, e_conv1_tail])\n",
    "    d_conv5_tail = UpSampling2D(size=(2, 2))(resid3)\n",
    "#     d_conv5_tail = Dropout(DropoutRatio)(d_conv5_tail)\n",
    "\n",
    "    d_conv6_head = decoder_layer(d_conv5_tail, e_conv1_head, filters_base//2, 3, bn=False)\n",
    "\n",
    "\n",
    "    outputs = Conv2D(1, 1, activation = 'relu', kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(d_conv6_head)\n",
    "\n",
    "    # Setup the model inputs / outputs\n",
    "    model = Model(inputs=history_traffic_volume, outputs=outputs)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer = Adam(lr=learn_rate_g),\n",
    "        loss='mse'\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples):\n",
    "    \"\"\"\n",
    "    Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "    \"\"\"\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    # compute the euclidean norm by squaring ...\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    #   ... summing over the rows ...\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    #   ... and sqrt\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "    gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "    # return the mean as loss over all the batch samples\n",
    "    return K.mean(gradient_penalty)\n",
    "\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "def neg_wasserstein_loss(y_true, y_pred):\n",
    "    return -K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_filters_base = 32\n",
    "# Input shape\n",
    "\n",
    "# Discriminator\n",
    "def build_discriminator():\n",
    "    def d_layer(layer_input, filters, f_size=3, bn=True, stride=1):\n",
    "        \"\"\"Discriminator layer\"\"\"\n",
    "        d = Conv2D(filters, kernel_size=f_size, strides=stride, padding='same', kernel_initializer=kernel_init, bias_initializer=bias_init,\n",
    "                  kernel_regularizer=kernel_regul, bias_regularizer=activity_regul)(layer_input)\n",
    "        if bn:\n",
    "            d = BatchNormalization()(d)\n",
    "        d = LeakyReLU(alpha=0.1)(d)\n",
    "        return d\n",
    "    \n",
    "    matrix_A = Input(shape=true_volume_shape)\n",
    "    matrix_B = Input(shape=history_volume_shape)\n",
    "\n",
    "    # Concatenate image and conditioning image生成输入对象\n",
    "    combined_matrix = Concatenate(axis=-1)([matrix_A, matrix_B])\n",
    "\n",
    "    d1 = d_layer(combined_matrix, d_filters_base, bn=False)\n",
    "    d2 = d_layer(d1, d_filters_base*2, stride=2)\n",
    "#     d2 = AveragePooling2D((2, 2))(d2)\n",
    "    d3 = d_layer(d2, d_filters_base*4, stride=2)\n",
    "#     d3 = AveragePooling2D((2, 2))(d3)\n",
    "    d4 = d_layer(d3, d_filters_base*8, stride=2)\n",
    "#     d4 = AveragePooling2D((2, 2))(d4)\n",
    "    d4 = d_layer(d4, d_filters_base*4)\n",
    "    d5 = d_layer(d4, d_filters_base*2)\n",
    "    d6 = d_layer(d5, d_filters_base*1)\n",
    "    \n",
    "    validity = Conv2D(1, kernel_size=3, strides=1, padding='same')(d6)\n",
    "    model = Model([matrix_A, matrix_B], validity)\n",
    "    model.compile(optimizer=Adam(lr=learn_rate_d), loss=wasserstein_loss, metrics=['mse'])   #binary_crossentropy\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((64, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "    \n",
    "    \n",
    "class GradNorm(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(GradNorm, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(GradNorm, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        vaild_interpolated, interpolation_volume = inputs\n",
    "        grads = K.gradients(vaild_interpolated, interpolation_volume)\n",
    "        assert len(grads) == 1\n",
    "        grad = grads[0]\n",
    "#         a = K.sqrt(K.sum(K.batch_flatten(K.square(grad)), axis=1, keepdims=True))\n",
    "        return grad\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (MAX_BATCH_SIZE,) + true_volume_shape\n",
    "    \n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        return (MAX_BATCH_SIZE,) + true_volume_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "discriminator = build_discriminator()\n",
    "generator = build_generator()\n",
    "\n",
    "\n",
    "true_volume = Input(shape=true_volume_shape)\n",
    "history_volume = Input(shape=history_volume_shape)\n",
    "interpolation_volume = Input(shape=true_volume_shape)\n",
    "\n",
    "forecast_volume = generator(history_volume)\n",
    "\n",
    "discriminator.trainable = False\n",
    "true_vaild = discriminator([true_volume, history_volume])\n",
    "fake_vaild = discriminator([forecast_volume, history_volume])\n",
    "\n",
    "# gp = gradient_penalty_loss(true_volume, forecast_volume, interpolation_volume)\n",
    "norm = GradNorm()([discriminator([interpolation_volume, history_volume]), interpolation_volume])\n",
    "\n",
    "combined = Model(inputs=[true_volume, history_volume, interpolation_volume],\n",
    "                    outputs=[true_vaild, fake_vaild, norm, forecast_volume])\n",
    "combined.compile(loss=[wasserstein_loss,\n",
    "                        neg_wasserstein_loss,\n",
    "                       'mse',\n",
    "                        'mse'],\n",
    "                        optimizer=RMSprop(lr=learn_rate_c),\n",
    "                        loss_weights=[1, 1, 5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_step = []\n",
    "l2_validation = []\n",
    "\n",
    "def train(train_matrix, epochs, batch_size=MAX_BATCH_SIZE, learn_rate=0.01):\n",
    "\n",
    "    min_mse = 999\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(\"train start \"+str(start_time))\n",
    "\n",
    "    # Adversarial loss ground truths\n",
    "#     valid = np.ones((MAX_BATCH_SIZE,) + disc_patch)+np.random.rand(MAX_BATCH_SIZE, patch, patch, 1)/5\n",
    "#     fake = np.zeros((MAX_BATCH_SIZE,) + disc_patch)+np.random.rand(MAX_BATCH_SIZE, patch, patch, 1)/5\n",
    "    valid = np.ones((MAX_BATCH_SIZE,) + disc_patch)\n",
    "    fake = -np.ones((MAX_BATCH_SIZE,) + disc_patch)\n",
    "    dummy = np.zeros((MAX_BATCH_SIZE,) + true_volume_shape)\n",
    "\n",
    "    #　周期修改学习率　https://zhuanlan.zhihu.com/p/52084949\n",
    "    for epoch in range(epochs):\n",
    "        if epoch>=100 and epoch % 5 == 0 and epoch != 0:\n",
    "            generator_lr = K.get_value(generator.optimizer.lr)\n",
    "            discriminator_lr = K.get_value(discriminator.optimizer.lr)\n",
    "            combined_lr = K.get_value(combined.optimizer.lr)\n",
    "            if generator_lr>0.0001:\n",
    "                K.set_value(generator.optimizer.lr, generator_lr*0.9)\n",
    "            if discriminator_lr>0.0005:\n",
    "                K.set_value(discriminator.optimizer.lr, discriminator_lr*0.9)\n",
    "            if combined_lr>0.0001:\n",
    "                K.set_value(combined.optimizer.lr, combined_lr*0.9)\n",
    "\n",
    "        for batch_i, (true_volume, history_volume) in enumerate(load_data(train_matrix,batch_size)):\n",
    "            # true_volume 真实待预测路网交通量  history_volume 路网交通量历史数据\n",
    "            #  训练 Discriminator\n",
    "\n",
    "            # 根据历史数据生成预测数据\n",
    "            forecast_volume = generator.predict(history_volume)\n",
    "\n",
    "            # 训练 the discriminators (original images = real / generated = Fake)\n",
    "            discriminator.trainable = True\n",
    "            d_loss_real = discriminator.train_on_batch([true_volume, history_volume], valid)\n",
    "            d_loss_fake = discriminator.train_on_batch([forecast_volume, history_volume], fake)\n",
    "            discriminator.trainable = False\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            \n",
    "\n",
    "            epsilon = np.random.uniform(0, 1, size=(MAX_BATCH_SIZE,1,1,1))\n",
    "            interpolation_volume = epsilon*true_volume + (1-epsilon)*forecast_volume\n",
    "            #  训练 Generator\n",
    "            g_loss = combined.train_on_batch([true_volume, history_volume, interpolation_volume], [valid, fake, dummy, true_volume])\n",
    "\n",
    "            elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "        # Plot the progress\n",
    "        y_pred = generator.predict(X_test[:, :, :, 1:])\n",
    "        y_true = X_test[:, :, :, :1]\n",
    "\n",
    "        l2_epoch_validation = l2(y_true, y_pred)\n",
    "        l1_epoch_validation = l1(y_true, y_pred)\n",
    "        \n",
    "        y_pred[y_true==0] += 1\n",
    "        y_true[y_true==0] += 1\n",
    "        mape_epoch_validation = mape(y_true, y_pred)\n",
    "        \n",
    "#         lr_step.append(K.get_value(discriminator.optimizer.lr))\n",
    "        if(l2_epoch_validation<12 and l2_epoch_validation < min_mse):\n",
    "            generator.save_weights('./model/wganpg/tmp/min_generator_wganpg.h5')\n",
    "            discriminator.save_weights('./model/wganpg/tmp/min_discriminator_wganpg.h5')\n",
    "            combined.save_weights('./model/wganpg/tmp/min_combined_wganpg.h5')\n",
    "            min_mse = l2_epoch_validation\n",
    "            \n",
    "        l2_validation.append(l2_epoch_validation)\n",
    "        if epoch%1==0:\n",
    "#             print(\"unet lr:\"+ str(K.get_value(unet.optimizer.lr)))\n",
    "            print (\"[Epoch %d/%d]  [D loss: %f, mse: %f] [mae: %f] [mape: %f] [G loss: %f] time: %s\" % (epoch+1, epochs,\n",
    "                                                                    d_loss[0], l2_epoch_validation,\n",
    "                                                                    l1_epoch_validation,\n",
    "                                                                    mape_epoch_validation,\n",
    "                                                                    g_loss[0],\n",
    "                                                                    elapsed_time))\n",
    "        # If at show interval => show generated image samples\n",
    "#             if epoch % show_interval == 0:\n",
    "#                     show_images(dataset_name,epoch, batch_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train start 2019-11-26 20:04:30.465092\n",
      "[Epoch 1/200]  [D loss: -11.302105, mse: 127.779220] [mae: 95.495352] [mape: 36.658643] [G loss: 60153.167969] time: 0:01:42.281047\n",
      "[Epoch 2/200]  [D loss: 1.444169, mse: 96.376003] [mae: 71.807692] [mape: 28.695293] [G loss: 29995.324219] time: 0:02:26.987401\n",
      "[Epoch 3/200]  [D loss: 0.898896, mse: 109.318951] [mae: 82.933535] [mape: 32.708273] [G loss: 83439.632812] time: 0:03:11.448609\n",
      "[Epoch 4/200]  [D loss: 0.346400, mse: 101.588147] [mae: 77.193832] [mape: 30.490140] [G loss: 71639.664062] time: 0:03:56.022646\n",
      "[Epoch 5/200]  [D loss: 0.431405, mse: 77.507787] [mae: 58.171567] [mape: 23.417914] [G loss: 39583.148438] time: 0:04:40.559530\n",
      "[Epoch 6/200]  [D loss: 0.419912, mse: 69.687750] [mae: 51.820428] [mape: 20.581459] [G loss: 31995.087891] time: 0:05:25.068869\n",
      "[Epoch 7/200]  [D loss: 1.311784, mse: 76.380736] [mae: 57.420137] [mape: 22.676815] [G loss: 29640.492188] time: 0:06:09.543300\n",
      "[Epoch 8/200]  [D loss: 0.395394, mse: 55.096535] [mae: 40.566156] [mape: 16.316570] [G loss: 7590.178223] time: 0:06:53.963820\n",
      "[Epoch 9/200]  [D loss: 0.095937, mse: 44.339789] [mae: 32.147718] [mape: 13.255866] [G loss: 7355.425293] time: 0:07:38.938797\n",
      "[Epoch 10/200]  [D loss: 0.070688, mse: 27.317759] [mae: 18.948692] [mape: 9.956263] [G loss: 6436.239746] time: 0:08:24.124709\n",
      "[Epoch 11/200]  [D loss: 0.105966, mse: 20.087387] [mae: 12.600302] [mape: 6.884440] [G loss: 5065.823242] time: 0:09:09.557366\n",
      "[Epoch 12/200]  [D loss: 0.405596, mse: 19.336041] [mae: 12.022382] [mape: 6.558574] [G loss: 4814.766113] time: 0:09:54.064394\n",
      "[Epoch 13/200]  [D loss: 0.387014, mse: 21.053421] [mae: 13.629927] [mape: 7.164380] [G loss: 5102.846191] time: 0:10:39.186253\n",
      "[Epoch 14/200]  [D loss: 0.200506, mse: 19.156819] [mae: 11.980658] [mape: 6.515202] [G loss: 4596.428223] time: 0:11:24.152570\n",
      "[Epoch 15/200]  [D loss: 0.501620, mse: 19.057972] [mae: 11.907486] [mape: 6.525435] [G loss: 4377.301758] time: 0:12:08.747853\n",
      "[Epoch 16/200]  [D loss: 0.128973, mse: 18.535493] [mae: 11.552106] [mape: 6.352613] [G loss: 4264.828613] time: 0:12:53.221352\n",
      "[Epoch 17/200]  [D loss: 0.130814, mse: 17.666579] [mae: 10.799564] [mape: 6.056016] [G loss: 3948.261230] time: 0:13:37.648166\n",
      "[Epoch 18/200]  [D loss: 0.196828, mse: 17.162415] [mae: 10.440535] [mape: 5.858360] [G loss: 3796.330322] time: 0:14:22.095756\n",
      "[Epoch 19/200]  [D loss: 0.460152, mse: 23.699679] [mae: 15.965888] [mape: 7.983293] [G loss: 4887.179199] time: 0:15:06.740718\n",
      "[Epoch 20/200]  [D loss: 1.236373, mse: 21.123790] [mae: 13.741361] [mape: 6.989164] [G loss: 4464.226562] time: 0:15:51.744421\n",
      "[Epoch 21/200]  [D loss: 0.172158, mse: 20.933590] [mae: 13.552137] [mape: 6.906746] [G loss: 4278.191895] time: 0:16:36.816088\n",
      "[Epoch 22/200]  [D loss: 0.732896, mse: 23.443915] [mae: 15.796718] [mape: 6.938097] [G loss: 3766.444580] time: 0:17:22.162323\n",
      "[Epoch 23/200]  [D loss: 0.444578, mse: 22.307756] [mae: 14.904084] [mape: 7.428274] [G loss: 4115.069824] time: 0:18:07.605878\n",
      "[Epoch 24/200]  [D loss: 0.169796, mse: 23.230907] [mae: 15.699995] [mape: 7.785181] [G loss: 4038.099121] time: 0:18:52.487074\n",
      "[Epoch 25/200]  [D loss: 0.056180, mse: 15.741316] [mae: 9.497047] [mape: 5.247916] [G loss: 3124.769287] time: 0:19:37.432369\n",
      "[Epoch 26/200]  [D loss: 1.159339, mse: 19.478555] [mae: 12.707765] [mape: 6.365868] [G loss: 3415.847900] time: 0:20:22.144877\n",
      "[Epoch 27/200]  [D loss: 0.272929, mse: 14.892324] [mae: 8.985235] [mape: 5.110095] [G loss: 2996.705078] time: 0:21:07.114504\n",
      "[Epoch 28/200]  [D loss: 0.131967, mse: 19.447743] [mae: 12.711624] [mape: 6.239149] [G loss: 3497.208984] time: 0:21:52.111196\n",
      "[Epoch 29/200]  [D loss: 0.372707, mse: 17.095438] [mae: 10.598767] [mape: 5.387475] [G loss: 3055.143311] time: 0:22:37.212372\n",
      "[Epoch 30/200]  [D loss: 0.948797, mse: 14.748137] [mae: 8.840599] [mape: 4.958698] [G loss: 2871.420166] time: 0:23:21.815231\n",
      "[Epoch 31/200]  [D loss: 0.167937, mse: 20.632202] [mae: 13.856957] [mape: 6.850920] [G loss: 3543.062988] time: 0:24:06.173027\n",
      "[Epoch 32/200]  [D loss: 0.318714, mse: 14.723052] [mae: 8.843081] [mape: 4.829510] [G loss: 2813.553955] time: 0:24:50.525748\n",
      "[Epoch 33/200]  [D loss: 0.619152, mse: 21.090478] [mae: 14.112487] [mape: 6.621932] [G loss: 3630.049316] time: 0:25:35.620944\n",
      "[Epoch 34/200]  [D loss: 0.034012, mse: 14.963129] [mae: 9.073236] [mape: 4.830116] [G loss: 2783.155518] time: 0:26:20.850026\n",
      "[Epoch 35/200]  [D loss: 0.118831, mse: 21.537249] [mae: 14.622165] [mape: 6.852130] [G loss: 3321.096436] time: 0:27:06.181922\n",
      "[Epoch 36/200]  [D loss: 0.068336, mse: 14.660390] [mae: 8.877207] [mape: 4.877322] [G loss: 2719.742920] time: 0:27:51.212876\n",
      "[Epoch 37/200]  [D loss: 0.579687, mse: 17.897810] [mae: 11.659016] [mape: 5.878194] [G loss: 2855.298584] time: 0:28:36.423586\n",
      "[Epoch 38/200]  [D loss: 0.083237, mse: 14.199946] [mae: 8.571342] [mape: 4.693683] [G loss: 2603.576172] time: 0:29:21.701903\n",
      "[Epoch 39/200]  [D loss: 0.165078, mse: 20.098262] [mae: 13.554783] [mape: 6.452878] [G loss: 2969.944824] time: 0:30:06.546033\n",
      "[Epoch 40/200]  [D loss: 0.493113, mse: 16.975890] [mae: 11.010170] [mape: 5.657586] [G loss: 2672.216553] time: 0:30:51.395901\n",
      "[Epoch 41/200]  [D loss: 0.152895, mse: 16.943679] [mae: 10.740053] [mape: 5.349404] [G loss: 2801.018311] time: 0:31:36.279059\n",
      "[Epoch 42/200]  [D loss: 0.231073, mse: 18.076139] [mae: 12.029399] [mape: 5.967047] [G loss: 2698.527832] time: 0:32:21.027923\n",
      "[Epoch 43/200]  [D loss: 0.129430, mse: 14.204544] [mae: 8.554215] [mape: 4.653208] [G loss: 2413.770264] time: 0:33:05.615855\n",
      "[Epoch 44/200]  [D loss: 0.091255, mse: 19.978331] [mae: 13.564973] [mape: 6.488123] [G loss: 2890.579102] time: 0:33:50.363337\n",
      "[Epoch 45/200]  [D loss: 0.093170, mse: 15.446625] [mae: 9.721771] [mape: 5.092290] [G loss: 2417.183594] time: 0:34:35.121808\n",
      "[Epoch 46/200]  [D loss: 1.675068, mse: 19.129640] [mae: 12.907462] [mape: 6.227806] [G loss: 2725.220947] time: 0:35:20.148155\n",
      "[Epoch 47/200]  [D loss: 0.118335, mse: 21.775822] [mae: 14.989218] [mape: 6.995183] [G loss: 2806.055664] time: 0:36:05.515876\n",
      "[Epoch 48/200]  [D loss: 0.506245, mse: 14.471502] [mae: 8.915084] [mape: 4.625101] [G loss: 2476.515869] time: 0:36:50.951744\n",
      "[Epoch 49/200]  [D loss: 0.092923, mse: 14.970824] [mae: 9.321271] [mape: 4.799245] [G loss: 2379.274414] time: 0:37:37.605551\n",
      "[Epoch 50/200]  [D loss: 0.272149, mse: 13.494240] [mae: 8.180936] [mape: 4.409267] [G loss: 2387.143311] time: 0:38:24.504319\n",
      "[Epoch 51/200]  [D loss: 0.065368, mse: 13.239308] [mae: 8.026170] [mape: 4.448446] [G loss: 2362.499756] time: 0:39:10.951726\n",
      "[Epoch 52/200]  [D loss: 0.344307, mse: 24.159214] [mae: 16.828814] [mape: 6.930575] [G loss: 3729.415039] time: 0:39:56.052552\n",
      "[Epoch 53/200]  [D loss: 0.598368, mse: 17.052978] [mae: 11.139157] [mape: 5.663683] [G loss: 2852.379150] time: 0:40:41.749522\n",
      "[Epoch 54/200]  [D loss: 0.309317, mse: 13.042361] [mae: 7.902102] [mape: 4.448990] [G loss: 2341.129395] time: 0:41:27.164220\n",
      "[Epoch 55/200]  [D loss: 0.117508, mse: 18.783053] [mae: 12.448374] [mape: 5.474540] [G loss: 3008.718994] time: 0:42:12.425145\n",
      "[Epoch 56/200]  [D loss: 0.087649, mse: 18.420788] [mae: 12.537253] [mape: 6.281917] [G loss: 2735.188232] time: 0:42:57.299310\n",
      "[Epoch 57/200]  [D loss: 0.245861, mse: 12.787983] [mae: 7.701945] [mape: 4.177201] [G loss: 2256.992188] time: 0:43:42.623590\n",
      "[Epoch 58/200]  [D loss: 0.142860, mse: 12.817720] [mae: 7.805917] [mape: 4.359852] [G loss: 2257.915527] time: 0:44:27.936431\n",
      "[Epoch 59/200]  [D loss: 0.037352, mse: 14.284081] [mae: 8.958327] [mape: 4.770942] [G loss: 2550.176270] time: 0:45:14.501998\n",
      "[Epoch 60/200]  [D loss: 0.021475, mse: 12.833367] [mae: 7.718600] [mape: 4.105504] [G loss: 2249.458252] time: 0:46:00.796716\n",
      "[Epoch 61/200]  [D loss: 0.090734, mse: 12.813070] [mae: 7.644875] [mape: 4.143152] [G loss: 2169.733398] time: 0:46:46.260918\n",
      "[Epoch 62/200]  [D loss: 0.173214, mse: 12.982190] [mae: 7.932064] [mape: 4.204065] [G loss: 2211.481201] time: 0:47:34.272546\n",
      "[Epoch 63/200]  [D loss: 0.064514, mse: 13.117836] [mae: 8.148243] [mape: 4.391779] [G loss: 2223.604736] time: 0:48:22.343150\n",
      "[Epoch 64/200]  [D loss: 0.082513, mse: 12.436945] [mae: 7.599220] [mape: 4.175914] [G loss: 2139.672119] time: 0:49:11.226957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 65/200]  [D loss: 0.039713, mse: 13.688999] [mae: 8.693073] [mape: 4.608637] [G loss: 2243.221924] time: 0:50:00.859000\n",
      "[Epoch 66/200]  [D loss: 0.033947, mse: 13.575590] [mae: 8.474314] [mape: 4.272195] [G loss: 2133.914795] time: 0:50:50.985971\n",
      "[Epoch 67/200]  [D loss: 0.033425, mse: 17.127753] [mae: 11.481602] [mape: 5.281972] [G loss: 2264.691895] time: 0:51:38.099640\n",
      "[Epoch 68/200]  [D loss: 0.299351, mse: 18.760525] [mae: 12.820165] [mape: 5.942230] [G loss: 2570.976074] time: 0:52:25.034556\n",
      "[Epoch 69/200]  [D loss: 0.085025, mse: 21.644220] [mae: 15.061259] [mape: 6.315371] [G loss: 2928.061768] time: 0:53:09.926545\n",
      "[Epoch 70/200]  [D loss: 0.293273, mse: 15.726237] [mae: 10.657795] [mape: 5.292880] [G loss: 2170.220459] time: 0:53:54.788583\n",
      "[Epoch 71/200]  [D loss: 0.083260, mse: 15.243834] [mae: 10.079328] [mape: 4.924417] [G loss: 2840.927979] time: 0:54:39.415334\n",
      "[Epoch 72/200]  [D loss: 0.030261, mse: 13.738163] [mae: 8.848586] [mape: 4.462246] [G loss: 1964.566040] time: 0:55:23.837904\n",
      "[Epoch 73/200]  [D loss: 0.138703, mse: 18.996901] [mae: 13.378536] [mape: 6.246299] [G loss: 2506.928223] time: 0:56:08.261428\n",
      "[Epoch 74/200]  [D loss: 0.334838, mse: 11.672889] [mae: 7.087825] [mape: 3.831137] [G loss: 1828.850708] time: 0:56:52.766805\n",
      "[Epoch 75/200]  [D loss: 0.198197, mse: 17.228416] [mae: 11.954979] [mape: 5.870750] [G loss: 2374.621582] time: 0:57:43.900511\n",
      "[Epoch 76/200]  [D loss: 0.225132, mse: 11.426592] [mae: 6.943202] [mape: 3.798448] [G loss: 1804.759277] time: 0:58:28.693891\n",
      "[Epoch 77/200]  [D loss: 0.127365, mse: 21.252940] [mae: 14.963067] [mape: 6.284111] [G loss: 3540.101318] time: 0:59:15.034059\n",
      "[Epoch 78/200]  [D loss: 0.093948, mse: 12.810421] [mae: 7.981367] [mape: 4.084888] [G loss: 1983.778198] time: 0:59:59.952033\n",
      "[Epoch 79/200]  [D loss: 0.234309, mse: 13.416935] [mae: 8.462363] [mape: 4.181780] [G loss: 1928.817505] time: 1:00:45.031195\n",
      "[Epoch 80/200]  [D loss: 0.042285, mse: 17.447629] [mae: 12.136518] [mape: 5.642138] [G loss: 2316.593506] time: 1:01:30.110636\n",
      "[Epoch 81/200]  [D loss: 0.120037, mse: 11.829167] [mae: 7.228575] [mape: 3.826635] [G loss: 1802.625732] time: 1:02:15.172237\n",
      "[Epoch 82/200]  [D loss: 0.053287, mse: 18.143315] [mae: 12.634104] [mape: 5.876637] [G loss: 3302.448486] time: 1:03:00.226880\n",
      "[Epoch 83/200]  [D loss: 0.066865, mse: 13.670100] [mae: 8.742106] [mape: 4.175096] [G loss: 1987.022705] time: 1:03:44.808140\n",
      "[Epoch 84/200]  [D loss: 0.275710, mse: 15.504246] [mae: 10.033118] [mape: 4.616174] [G loss: 2555.843506] time: 1:04:29.306402\n",
      "[Epoch 85/200]  [D loss: 0.916249, mse: 13.671762] [mae: 8.524503] [mape: 4.227554] [G loss: 1862.674927] time: 1:05:14.445962\n",
      "[Epoch 86/200]  [D loss: 0.202673, mse: 12.039943] [mae: 7.424281] [mape: 3.782578] [G loss: 1708.060303] time: 1:05:59.179431\n",
      "[Epoch 87/200]  [D loss: 0.101606, mse: 14.501368] [mae: 9.504095] [mape: 4.518790] [G loss: 1904.739136] time: 1:06:43.603231\n",
      "[Epoch 88/200]  [D loss: 0.106051, mse: 11.373093] [mae: 6.732830] [mape: 3.608164] [G loss: 1690.254517] time: 1:07:27.972204\n",
      "[Epoch 89/200]  [D loss: 0.108674, mse: 13.430815] [mae: 8.490434] [mape: 4.025781] [G loss: 1871.127686] time: 1:08:13.327289\n",
      "[Epoch 90/200]  [D loss: 0.071553, mse: 14.150173] [mae: 9.025241] [mape: 4.352539] [G loss: 1982.974121] time: 1:08:58.345053\n",
      "[Epoch 91/200]  [D loss: 0.050822, mse: 11.108974] [mae: 6.596056] [mape: 3.530500] [G loss: 1590.809204] time: 1:09:42.801284\n",
      "[Epoch 92/200]  [D loss: 0.069163, mse: 11.206465] [mae: 6.738686] [mape: 3.582426] [G loss: 1606.613892] time: 1:10:28.175679\n",
      "[Epoch 93/200]  [D loss: 0.756427, mse: 11.155573] [mae: 6.731388] [mape: 3.787274] [G loss: 1625.064453] time: 1:11:12.588117\n",
      "[Epoch 94/200]  [D loss: 0.104901, mse: 11.782458] [mae: 6.945824] [mape: 3.630380] [G loss: 1585.214478] time: 1:11:57.192595\n",
      "[Epoch 95/200]  [D loss: 0.040380, mse: 11.668955] [mae: 7.013339] [mape: 3.641007] [G loss: 1614.606934] time: 1:12:42.395591\n",
      "[Epoch 96/200]  [D loss: 0.283119, mse: 17.805738] [mae: 12.443245] [mape: 5.786851] [G loss: 3126.768799] time: 1:13:27.232844\n",
      "[Epoch 97/200]  [D loss: 0.373306, mse: 11.271344] [mae: 6.862637] [mape: 3.789398] [G loss: 1568.497437] time: 1:14:12.290560\n",
      "[Epoch 98/200]  [D loss: 0.058361, mse: 12.177204] [mae: 7.411714] [mape: 3.722501] [G loss: 1627.298096] time: 1:14:57.185731\n",
      "[Epoch 99/200]  [D loss: 0.190313, mse: 11.641764] [mae: 7.090034] [mape: 3.780113] [G loss: 1563.713257] time: 1:15:42.057516\n",
      "[Epoch 100/200]  [D loss: 0.038638, mse: 11.567601] [mae: 6.990753] [mape: 3.701071] [G loss: 1587.278687] time: 1:16:27.369321\n",
      "[Epoch 101/200]  [D loss: 0.019113, mse: 11.446172] [mae: 6.748603] [mape: 3.523908] [G loss: 1560.118530] time: 1:17:14.002921\n",
      "[Epoch 102/200]  [D loss: 0.236781, mse: 13.767154] [mae: 8.700330] [mape: 4.177412] [G loss: 1834.825439] time: 1:17:58.905336\n",
      "[Epoch 103/200]  [D loss: 0.037584, mse: 11.806306] [mae: 7.118974] [mape: 3.642486] [G loss: 1613.476562] time: 1:18:43.603917\n",
      "[Epoch 104/200]  [D loss: 0.032184, mse: 11.390512] [mae: 6.820766] [mape: 3.680779] [G loss: 1587.482788] time: 1:19:28.613765\n",
      "[Epoch 105/200]  [D loss: 0.252559, mse: 11.134362] [mae: 6.542951] [mape: 3.492638] [G loss: 1526.088989] time: 1:20:13.759290\n",
      "[Epoch 106/200]  [D loss: 0.103139, mse: 11.381775] [mae: 6.659981] [mape: 3.536010] [G loss: 1494.697876] time: 1:20:58.521867\n",
      "[Epoch 107/200]  [D loss: 0.071021, mse: 11.891812] [mae: 7.076078] [mape: 3.662099] [G loss: 1532.757324] time: 1:21:43.030205\n",
      "[Epoch 108/200]  [D loss: 0.094309, mse: 12.137808] [mae: 7.293181] [mape: 3.627292] [G loss: 1526.730591] time: 1:22:30.720640\n",
      "[Epoch 109/200]  [D loss: 0.122483, mse: 13.080744] [mae: 8.320746] [mape: 4.170749] [G loss: 1621.661987] time: 1:23:18.750035\n",
      "[Epoch 110/200]  [D loss: 0.012425, mse: 11.601919] [mae: 6.760339] [mape: 3.529677] [G loss: 1451.849243] time: 1:24:06.152485\n",
      "[Epoch 111/200]  [D loss: 0.094088, mse: 11.309886] [mae: 6.820288] [mape: 3.621737] [G loss: 1586.270264] time: 1:24:52.108245\n",
      "[Epoch 112/200]  [D loss: 0.054794, mse: 11.064998] [mae: 6.441383] [mape: 3.400230] [G loss: 1430.169922] time: 1:25:42.106732\n",
      "[Epoch 113/200]  [D loss: 0.048774, mse: 12.127901] [mae: 7.470820] [mape: 3.903607] [G loss: 1468.459351] time: 1:26:30.613344\n",
      "[Epoch 114/200]  [D loss: 0.022237, mse: 11.283929] [mae: 6.865550] [mape: 3.688847] [G loss: 1512.249146] time: 1:27:18.277650\n",
      "[Epoch 115/200]  [D loss: 0.013975, mse: 13.317520] [mae: 8.629451] [mape: 4.428245] [G loss: 1728.850708] time: 1:28:06.273758\n",
      "[Epoch 116/200]  [D loss: 0.015540, mse: 13.183076] [mae: 8.071390] [mape: 3.798195] [G loss: 1642.162476] time: 1:28:53.663064\n",
      "[Epoch 117/200]  [D loss: 0.016771, mse: 11.379383] [mae: 6.791645] [mape: 3.553895] [G loss: 1456.776489] time: 1:29:41.820287\n",
      "[Epoch 118/200]  [D loss: 0.055892, mse: 11.585623] [mae: 6.651701] [mape: 3.496853] [G loss: 1388.415161] time: 1:30:27.556952\n",
      "[Epoch 119/200]  [D loss: 0.158359, mse: 10.928952] [mae: 6.314442] [mape: 3.284814] [G loss: 1389.360474] time: 1:31:13.667676\n",
      "[Epoch 120/200]  [D loss: 0.024675, mse: 12.852655] [mae: 7.730365] [mape: 3.693084] [G loss: 1568.336304] time: 1:32:02.887759\n",
      "[Epoch 121/200]  [D loss: 0.034796, mse: 10.974152] [mae: 6.338119] [mape: 3.328266] [G loss: 1379.715820] time: 1:32:47.569481\n",
      "[Epoch 122/200]  [D loss: 0.039545, mse: 11.480897] [mae: 6.747924] [mape: 3.382861] [G loss: 1370.836548] time: 1:33:33.958010\n",
      "[Epoch 123/200]  [D loss: 0.008318, mse: 11.855407] [mae: 6.816484] [mape: 3.459127] [G loss: 1374.613403] time: 1:34:21.096376\n",
      "[Epoch 124/200]  [D loss: 0.028805, mse: 11.366074] [mae: 6.720104] [mape: 3.513362] [G loss: 1393.909058] time: 1:35:05.970223\n",
      "[Epoch 125/200]  [D loss: 0.141795, mse: 11.251961] [mae: 6.573497] [mape: 3.318503] [G loss: 1356.598999] time: 1:35:50.520018\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-f2845cdbc646>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mMAX_BATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearn_rate_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-8f6589eeba8a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_matrix, epochs, batch_size, learn_rate)\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;31m# 训练 the discriminators (original images = real / generated = Fake)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0md_loss_real\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrue_volume\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_volume\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[0md_loss_fake\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mforecast_volume\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_volume\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mdiscriminator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2617\u001b[0m                 array_vals.append(\n\u001b[0;32m   2618\u001b[0m                     np.asarray(value,\n\u001b[1;32m-> 2619\u001b[1;33m                                dtype=tf.as_dtype(tensor.dtype).as_numpy_dtype))\n\u001b[0m\u001b[0;32m   2620\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2621\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \"\"\"\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(X_train, epochs=200, batch_size=MAX_BATCH_SIZE, learn_rate=learn_rate_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator.save_weights('./model/wganpg/generator_167epoch_11rmse.h5')\n",
    "# discriminator.save_weights('./model/wganpg/discriminator_167epoch_11rmse.h5')\n",
    "# combined.save_weights('./model/wganpg/combined_167epoch_11rmse.h5')\n",
    "\n",
    "# generator.load_weights('./model/wganpg/DS_rmse11/generator_167epoch_11rmse.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = generator.predict(X_test[:, :, :, 1:])\n",
    "y_true = X_test[:, :, :, :1]\n",
    "\n",
    "l2(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.sqrt(np.sum(np.mean(np.square(y_true - y_pred), axis=0))/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2(y_true, y_pred+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = y_true.reshape(-1,)[1600:1700]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[1600:1700]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yi = l2_validation\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "\n",
    "y = [i*10000 for i in lr_step]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "lines = plt.plot(x, y, 'ko-', xi, yi, 'k^--', linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lr_step\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "# fig, ax = plt.subplots(figsize=(6, 6))\n",
    "# lines = plt.plot(x, y, 'ko-', linewidth=1, markersize=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_true.reshape(-1,)[3600:3700]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[3600:3700]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = 0\n",
    "y = y_true.reshape(-1,)[start_time: start_time+100]\n",
    "x = np.linspace(0, len(y), len(y))\n",
    "\n",
    "yi = y_pred.reshape(-1,)[start_time: start_time+100]\n",
    "xi = np.linspace(0, len(yi), len(yi))\n",
    "fig, ax = plt.subplots(figsize=(25, 6))\n",
    "# ax.plot(x, y, '.', linewidth=1, markersize=10)\n",
    "lines = plt.plot(xi, yi, 'k^--', x, y, 'ro-',linewidth=1, markersize=6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
